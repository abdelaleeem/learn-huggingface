{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "# Please don't change the text below, it's for formatting the documentation.\n",
    "title: \"Text Classification with Hugging Face Transformers\"\n",
    "format: \n",
    "  html:\n",
    "    code-fold: false\n",
    "jupyter: python3\n",
    "number-sections: true\n",
    "toc: true\n",
    "toc-depth: 3\n",
    "toc-expand: 2 # expand toc to multiple levels\n",
    "code-block-border-left: true\n",
    "code-block-bg: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mrdbourke/learn-huggingface/blob/main/notebooks/hugging_face_text_classification_tutorial.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next:\n",
    "# Add tools used in this overview (e.g. overview of the project)\n",
    "# Create a small dataset with text generation, e.g. 50x spam/not_spam emails and train a classifier on it ✅\n",
    "   # Done, see notebook: https://colab.research.google.com/drive/14xr3KN_HINY5LjV0s2E-4i7v0o_XI3U8?usp=sharing \n",
    "# Save the dataset to Hugging Face Datasets ✅\n",
    "   # Done, see dataset: https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions\n",
    "# Train a classifier on it ✅\n",
    "# Save the model to the Hugging Face Model Hub ✅\n",
    "# Create a with Gradio and test the model in the wild ✅ \n",
    "\n",
    "# TODO:\n",
    "# Make sure notebook runs in Google Colab\n",
    "# Add images and diagrams throughout\n",
    "# Make sure the online book version looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TK - Overview\n",
    "\n",
    "### TK - What we're going to build\n",
    "\n",
    "In this project, we're going to learn various aspects of the Hugging Face ecosystem whilst building a text classification model.\n",
    "\n",
    "To keep things as practical as possible, we're going to be bulding a `food`/`not_food` text classification model. \n",
    "\n",
    "Given a piece of a text, our model will be able to predict if it's about food or not.\n",
    "\n",
    "This is the same kind of model I use in my own work on [Nutrify](https://www.nutrify.app) (an app to help people learn about food).\n",
    "\n",
    "More specifically, we're going to follow the following steps:\n",
    "\n",
    "1. **Problem defintion and dataset preparation** - Getting a dataset/setting up the problem space.\n",
    "2. **Finding, training and evaluating a model** - Finding a text classification model suitable for our problem on Hugging Face and customizing it to our own dataset.\n",
    "3. **Creating a demo and put our model into the real world** - Sharing our trained model in a way others can access and use.\n",
    "\n",
    "By the end of this project, you'll have a trained model and [demo on Hugging Face](https://huggingface.co/spaces/mrdbourke/learn_hf_food_not_food_text_classifier_demo) you can share with others.\n",
    "\n",
    "TK image - see the finished product (demo)\n",
    "\n",
    "::: {.callout-note}\n",
    "Note this is a hands-on project, so we'll be focused on writing reusable code and building a model that can be used in the real world. If you are looking for explainers to the theory of what we're doing, I'll leave links in the extra-curriculum section.\n",
    "::: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TK - What is Hugging Face?\n",
    "\n",
    "TK - perhaps put this in the front of the website? e.g. on the index page\n",
    "\n",
    "Hugging Face is a platform that offers access to many different kinds of open-source machine learning models and datasets.\n",
    "\n",
    "They're also the creators of the popular `transformers` library which is a Python-based library for working with pre-trained models as well as custom models and datasets.\n",
    "\n",
    "If you're getting into the world of AI and machine learning, you're going to come across Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### TK - Why Hugging Face?\n",
    "\n",
    "Many of the biggest companies in the world use Hugging Face for their open-source machine learning projects including [Apple](https://huggingface.co/apple), [Google](https://huggingface.co/google), [Facebook](https://huggingface.co/facebook) (Meta), [Microsoft](https://huggingface.co/microsoft), [OpenAI](https://huggingface.co/openai), [ByteDance](https://huggingface.co/ByteDance) and more.\n",
    "\n",
    "TK image - image of people using Hugging Face\n",
    "\n",
    "Not only does Hugging Face make it so you can use state-of-the-art machine learning models such as [Stable Diffusion](https://huggingface.co/stabilityai/stable-diffusion-2-1) (for image generation) and [Whipser](https://huggingface.co/openai/whisper-large-v3) (for audio transcription) easily, it also makes it so you can share your own models, datasets and resources.\n",
    "\n",
    "Consider Hugging Face the homepage of your AI/machine learning profile.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TK - What is text classification?\n",
    "\n",
    "Text classification is the process of assigning a category to a piece of text.\n",
    "\n",
    "Where a category can be almost anything and a piece of text can be a word, phrase, sentence, paragraph or entire document.\n",
    "\n",
    "TK image - example of text classification\n",
    "\n",
    "Example text classification problems include:\n",
    "\n",
    "| **Problem** | **Description** | **Problem Type** |\n",
    "|-----|-----|-----|\n",
    "| Spam/phishing email detection | Is an email spam or not spam? Or is it a phishing email or not? | Binary classification (one thing or another) |\n",
    "| Sentiment analysis | Is a piece of text positive, negative or neutral? | Multi-class classification (one thing from many) |\n",
    "| Language detection | What language is a piece of text written in? | Multi-class classification (one thing from many) |\n",
    "| Topic classification | What topic(s) does a news article belong to? | Multi-label classification (one or more things from many) |\n",
    "| Hate speech detection | Is a comment hateful or not hateful? | Binary classification (one thing or another) |\n",
    "| Product categorization | What categories does a product belong to? | Multi-label classification (one or more things from many) |\n",
    "\n",
    "There are several different kinds of models you can use for text classification.\n",
    "\n",
    "And each will have its pros and cons depending on the problem you're working on.\n",
    "\n",
    "Example text classification models include:\n",
    "\n",
    "| **Model** | **Description** | **Pros** | **Cons** |\n",
    "|-----|-----|-----|-----|\n",
    "| Rule-based | Uses a set of rules to classify text (e.g. if text contains \"sad\" -> sentiment = low) | Simple, easy to understand | Requires manual creation of rules |\n",
    "| [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) | Counts the frequency of words in a piece of text | Simple, easy to understand | Doesn't capture word order |\n",
    "| [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) | Weighs the importance of words in a piece of text | Simple, easy to understand | Doesn't capture word order |\n",
    "| Deep learning-based models | Uses neural networks to learn patterns in text | Can learn complex patterns at scale | Can require large amounts of data/compute power to run, not as easy to understand (can be hard to debug) |\n",
    "\n",
    "We're going to use a deep learning model our case.\n",
    "\n",
    "Why?\n",
    "\n",
    "Because Hugging Face helps us do so.\n",
    "\n",
    "And in most cases, with a large enough dataset, a deep learning model will often perform better than a rule-based or other model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TK - Why train your own text classification models?\n",
    "\n",
    "You can use pre-trained models for text classification as well as API-powered models and LLMs such as GPT-4 or Gemini.\n",
    "\n",
    "However, it's often a good idea to train your own text classification models for a few reasons:\n",
    "\n",
    "* They can be much faster than API-powered models (since they're running on your own hardware, this can save on costs and time).\n",
    "* They're customized to your own data.\n",
    "* They don't require you to send your data elsewhere (privacy).\n",
    "* If a service goes down, you'll still have access to your model (reliability).\n",
    "\n",
    "TK image - example of training your own model vs using an API-powered model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TK - Importing necessary libraries\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "First, we'll import the required libraries.\n",
    "\n",
    "If you're running on your local computer, be sure to check out the getting setup guide (tk - link to getting setup guide) to make sure you have everything you need.\n",
    "\n",
    "If you're using Google Colab, many of them the following libraries will be installed by default.\n",
    "\n",
    "However, we'll have to install a few extras to get everything working.\n",
    "\n",
    "::: {.callout-note}\n",
    "If you're running on Google Colab, this notebook will work best with access to a GPU. To enable a GPU, go to `Runtime` ➡️ `Change runtime type` ➡️ `Hardware accelerator` ➡️ `GPU`.\n",
    "::: \n",
    "\n",
    "We'll need to install the following libraries from the Hugging Face ecosystem:\n",
    "\n",
    "* [`transformers`](https://huggingface.co/docs/transformers/en/installation) - comes pre-installed on Google Colab but if you're running on your local machine, you can install it via `pip install transformers`.\n",
    "* [`datasets`](https://huggingface.co/docs/datasets/installation) - a library for accessing and manipulating datasets on and off the Hugging Face Hub, you can install it via `pip install datasets`.\n",
    "* [`evaluate`](https://huggingface.co/docs/evaluate/installation) - a library for evaluating machine learning model performance with various metrics, you can install it via `pip install evaluate`.\n",
    "* [`accelerate`](https://huggingface.co/docs/accelerate/basic_tutorials/install) - a library for training machine learning models faster, you can install it via `pip install accelerate`.\n",
    "* [`gradio`](https://www.gradio.app/guides/quickstart#installation) - a library for creating interactive demos of machine learning models, you can install it via `pip install gradio`.\n",
    "\n",
    "We can also check the versions of our software with `package_name.__version__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "qJWCmI1rzOR6",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ccd72531-249d-402f-91b0-7b701b2dc7eb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using transformers version: 4.41.2\n",
      "Using datasets version: 2.19.1\n",
      "Using torch version: 2.2.0+cu121\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies (this is mostly for Google Colab, as the other dependences are available by default in Colab)\n",
    "try:\n",
    "  import datasets, evaluate, accelerate\n",
    "  import gradio as gr\n",
    "except ModuleNotFoundError:\n",
    "  !pip install -U datasets evaluate accelerate gradio # -U stands for \"upgrade\" so we'll get the latest version by default\n",
    "  import datasets, evaluate, accelerate\n",
    "  import gradio as gr\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "print(f\"Using transformers version: {transformers.__version__}\")\n",
    "print(f\"Using datasets version: {datasets.__version__}\")\n",
    "print(f\"Using torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful, as long as your versions are the same or higher to the versions above, you should be able to run the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TK - Getting a dataset\n",
    "\n",
    "Okay, now we're got the required libraries, let's get a dataset.\n",
    "\n",
    "Getting a dataset is one of the most important things a machine learning project.\n",
    "\n",
    "The dataset you often determines the type of model you use as well as the quality of the outputs of that model.\n",
    "\n",
    "Meaning, if you have a high quality dataset, chances are, your future model could also have high quality outputs.\n",
    "\n",
    "It also means if your dataset is of poor quality, your model will likely also have poor quality outputs.\n",
    "\n",
    "For a text classificaiton problem, your dataset will likely come in the form of text (e.g. a paragraph, sentence or phrase) and a label (e.g. what category the text belongs to).\n",
    "\n",
    "* TK image - showcase what a supervised dataset looks like (e.g. text and label, this can be the dataset we've got on Hugging Face hub, showcase the different parts of the dataset as well including the name etc)\n",
    "\n",
    "In our case, our dataset comes in the form of a collection of synthetic image captions and their corresponding labels (food or not food).\n",
    "\n",
    "This is a dataset I've created earlier to help us practice building a text classification model.\n",
    "\n",
    "You can find it on Hugging Face under the name [`mrdbourke/learn_hf_food_not_food_image_captions`](https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions).\n",
    "\n",
    "::: {.callout-tip}\n",
    "## Resource\n",
    "\n",
    "See how the food/not_food image caption dataset was created in the (TK - add notebook link and title, make this available on the website)\n",
    "\n",
    "* TK - see dataset creation: \n",
    "   * Done, see notebook: https://colab.research.google.com/drive/14xr3KN_HINY5LjV0s2E-4i7v0o_XI3U8?usp=sharing \n",
    "   * Done, see dataset: https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where can you get more datasets?\n",
    "\n",
    "The are many different places you can get datasets for text-based problems.\n",
    "\n",
    "One of the best places is on the Hugging Face Hub, specifically [huggingface.co/datasets](https://huggingface.co/datasets).\n",
    "\n",
    "Here you can find many different kinds of problem specific data such as [text classification](https://huggingface.co/datasets?task_categories=task_categories:text-classification&sort=trending).\n",
    "\n",
    "TK image - show example image of text classification datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "Once we've found/prepared a dataset on the Hugging Face Hub, we can use the `datasets` library to load it.\n",
    "\n",
    "To load a dataset we can use the [`datasets.load_dataset(path=NAME_OR_PATH_OF_DATASET)`](https://huggingface.co/docs/datasets/v2.20.0/en/package_reference/loading_methods#datasets.load_dataset) function and pass it the name/path of the dataset we want to load.\n",
    "\n",
    "In our case, our dataset name is `mrdbourke/learn_hf_food_not_food_image_captions`.\n",
    "\n",
    "And since our dataset is hosted on Hugging Face, when we run the following code for the first time, it will download it.\n",
    "\n",
    "If your target dataset is quite large, this download may take a while.\n",
    "\n",
    "However, once the dataset is downloaded, subsequent reloads will be mush faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "id": "PGt72DrtqRfM",
    "outputId": "2f45489c-02e8-4c03-bb7c-d73faa46c5b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 250\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset from Hugging Face Hub\n",
    "dataset = datasets.load_dataset(path=\"mrdbourke/learn_hf_food_not_food_image_captions\")\n",
    "\n",
    "# Inspect the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset loaded!\n",
    "\n",
    "Looks like our dataset has two features, `text` and `label`.\n",
    "\n",
    "And 250 total rows (the number of examples in our dataset).\n",
    "\n",
    "We can check the column names with `dataset.column_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['text', 'label']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What features are there?\n",
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our dataset comes with a `train` split already (the whole dataset).\n",
    "\n",
    "We can access the `train` split with `dataset[\"train\"]` (some datasets also come with built-in `\"test\"` splits too)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the training split\n",
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about we check out a single sample?\n",
    "\n",
    "We can do so with indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Creamy cauliflower curry with garlic naan, featuring tender cauliflower in a rich sauce with cream and spices, served with garlic naan bread.',\n",
       " 'label': 'food'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We get back a dictionary with the keys `text` and `label`.\n",
    "\n",
    "The `text` key contains the text of the image caption and the `label` key contains the label (food or not food)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TK - Inspect random examples from the dataset\n",
    "\n",
    "At 250 total samples, our dataset isn't too large.\n",
    "\n",
    "So we could sit there and explore the samples one by one.\n",
    "\n",
    "But whenever I interact with a new dataset, I like to view a bunch of random examples and get a *feel* of the data.\n",
    "\n",
    "Doing so is inline with the data explorer's motto: *visualize, visualize, visualize!* \n",
    "\n",
    "As a rule of thumb, I like to view at least 20-100 random examples when interacting with a new dataset.\n",
    "\n",
    "Let's write some code to view 5 random indexes of our data and their corresponding text and labels at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Random samples from dataset:\n",
      "\n",
      "Text: Spicy prawn curry with fresh mint garnish, featuring juicy prawns in a fiery sauce with onions and tomatoes, finished with mint leaves. | Label: food\n",
      "Text: A bowl of sliced oranges with a sprinkle of cinnamon and a side of cloves | Label: food\n",
      "Text: Set of headphones placed on a desk | Label: not_food\n",
      "Text: Set of baking sheets stacked in a cabinet | Label: not_food\n",
      "Text: Set of cake pans tucked in a drawer | Label: not_food\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_indexs = random.sample(range(len(dataset[\"train\"])), 5)\n",
    "random_samples = dataset[\"train\"][random_indexs]\n",
    "\n",
    "print(f\"[INFO] Random samples from dataset:\\n\")\n",
    "for item in zip(random_samples[\"text\"], random_samples[\"label\"]):\n",
    "    print(f\"Text: {item[0]} | Label: {item[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful! Looks like our data contains a mix of shorter and longer sentences (between 5 and 20 words) of texts about food and not food.\n",
    "\n",
    "We can get the unique labels in our dataset with [`dataset[\"train\"].unique(\"label\")`](https://huggingface.co/docs/datasets/v2.20.0/en/package_reference/main_classes#datasets.Dataset.unique)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['food', 'not_food']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get unique label values\n",
    "dataset[\"train\"].unique(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our dataset is small enough to fit into memory, we can count the number of different labels with Python's [`collections.Counter`](https://docs.python.org/3/library/collections.html#counter-objects) (a method for counting objects in an iterable or mapping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'food': 125, 'not_food': 125})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of each label\n",
    "from collections import Counter\n",
    "\n",
    "Counter(dataset[\"train\"][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, looks like our dataset is well balanced with 125 samples of food and 125 samples of not food.\n",
    "\n",
    "In a binary classification case, this is ideal.\n",
    "\n",
    "If the classes were dramatically unbalanced (e.g. 90% food and 10% not food) we might have to consider collecting/creating more data.\n",
    "\n",
    "But best to train a model and see how it goes before making any drastic dataset changes.\n",
    "\n",
    "Because our dataset is small, we could also inspect it via a pandas DataFrame (however, this may not be possible for extremely large datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Set of binoculars placed on a table</td>\n",
       "      <td>not_food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Luxurious coconut shrimp curry on a generous p...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Red brick fireplace with a mantel serving as a...</td>\n",
       "      <td>not_food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Ceiling fan with lights illuminating a bedroom</td>\n",
       "      <td>not_food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>Sweet and savory mango curry with chicken and ...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Sweet and savory sushi roll with ingredients l...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Carrots on a plate, served with a side of crea...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text     label\n",
       "155                Set of binoculars placed on a table  not_food\n",
       "246  Luxurious coconut shrimp curry on a generous p...      food\n",
       "50   Red brick fireplace with a mantel serving as a...  not_food\n",
       "200     Ceiling fan with lights illuminating a bedroom  not_food\n",
       "223  Sweet and savory mango curry with chicken and ...      food\n",
       "72   Sweet and savory sushi roll with ingredients l...      food\n",
       "103  Carrots on a plate, served with a side of crea...      food"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn our dataset into a DataFrame and get a random sample\n",
    "food_not_food_df = pd.DataFrame(dataset[\"train\"])\n",
    "food_not_food_df.sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "food        125\n",
       "not_food    125\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the value counts of the label column\n",
    "food_not_food_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXTxiYra4ahX"
   },
   "source": [
    "## TK - Preparing data for text classification\n",
    "\n",
    "We've got our data ready but there are a few steps we'll need to take before we can model it.\n",
    "\n",
    "The main two being:\n",
    "\n",
    "1. **Tokenization** - turning our text into a numerical representation (machines prefer numbers rather than words), for example, `{\"a\": 0, \"b\": 1, \"c\": 2...}`.\n",
    "2. **Creating a train/test split** - right now our data is in a training split only but we'll create a test set to evaluate our model's performance.\n",
    "\n",
    "These don't necessarily have to be in order either.\n",
    "\n",
    "Before we get to them, let's create a small mapping from our labels to numbers.\n",
    "\n",
    "In the same way we need to tokenize our text into numerical representation, we also need to do the same for our labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TK - Creating a mapping from labels to numbers\n",
    "\n",
    "Our machine learning model will want to see all numbers.\n",
    "\n",
    "This goes for text as well as label input.\n",
    "\n",
    "So let's create a mapping from our labels to numbers.\n",
    "\n",
    "Since we've only got a couple of labels (`\"food\"` and `\"not_food\"`), we can create a dictionary to map them to numbers, however, if you've got a fair few labels, you may want to make this mapping programmatically.\n",
    "\n",
    "We can use these dictionaries later on for our model training as well as evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label to ID mapping: {'not_food': 0, 'food': 1}\n",
      "ID to Label mapping: {0: 'not_food', 1: 'food'}\n"
     ]
    }
   ],
   "source": [
    "# Create mapping from id2label and label2id\n",
    "id2label = {0: \"not_food\", 1: \"food\"}\n",
    "label2id = {\"not_food\": 0, \"food\": 1}\n",
    "\n",
    "print(f\"Label to ID mapping: {label2id}\")\n",
    "print(f\"ID to Label mapping: {id2label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "In a binary classification task, the positive class, in our case `\"food\"`, is usually given the label `1` and the negative class (`\"not_food\"`) is given the label `0`. \n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label to ID mapping: {'not_food': 0, 'food': 1}\n",
      "ID to Label mapping: {0: 'not_food', 1: 'food'}\n"
     ]
    }
   ],
   "source": [
    "# Create mappings programmatically from dataset\n",
    "id2label = {idx: label for idx, label in enumerate(dataset[\"train\"].unique(\"label\")[::-1])} # reverse sort list to have \"not_food\" first\n",
    "label2id = {label: idx for idx, label in id2label.items()}\n",
    "\n",
    "print(f\"Label to ID mapping: {label2id}\")\n",
    "print(f\"ID to Label mapping: {id2label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our dictionary mappings created, we can update the labels of our dataset to be numeric.\n",
    "\n",
    "We can do this using the [`datasets.Dataset.map`](https://huggingface.co/docs/datasets/en/process#map) method and passing it a function to apply to each example.\n",
    "\n",
    "Let's create a small function which turns an example label into a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156,
     "referenced_widgets": [
      "cf51cae23d2b4026a4f890069983da4c",
      "ca79aac952ca41c096241ad371c055fe",
      "0233456210f8469f85e648ba1c1f83ff",
      "c0028b925edf40ecb94ee219a26dfe36",
      "d4a7a1f7a10040ccb07704d845de6cd6",
      "1b61ce44fb554db2bb906fb39b41b7e0",
      "dcc6949983954dfb9b7f8bd0ad6cd6d8",
      "2afc2c8167ad4d15ab6a35c1d1f1f3cb",
      "070785d29788406db9369ce39e24b7cf",
      "b6cb26e5df334ac980005863a38581a1",
      "ae6d2d013c03448887d53f8a0760e77d"
     ]
    },
    "id": "sCX-iCcW2riv",
    "outputId": "eba840c3-7652-41a8-a121-546cb9a8147f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'This is a sentence about my favourite food: honey.', 'label': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn labels into 0 or 1 (e.g. 0 for \"not_food\", 1 for \"food\")\n",
    "def map_labels_to_number(example):\n",
    "  example[\"label\"] = label2id[example[\"label\"]]\n",
    "  return example\n",
    "\n",
    "example_sample = {\"text\": \"This is a sentence about my favourite food: honey.\", \"label\": \"food\"}\n",
    "\n",
    "# Test the function\n",
    "map_labels_to_number(example_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our function works!\n",
    "\n",
    "How about we map it to the whole dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Creamy cauliflower curry with garlic naan, featuring tender cauliflower in a rich sauce with cream and spices, served with garlic naan bread.',\n",
       "  'Set of books stacked on a desk',\n",
       "  'Watching TV together, a family has their dog stretched out on the floor',\n",
       "  'Wooden dresser with a mirror reflecting the room',\n",
       "  'Lawn mower stored in a shed'],\n",
       " 'label': [1, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map our dataset labels to numbers\n",
    "dataset = dataset[\"train\"].map(map_labels_to_number)\n",
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Looks like our labels are all numerical now.\n",
    "\n",
    "We can check a few random samples using [`dataset.shuffle()`](https://huggingface.co/docs/datasets/en/process#shuffle) and indexing for the first few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5XEUYH_31IS",
    "outputId": "8089bfe3-2322-4bac-d2fd-89f626d0a1e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Set of test tubes arranged in a rack',\n",
       "  'Cherry tomatoes and mozzarella balls in a bowl, drizzled with balsamic glaze for a tasty appetizer.',\n",
       "  'Set of napkins dispensed from a dispenser',\n",
       "  'Gluten-free sushi roll using tamari sauce instead of soy sauce.',\n",
       "  'Vibrant red curry with tofu and bell peppers, featuring tofu and sweet bell peppers in a rich coconut milk sauce.'],\n",
       " 'label': [0, 1, 0, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the dataset and view the first 5 samples (will return different results each time) \n",
    "dataset.shuffle()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TK - Split the dataset into training and test sets\n",
    "\n",
    "Right now our dataset only has a training split.\n",
    "\n",
    "However, we'd like to create a test split so we can evaluate our model.\n",
    "\n",
    "In essence, our model will learn patterns (the relationship between text captions and their labels of food/not_food) on the training data.\n",
    "\n",
    "And we will evaluate those learned patterns on the test data.\n",
    "\n",
    "We can split our data using the [`datasets.Dataset.train_test_split`](https://huggingface.co/docs/datasets/v2.20.0/en/package_reference/main_classes#datasets.Dataset.train_test_split) method.\n",
    "\n",
    "We can use the `test_size` parameter to define the percentage of data we'd like to use in our test set (e.g. `test_size=0.2` would mean 20% of the data goes to the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OO53Xx9l3c_l"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create train/test splits\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42) # note: seed isn't needed, just here for reproducibility, without it you will get different splits each time you run the cell\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect!\n",
    "\n",
    "Our dataset has been split into 200 training examples and 50 testing examples.\n",
    "\n",
    "Let's visualize a few random examples to make sure they still look okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIVwtdEw4VHF",
    "outputId": "35f0da00-ab15-42e0-9d81-757ad3da1618"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Random sample from training dataset:\n",
      "Text: Black and white checkered kitchen floor adding a classic touch\n",
      "Label: 0 (not_food)\n",
      "\n",
      "[INFO] Random sample from testing dataset:\n",
      "Text: Artichokes in a bowl, sprinkled with garlic and served with a side of lemon aioli for a tasty, sophisticated dish.\n",
      "Label: 1 (food)\n"
     ]
    }
   ],
   "source": [
    "random_idx_train = random.randint(0, len(dataset[\"train\"]))\n",
    "random_sample_train = dataset[\"train\"][random_idx_train]\n",
    "\n",
    "random_idx_test = random.randint(0, len(dataset[\"test\"]))\n",
    "random_sample_test = dataset[\"test\"][random_idx_test]\n",
    "\n",
    "print(f\"[INFO] Random sample from training dataset:\")\n",
    "print(f\"Text: {random_sample_train['text']}\\nLabel: {random_sample_train['label']} ({id2label[random_sample_train['label']]})\\n\")\n",
    "print(f\"[INFO] Random sample from testing dataset:\")\n",
    "print(f\"Text: {random_sample_test['text']}\\nLabel: {random_sample_test['label']} ({id2label[random_sample_test['label']]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TK - Tokenizing text data\n",
    "\n",
    "Labels numericalized, dataset split, time to turn our text into numbers.\n",
    "\n",
    "Tokenization is the process of converting a non-numerical data source into numbers.\n",
    "\n",
    "Why?\n",
    "\n",
    "Because machines (especially machine learning models) prefer numbers to human-style data.\n",
    "\n",
    "In the case of the text `\"I love pizza\"` a very simple method of tokenization might be to convert each word to a number.\n",
    "\n",
    "For example, `{\"I\": 0, \"love\": 1, \"pizza\": 2}`.\n",
    "\n",
    "However, for most modern machine learning models, the tokenization process is a bit more nuanced.\n",
    "\n",
    "For example, the text `\"I love pizza\"` might be tokenized into something more like `[101, 1045, 2293, 10733, 102]`.\n",
    "\n",
    "TK image - showcase an example using OpenAI's tokenization tool and what this looks like with \"I love pizza\": https://platform.openai.com/tokenizer  \n",
    "\n",
    "::: {.callout-note}\n",
    "Depending on the model you use, the tokenization process could be different. For example, one model might turn `\"I love pizza\"` into `[40, 3021, 23317]`, where as another model might turn it into `[101, 1045, 2293, 10733, 102]`. \n",
    "\n",
    "To deal with this, Hugging Face models often pair models with their own tokenizers by pairing a tokenizer configuration with a model's weights. \n",
    "\n",
    "Such is the case with [`distilbert/distilbert-base-uncased`](https://huggingface.co/distilbert/distilbert-base-uncased) (there is a `tokenizer.json` file as well as a `tokenizer_config.json` file which contains all of the tokenizer implementation details). \n",
    "\n",
    "For more examples of tokenization, you can see OpenAI's [tokenization visualizer tool](https://platform.openai.com/tokenizer) as well as their open-source library [`tiktoken`](https://github.com/openai/tiktoken), Google also have an open-source tokenization library called [`sentencepiece`](https://github.com/google/sentencepiece), finally Hugging Face's [`tokenizers`](https://github.com/huggingface/tokenizers) library is also a great resource (this is what we'll be using behind the scenes).\n",
    ":::\n",
    "\n",
    "Many of the text-based models on Hugging Face come paired with their own tokenizer.\n",
    "\n",
    "For example, the [`distilbert/distilbert-base-uncased`](https://huggingface.co/distilbert/distilbert-base-uncased) model can be used with the `distilbert/distilbert-base-uncased` tokenizer.\n",
    "\n",
    "We can load the tokenizer for a given model using the [`transformers.AutoTokenizer.from_pretrained`](https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes) method and passing it the name of the model we'd like to use.\n",
    "\n",
    "The `transformers.AutoTokenizer` class is part of a series of Auto Classes (such as `AutoConfig`, `AutoModel`, `AutoProcessor`) which automatically loads the correct configuration settings for a given model.\n",
    "\n",
    "Let's load the tokenizer for the `distilbert/distilbert-base-uncased` model and see how it works.\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "Why use the `distilbert/distilbert-base-uncased` model?\n",
    "\n",
    "The short answer is that I've used it before and it works well (and fast) on various text classification tasks.\n",
    "\n",
    "The longer answer is that Hugging Face has many available open-source models for many different problems available at [https://huggingface.co/models](https://huggingface.co/models).\n",
    "\n",
    "Navigating these models can take some practice.\n",
    "\n",
    "And several models may be suited for the same task (though with various tradeoffs such as size and speed).\n",
    "\n",
    "However, overtime and with adequate experimentation, you'll start to build an intuition on which models are good for which problems.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201,
     "referenced_widgets": [
      "6cdf8319c7184ee5b107cea1718b03c6",
      "4ef091cf8c084af1acb95d62ffe1f0f2",
      "35205e647f814b97a2e977af48cc7a17",
      "0eaf7b7abd71413597e0533efa3feda2",
      "34edb9945bb54a69868d62ecd5b1ae56",
      "495cd338f7614f7a9a68fc3dc3916dcf",
      "dc17c88c62244e2f9a574ef895132a83",
      "8580f622bac8422a992b56b5f39b3693",
      "ca74fdefa7844f11b1d9052269efd585",
      "c56c3c1ff2a040b0952b5bac655b71dc",
      "3d5b841a59a3489ea5b4112182d4ae70",
      "f1fa4e09b42c4904b9be033d33943e05",
      "470f2248b2444869914197c7481e10ae",
      "c6cbefbc4f4d4cfe98400aa05370e3e7",
      "6a7b459551104d47a90527c885647830",
      "d0080133b35c40578eba5db18dc82db6",
      "54e1a02cf71743ec9da128ff6c882f34",
      "6429d7801ba14ed4861726bd7832ecee",
      "d0c86013f8194633be0ac27ca9b8876c",
      "7b91cb6b9b5e4ba891a6cfaab2d5948b",
      "4f8713c60a0947ae86ee9d1d4de8ab3a",
      "4b6f81607c31496a83d5964983fed168",
      "c4055bbdeb6c4833b4fb50970248a154",
      "31cf1f106af54f7986b4d2f462125cca",
      "1c5d45fa6fb14097bf36584ec7ef651c",
      "1aa8fdd087ce42bdb16ae19a38e79b27",
      "ef3b72dea05d404db610152cb17ba8f9",
      "506baa608274455eae12e54e1d7b7dd5",
      "103ff0dd04da40a69d85763bbf571490",
      "9ccbc146f4ad4382879b5d8b526cac4e",
      "fda99413adc8497cba92a9c6416f77ad",
      "80ef815be4ac4b64b3b3d99a6c43fadb",
      "d1cc110825034545a2532fc567b5325e",
      "ec74087b5c9d4e538675956af72ae138",
      "fe87ce116db84d5ba3d00e4a8c249dc1",
      "c45933bed8084b53b562621699870695",
      "72df691ec7784ae7b0542171a7789555",
      "4c92f17cff2944a9a2a49be56649d81e",
      "20a883d10ea945a9889429c5efb76a43",
      "2f44328dc8a04f84a59e26d08d343c77",
      "37cede645a5b48a9b89df987b9ef91e2",
      "13b1494b02194773a5fafe39b211c88a",
      "39f3c0ec5b4446cda93df7a999d4fd41",
      "bb7f6dabc3ad4779a87b211227040606"
     ]
    },
    "id": "EfFLh-Sw4ifl",
    "outputId": "f277b622-267d-4d06-b2cb-6bc204b2f62d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n",
    "                                          use_fast=True) # uses fast tokenization (backed by tokenziers library and implemented in Rust) by default, if not available will default to Python implementation\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!\n",
    "\n",
    "There's our tokenizer!\n",
    "\n",
    "It's an instance of the [`transformers.DistilBertTokenizerFast`](https://huggingface.co/docs/transformers/v4.41.3/en/model_doc/distilbert#transformers.DistilBertTokenizerFast) class.\n",
    "\n",
    "You can read more about it in the documentation.\n",
    "\n",
    "For now, let's try it out by passing it a string of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2293, 10733, 102], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test out tokenizer\n",
    "tokenizer(\"I love pizza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2293, 10733, 999, 102], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try adding a \"!\" at the end\n",
    "tokenizer(\"I love pizza!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo!\n",
    "\n",
    "Our text gets turned into numbers (or tokens).\n",
    "\n",
    "Notice how with even a slight change in the text, the tokenizer produces different results?\n",
    "\n",
    "The `input_ids` are our tokens.\n",
    "\n",
    "And the `attention_mask` (in our case, all `[1, 1, 1, 1, 1, 1]`) is a mask which tells the model which tokens to use or not. Tokens with a mask value of `1` get used and tokens with a mask value of `0` get ignored.\n",
    "\n",
    "There are several attributes of the `tokenizer` we can explore.\n",
    "\n",
    "* `tokenizer.vocab` will return the vocabulary of the tokenizer or in other words, the unique words/word pieces the tokenizer is capable of converting into numbers.\n",
    "* `tokenizer.model_max_length` will return the maximum length of a sequence the tokenizer can process, pass anything longer than this and the sequence will be truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of tokenizer vocabulary: 30522\n",
      "Max tokenizer input sequence length: 512\n"
     ]
    }
   ],
   "source": [
    "# Get the length of the vocabulary \n",
    "length_of_tokenizer_vocab = len(tokenizer.vocab)\n",
    "print(f\"Length of tokenizer vocabulary: {length_of_tokenizer_vocab}\")\n",
    "\n",
    "# Get the maximum sequence length the tokenizer can handle\n",
    "max_tokenizer_input_sequence_length = tokenizer.model_max_length\n",
    "print(f\"Max tokenizer input sequence length: {max_tokenizer_input_sequence_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah, looks like our tokenizer has a vocabulary of `30,522` different words and word pieces.\n",
    "\n",
    "And it can handle a sequence length of up to `512` (any sequence longer than this will be automatically truncated from the end).\n",
    "\n",
    "Let's check out some of the vocab.\n",
    "\n",
    "Can I find my own name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3817"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does \"daniel\" occur in the vocab?\n",
    "tokenizer.vocab[\"daniel\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oooh, looks like my name is `3817` in the tokenizer's vocab.\n",
    "\n",
    "Can you find your own name? (note: there may be an error if the token doesn't exist, we'll get to this)\n",
    "\n",
    "How about \"pizza\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10733"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab[\"pizza\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if a word doesn't exist in the vocab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'akash'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43makash\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'akash'"
     ]
    }
   ],
   "source": [
    "tokenizer.vocab[\"akash\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dam, we get a `KeyError`.\n",
    "\n",
    "Not to worry, this is okay, since when calling the `tokenizer` on the word, it will automatically split the word into word pieces or subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 9875, 4095, 102], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"akash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works!\n",
    "\n",
    "We can check what word pieces `\"akash\"` got broken into with [`tokenizer.convert_ids_to_tokens(input_ids)`](https://huggingface.co/docs/transformers/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.convert_ids_to_tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'aka', '##sh', '[SEP]']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer(\"akash\").input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahhh, it seems `\"akash\"` was split into two tokens, `[\"aka\", \"##sh\"]`.\n",
    "\n",
    "The `\"##\"` at the start of `\"##sh\"` means that the sequence is part of a larger sequence. \n",
    "\n",
    "And the `\"[CLS]\"` and `\"[SEP]\"` tokens are special tokens indicating the start and end of a sequence.\n",
    "\n",
    "Now, since tokenizers can deal with any text, what if there was an unknown token?\n",
    "\n",
    "For example, rather than `\"pizza\"` someone used the pizza emoji 🍕?\n",
    "\n",
    "Let's try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '[UNK]', '[SEP]']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try to tokenize an emoji\n",
    "tokenizer.convert_ids_to_tokens(tokenizer(\"🍕\").input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahh, we get the special `\"[UNK]\"` token.\n",
    "\n",
    "This stands for \"unknown\".\n",
    "\n",
    "The combination of word pieces and `\"[UNK]\"` special token means that our `tokenizer` will be able to turn almost any text into numbers for our model.\n",
    "\n",
    "::: {.callout-note}\n",
    "Keep in mind that just because one tokenizer uses an unknown special token for a particular word or emoji (🍕) doesn't mean another will.\n",
    ":::\n",
    "\n",
    "Since the `tokenizer.vocab` is a Python dictionary, we can get a sample of the vocabulary using `tokenizer.vocab.items()`.\n",
    "\n",
    "How about we get the first 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('!', 999), ('\"', 1000), ('#', 1001), ('##!', 29612), ('##\"', 29613)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the first 5 items in the tokenizer vocab\n",
    "sorted(tokenizer.vocab.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's our `'!'` from before! Looks like the first five items are all related to punctuation points.\n",
    "\n",
    "How about a random sample of tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('seal', 7744),\n",
       " ('giant', 5016),\n",
       " ('22', 2570),\n",
       " ('munich', 7469),\n",
       " ('555', 29541)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.sample(sorted(tokenizer.vocab.items()), k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TK - Making a preprocessing function to tokenize text\n",
    "\n",
    "Rather than tokenizing our texts one by one, it's best practice to define a preprocessing function which does it for us.\n",
    "\n",
    "This process works regardless of whether you're working with text data or other kinds of data such as images or audio.\n",
    "\n",
    "::: {.callout-tip}\n",
    "## Turning data into numbers\n",
    "\n",
    "For any kind of machine learning workflow, an important first step is turning your input data into numbers.\n",
    "\n",
    "As machine learning models are algorithms which find patterns in numbers, before they can find patterns in your data (text, images, audio, tables) it must be numerically encoded first (e.g. tokenizing text).\n",
    "\n",
    "To help with this, `transformers` has an [`AutoProcessor`](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoProcessor) class which can preprocess data in a specific format required for a paired model.\n",
    ":::\n",
    "\n",
    "To prepare our text data, let's create a preprocessing function to take in a dictionary which contains the key `\"text\"` which has the value of a target string (our data samples come in the form of dictionaries) and then returns the tokenized `\"text\"`.\n",
    "\n",
    "We'll set the following parameters in our `tokenizer`:\n",
    "\n",
    "* `padding=True` - This will make all the sequences in a batch the same length by padding shorter sequences with 0's until they equal the longest size in the batch. Why? If there are different size sequences in a batch, you can sometimes run into dimensionality issues.\n",
    "* `truncation=True` - This will shorten sequences longer than the model can handle to the model's max input size (e.g. if a sequence is 1000 long and the model can handle 512, it will be shortened to 512 via removing all tokens after 512).\n",
    "\n",
    "You can see more parameters available for the `tokenizer` in the [`transformers.PreTrainedTokenizer` documentation](https://huggingface.co/docs/transformers/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).\n",
    "\n",
    "::: {.callout-note}\n",
    "For more on padding and truncation (two important concepts in sequence processing), I'd recommend reading the Hugging Face documentation on [Padding and Truncation](https://huggingface.co/docs/transformers/en/pad_truncation).\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(examples):\n",
    "    \"\"\"\n",
    "    Tokenize given example text and return the tokenized text.\n",
    "    \"\"\"\n",
    "    return tokenizer(examples[\"text\"],\n",
    "                     padding=True, # pad short sequences to longest sequence in the batch\n",
    "                     truncation=True) # truncate long sequences to the maximum length the model can handle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful!\n",
    "\n",
    "Now let's try it out on an example sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2293, 10733, 102], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sample_2 = {\"text\": \"I love pizza\", \"label\": 1}\n",
    "\n",
    "# Test the function\n",
    "tokenize_text(example_sample_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good!\n",
    "\n",
    "How about we map our `tokenize_text` function to our whole `dataset`?\n",
    "\n",
    "We can do so with the [`datasets.Dataset.map` method](https://huggingface.co/docs/datasets/v2.20.0/en/package_reference/main_classes#datasets.Dataset.map).\n",
    "\n",
    "The `map` method allows us to apply a given function to all examples in a dataset.\n",
    "\n",
    "By setting `batched=True` we can apply the given function to batches of examples (many at a time) to speed up computation time.\n",
    "\n",
    "Let's create a `tokenized_dataset` object by calling `map` on our `dataset` and passing it our `tokenize_text` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260,
     "referenced_widgets": [
      "985174675f354a9bb2580a70ad2b7625",
      "b9e83e2c1d2f4970a0905efde0055e3d",
      "8c3f85da75764955a572261460db619b",
      "80995fb6ed6f4557b1037dea2798b2fb",
      "016181d9f51a4abea666046fb95f2aee",
      "5b1fd51b695b4c81ab351ed5b1f92966",
      "a5076ac1d4ad4c09aa1e6f12449915af",
      "e2f32fe9433d458dbc278da438f27449",
      "64513b9e43254d5a9bbbffc9cb9a054c",
      "eb256ba48acd446bbe6b832c9558f7c6",
      "4ef76858d67f4ee6a99b305edb682a43",
      "9455512d944c418b87e4d1fe324619e6",
      "b93a51136d1146e08914db125d0259fb",
      "c74948f6d29e4e0e868a3ccf836d2377",
      "522f6c1e5e7340039b7bbf700704699f",
      "0673f25c77b04212bc2b63f56454a5d2",
      "054e759a27a442afa8c666fc3d4c3f2e",
      "e4633e912a57492497bc2fbfad02351c",
      "79144b8d1ec44b6bb3dcc381f7e829ce",
      "60e84d7134b646f6b291b0439d9ec890",
      "6294e85e97a8495bb18d11dff1991a79",
      "91ee20aae0474a4c90a26463f72bacf1"
     ]
    },
    "id": "KorM6An04xXH",
    "outputId": "d72b4325-b4de-4f65-f930-50007d45c8d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset.map() docs -> https://huggingface.co/docs/datasets/v2.20.0/en/package_reference/main_classes#datasets.Dataset.map \n",
    "tokenized_dataset = dataset.map(function=tokenize_text, \n",
    "                                batched=True, # set batched=True to operate across batches of examples rather than only single examples\n",
    "                                batch_size=1000) # defaults to 1000, can be increased if you have a large dataset\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset tokenized!\n",
    "\n",
    "Let's inspect a pair of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6GihQUpk5C2w",
    "outputId": "c1b0b958-b9a5-4274-e8c1-eaf0ac4cdf3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Key: text\n",
      "Train sample: Set of headphones placed on a desk\n",
      "Test sample: A slice of pepperoni pizza with a layer of melted cheese\n",
      "\n",
      "[INFO] Key: label\n",
      "Train sample: 0\n",
      "Test sample: 1\n",
      "\n",
      "[INFO] Key: input_ids\n",
      "Train sample: [101, 2275, 1997, 2132, 19093, 2872, 2006, 1037, 4624, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Test sample: [101, 1037, 14704, 1997, 11565, 10698, 10733, 2007, 1037, 6741, 1997, 12501, 8808, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "[INFO] Key: attention_mask\n",
      "Train sample: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Test sample: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get two samples from the tokenized dataset\n",
    "train_tokenized_sample = tokenized_dataset[\"train\"][0]\n",
    "test_tokenized_sample = tokenized_dataset[\"test\"][0]\n",
    "\n",
    "for key in train_tokenized_sample.keys():\n",
    "    print(f\"[INFO] Key: {key}\")\n",
    "    print(f\"Train sample: {train_tokenized_sample[key]}\")\n",
    "    print(f\"Test sample: {test_tokenized_sample[key]}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful! Our samples have been tokenized.\n",
    "\n",
    "Notice the zeroes on the end of the `inpud_ids` and `attention_mask` values.\n",
    "\n",
    "These are padding tokens to ensure that each sample has the same length as the longest sequence in a given batch.\n",
    "\n",
    "We can now use these tokenized samples later on in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization takeaways\n",
    "\n",
    "We've seen tokenizers in practice.\n",
    "\n",
    "A few takeaways before we start to build a model:\n",
    "\n",
    "* Tokenizers are used to turn text (or other forms of data such as images and audio) into a numerical representation ready to be used with a machine learning model.\n",
    "* Many models reuse existing tokenizers and many models have their own specific tokenizer paired with them. Hugging Face's `transformers.AutoTokenizer`, `transformers.AutoProcessor` and `transformers.AutoModel` classes make it easy to pair tokenizers and models based on their name (e.g. `distilbert/distilbert-base-uncased`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVb02xAu5I0x"
   },
   "source": [
    "## TK - Setting up an evaluation metric\n",
    "\n",
    "Aside from training a model, one of the most important steps in machine learning is evaluating a model.\n",
    "\n",
    "To do, we can use evaluation metrics.\n",
    "\n",
    "There are many different kinds of evaluation metrics for various problems.\n",
    "\n",
    "But since we're focused on text classification, we'll use [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification) as our evaluation metric.\n",
    "\n",
    "\n",
    "A model which gets 99/100 predictions correct has an accuracy of 99%.\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{correct classifications}}{\\text{all classifications}}\n",
    "$$\n",
    "\n",
    "For some projects, you may have a minimum standard of a metric.\n",
    "\n",
    "For example, when I worked on an insurance claim classification model, the clients required over 98% accuracy for it to be viable to use in production. \n",
    "\n",
    "We can craft these evaluation metrics ourselves.\n",
    "\n",
    "However, Hugging Face has a library called [`evaluate`](https://huggingface.co/docs/evaluate/en/index) which has various metrics built in ready to use.\n",
    "\n",
    "We can load a metric using `evaluate.load(\"METRIC_NAME\")`.\n",
    "\n",
    "Let's load in `\"accuracy\"` and build a function to measure accuracy by comparing arrays of predictions and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "90455578408b4f33bc9a9c732d431236",
      "a2e056a63fd34329a21330a4fc69429d",
      "347ce846f3564c58ac1eb26ae2f3adc4",
      "9a01bb7c8f5e4940ba2444fa63a3db6f",
      "b8beefe086284c7297004fcad9303617",
      "593b4aaa5c9e4e1898cc4d5064940770",
      "4df9f85ae9e943ceb3ed1a5d9e143f5c",
      "a20bdb762700449097d5010ce7d29921",
      "2cb043cfb0fc448e81a34710d65bb691",
      "b8d8eac2274a4040a4163663896753f7",
      "01f82250d2e14f198dd0f561969d54ff"
     ]
    },
    "id": "D06NRS1-5S_P",
    "outputId": "4a2b0f85-db47-4f0f-cbc3-fa148c19a646"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_accuracy(predictions_and_labels: Tuple[np.array, np.array]):\n",
    "  \"\"\"\n",
    "  Computes the accuracy of a model by comparing the predictions and labels.\n",
    "  \"\"\"\n",
    "  predictions, labels = predictions_and_labels\n",
    "\n",
    "  # Get highest prediction probability of each prediction if predictions are probabilities\n",
    "  if len(predictions.shape) >= 2:\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "  return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy function created!\n",
    "\n",
    "Now let's test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy when all predictions are correct: {'accuracy': 1.0}\n",
      "Accuracy when one prediction is wrong: {'accuracy': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# Create example list of predictions and labels\n",
    "example_predictions_all_correct = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "example_predictions_one_wrong = np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
    "example_labels = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "# Test the function\n",
    "print(f\"Accuracy when all predictions are correct: {compute_accuracy((example_predictions_all_correct, example_labels))}\")\n",
    "print(f\"Accuracy when one prediction is wrong: {compute_accuracy((example_predictions_one_wrong, example_labels))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, our function works just as we'd like.\n",
    "\n",
    "When all predictions are correct, it scores 1.0 (or 100% accuracy) and when 9/10 predictions are correct, it returns 0.9 (or 90% accuracy).\n",
    "\n",
    "We can use this function during training and evaluation of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38IFAZTl5-iI"
   },
   "source": [
    "## TK - Setting up a model for training\n",
    "\n",
    "We've gone through the important steps of setting data up for training (and evaluation).\n",
    "\n",
    "Now let's prepare a model.\n",
    "\n",
    "We'll go through the following steps:\n",
    "\n",
    "1. Create and preprocess data (done ✅).\n",
    "2. Define the model we'd like use with [`transformers.AutoModelForSequenceClassification`](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForSequenceClassification) (or another similar model class).\n",
    "3. Define training arguments (these are hyperparameters for our model) with [`transformers.TrainingArguments`](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments).\n",
    "4. Pass `TrainingArguments` from 3 and target datasets to an instance of [`transformers.Trainer`](https://huggingface.co/docs/transformers/en/main_classes/trainer).\n",
    "5. Train the model by calling [`Trainer.train()`](https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/trainer#transformers.Trainer.train).\n",
    "6. Save the model.\n",
    "7. Evaluate results.\n",
    "\n",
    "* TK image - steps for training in Hugging Face\n",
    "\n",
    "Let's start by creating an instance of a model.\n",
    "\n",
    "Since we're working on text classification, we'll do so with `transformers.AutoModelForSequenceClassification` (where sequence classification means a sequence of something, e.g. our sequences of text).\n",
    "\n",
    "We can use the `from_pretrained()` method to instatiate a pretrained model from the Hugging Face Hub.\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "The \"pretrained\" in [`transformers.AutoModelForSequenceClassification.from_pretrained`](https://huggingface.co/docs/transformers/v4.42.0/en/model_doc/auto#transformers.AutoConfig.from_pretrained) means acquiring a model which has already been trained on a certain dataset.\n",
    "\n",
    "This is common practice in many machine learning projects and is known as **transfer learning**.\n",
    "\n",
    "The idea is to take an existing model which works well on a task similar to your target task and then **fine-tune** it to work even better on your target task.\n",
    "\n",
    "In our case, we're going to use the pretrained DistilBERT base model ([`distilbert/distilbert-base-uncased`](https://huggingface.co/distilbert/distilbert-base-uncased)) which has been trained on [many thousands of books](https://huggingface.co/datasets/bookcorpus/bookcorpus) as well as a version of the [English Wikipedia](https://huggingface.co/datasets/legacy-datasets/wikipedia) (millions of words).\n",
    "\n",
    "This training gives it a very good baseline representation of the patterns in language.\n",
    "\n",
    "We'll take this baseline representation of the patterns in language and adjust it slightly to focus specifically on predicting whether an image caption is about food or not (based on the words it contains).\n",
    "\n",
    "The main two benefits of using transfer learning are:\n",
    "\n",
    "1. Ability to get good results with smaller amounts of data (since the main representations are learned on a larger dataset, we only have to show the model a few examples of our specific problem).\n",
    "2. This process can be repeated acorss various domains and tasks. For example, you can take a computer vision model trained on millions of images and customize it to your own use case. Or an audio model trained on many different nature sounds and customize it specifically for birds.\n",
    "\n",
    "So when starting a new machine learning project, one of the first questions you should ask is: does an existing pretrained model similar to my task exist and can I fine-tune it for my own task?\n",
    "\n",
    "For an end-to-end example of transfer learning in PyTorch (another popular deep learning framework), see [PyTorch Transfer Learning](https://www.learnpytorch.io/06_pytorch_transfer_learning/).\n",
    "\n",
    ":::\n",
    "\n",
    "Time to setup our `model` instance.\n",
    "\n",
    "A few things to note:\n",
    "* We'll use [`transformers.AutoModelForSequenceClassification.from_pretrained`](https://huggingface.co/docs/transformers/v4.42.0/en/model_doc/auto#transformers.AutoConfig.from_pretrained), this will create the model architecture we specify with the `pretrained_model_name_or_path` parameter. \n",
    "* The `AutoModelForSequenceClassification` class comes with a classification head on top of our mdoel (so we can customize this to the number of classes we have with the `num_labels` parameter).\n",
    "* Using `from_pretrained` will also call the [`transformers.PretrainedConfig`](https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/configuration#transformers.PretrainedConfig) class which will enable us to set `id2label` and `label2id` parameters for our fine-tuning task.\n",
    "\n",
    "Let's refresh what our `id2label` and `label2id` objects look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id2label: {0: 'not_food', 1: 'food'}\n",
      "label2id: {'not_food': 0, 'food': 1}\n"
     ]
    }
   ],
   "source": [
    "# Get id and label mappings\n",
    "print(f\"id2label: {id2label}\")\n",
    "print(f\"label2id: {label2id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "c3faf065f2d2435697fb730bf04ff1cb",
      "8fc36b3359d24eb987c4be9eaf5d4e95",
      "beb76be168f840d79c950efc4bdfc8d1",
      "63e64e93abca400293b7b7742bd94aaf",
      "d41ef095cc5b4c23b31236b501dcf8c1",
      "d6b2772488464e80b333332d6cd46eeb",
      "5616046a1d1347cfbbe82fa67b79af1a",
      "f8d378a142934482aa04a608ada9acdb",
      "b6c20dee590f40e5b2c96beefc1643e1",
      "b791095b95214d82af1e344a2b5898a2",
      "a3fa870efa0f4463b1e60fa106f782f1"
     ]
    },
    "id": "c5c6bxLV53D5",
    "outputId": "ed01beb4-3cf7-4209-eaea-444e5aa51c5b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Setup model for fine-tuning with classification head (top layers of network)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n",
    "    num_labels=2, # can customize this to the number of classes in your dataset\n",
    "    id2label=id2label, # mappings from class IDs to the class labels (for classification tasks)\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model created!\n",
    "\n",
    "You'll notice that a warning message gets displayed: \n",
    "\n",
    "> Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
    "> You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "This is essentially saying \"hey, some of the layers in this model are newly initialized (with random patterns) and you should probably customize them to your own dataset\".\n",
    "\n",
    "This happens because we used the `AutoModelForSequenceClassification` class.\n",
    "\n",
    "Whilst the majority of the layers in our model have already learned patterns from a large corpus of text, the top layers (classifier layers) have been randomly setup so we can customize them on our own.\n",
    "\n",
    "Let's try and make a prediction with our model and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DistilBertForSequenceClassification.forward() got an unexpected keyword argument 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Try and make a prediction with the loaded model (this will error)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: DistilBertForSequenceClassification.forward() got an unexpected keyword argument 'text'"
     ]
    }
   ],
   "source": [
    "# Try and make a prediction with the loaded model (this will error)\n",
    "model(**tokenized_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! We get an error.\n",
    "\n",
    "Not to worry, this is only because our model hasn't been trained on our own dataset yet.\n",
    "\n",
    "Let's take a look at the layers in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the model \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TK image - show what it looks like when fine-tuning a model for a specific task, e.g. backbone is pre-trained layers, classification head is swapped out\n",
    "\n",
    "You'll notice that the model comes in 3 main parts (data flows through these sequentially):\n",
    "\n",
    "1. `embeddings` - This part of the model turns the input tokens into a learned representation. So rather than just a list of integers, the values become a learned representation. This learned representation comes from the base model learning how different words and word pieces relate to eachother thanks to its training data. The size of `(30522, 768)` means the `30,522` words in the vocabulary are all represented by vectors of size `768` (one word gets represented by 768 numbers, these are often not human interpretable).\n",
    "2. `transformer` - This is the main body of the model. There are several `TransformerBlock` layers stacked on top of each other. These layers attempt to learn a deeper representation of the data going through the model. A thorough breakdown of these layers is beyond the scope of this tutorial, however, for and in-depth guide on Transformer-based models, I'd recommend reading [*Transformers from scratch*](https://peterbloem.nl/blog/transformers) by Peter Bloem, going through [Andrej Karpathy's lecture on Transformers and their history](https://www.youtube.com/watch?v=XfpMkf4rD6E) or reading the original [*Attention is all you need*](https://arxiv.org/abs/1706.03762) paper (this is the paper that introduced the Transformer architecture).\n",
    "3. `classifier` - This is what is going to take the representation of the data and compress it into our number of target classes (notice `out_features=2`, this means that we'll get two output numbers, one for each of our classes).\n",
    "\n",
    "For more on the entire DistilBert architecture and its training setup, I'd recommend reading the [*DistilBert paper*](https://arxiv.org/abs/1910.01108) from the Hugging Face team.\n",
    "\n",
    "Rather than breakdown the model itself, we're focused on using it for a particular task (classifying text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TK - Counting the parameters of our model\n",
    "\n",
    "Before we move into training, we can get another insight into our model by counting its number of parameters.\n",
    "\n",
    "Let's create a small function to count the number of trainable (these will update during training) and total parameters in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trainable_parameters': 66955010, 'total_parameters': 66955010}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_params(model):\n",
    "    \"\"\"\n",
    "    Count the parameters of a PyTorch model.\n",
    "    \"\"\"\n",
    "    trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_parameters = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    return {\"trainable_parameters\": trainable_parameters, \"total_parameters\": total_parameters}\n",
    "\n",
    "# Count the parameters of the model\n",
    "count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!\n",
    "\n",
    "Looks like our model has a total of 66,955,010 parameters and all of them are trainable.\n",
    "\n",
    "A parameter is a numerical value in a model which is capable of being updated to better represent the input data.\n",
    "\n",
    "I like to think of them as a small opportunity to learn patterns in the data.\n",
    "\n",
    "If a model has three parameters, it has three small opportunities to learn patterns in the data.\n",
    "\n",
    "Whereas, if a model has 60,000,000+ (60M) parameters (like our `model`), it has 60,000,000+ small opportunities to learn patterns in the data. \n",
    "\n",
    "Some models such as Large Language Models (LLMs) like [Llama 3 70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B) have 70,000,000,000+ (70B) parameters (over 1000x our model).\n",
    "\n",
    "In essence, the more parameters a model has, the more opportunities it has to learn (generally).\n",
    "\n",
    "More parameters often results in more capabilities.\n",
    "\n",
    "However, more parameters also often results in a much larger model size (e.g. multiple gigabytes versus hundreds of megabytes) as well as a much longer compute time (less samples per second).\n",
    "\n",
    "For our use case, a binary text classification task, 60M parameters is more than enough.\n",
    "\n",
    "::: {.callout-note}\n",
    "Why count the parameters in a model?\n",
    "\n",
    "While it may be tempting to always go with a model that has the most parameters, there are many considerations to take into account before doing so.\n",
    "\n",
    "> What hardware is the model going to run on?\n",
    "\n",
    "If you need the model to run on cheap hardware, you'll likely want a smaller model.\n",
    "\n",
    "> How fast do you need the model to be?\n",
    "\n",
    "If you need 100-1000s of predictions per second, you'll likely want a smaller model.\n",
    "\n",
    "> \"I don't mind about speed or cost, I just want quality.\"\n",
    "\n",
    "Go with the biggest model you can.\n",
    "\n",
    "However, often times you can get really good results by training a small model to do a specific task using quality data than by just always using a large model.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TK - Create a directory for saving models\n",
    "\n",
    "Training a model can take a while.\n",
    "\n",
    "So we'll want a place to save our models.\n",
    "\n",
    "Let's create a directory called `\"learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"` (it's a bit verbose and you can change this if you like but I like to be specific). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model output directory\n",
    "from pathlib import Path\n",
    "\n",
    "# Create models directory\n",
    "models_dir = Path(\"models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create model save name\n",
    "model_save_name = \"learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n",
    "\n",
    "# Create model save path\n",
    "model_save_dir = Path(models_dir, model_save_name)\n",
    "\n",
    "model_save_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TK - Setting up training arguments with TrainingArguments\n",
    "\n",
    "Time to get our model ready for training!\n",
    "\n",
    "We're up to step 3 of our process:\n",
    "\n",
    "1. Create and preprocess data (done ✅).\n",
    "2. Define the model we'd like use with [`transformers.AutoModelForSequenceClassification`](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForSequenceClassification) (or another similar model class) (done ✅). \n",
    "3. Define training arguments (these are hyperparameters for our model) with [`transformers.TrainingArguments`](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments).\n",
    "4. Pass `TrainingArguments` from 3 and target datasets to an instance of [`transformers.Trainer`](https://huggingface.co/docs/transformers/en/main_classes/trainer).\n",
    "5. Train the model by calling [`Trainer.train()`](https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/trainer#transformers.Trainer.train).\n",
    "6. Save the model.\n",
    "7. Evaluate results.\n",
    "\n",
    "The `transformers.TrainingArguments` class contains a series of helpful items, including hyperparameter settings and model saving strategies to use throughout training.\n",
    "\n",
    "It has many parameters, too many to explain here.\n",
    "\n",
    "However, the following table breaks down a helpful handful.\n",
    "\n",
    "Some of the parameters we'll set are the same as the defaults (this is on purpose as the defaults are often pretty good), some such as `learning_rate` are different.\n",
    "\n",
    "| Parameter | Explanation |\n",
    "|:-----|:-----|\n",
    "| `output_dir` | Name of output directory to save the model and checkpoints to. For example, 'learn_hf_food_not_food_text_classifier_model'.  |\n",
    "| `learning_rate` | Value of the initial learning rate to use during training. Passed to [`transformers.AdamW`](https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/trainer#transformers.TrainingArguments). Initial learning rate because the learning rate can be dynamic during training. The ideal learning is experimental in nature. Defaults to `5e-5` or `0.00001` but we'll use `0.0001`.  |\n",
    "| `per_device_train_batch_size`| Size of batches to place on target device during training. For example, a batch size of `32` means the model will look at 32 samples at a time. A batch size too large will result in out of memory issues (e.g. your GPU can't handle holding a large number of samples in memory at a time). |\n",
    "| `per_device_eval_batch_size` | Size of batches to place on target device during evaluation. Can often be larger than during training because no gradients are being calculated. For example, training batch size could be 32 where as evaluation batch size may be able to be 128 (4x larger). Though these are only esitmates. |\n",
    "| `num_train_epochs` | Number of times to pass over the data to try and learn patterns. For example, if `num_train_epochs=10`, the model will do 10 full passes of the training data. Because we're working with a small dataset, 10 epochs should be fine to begin with. However, if you had a larger dataset, you may want to do a few experiments using less data (e.g. 10% of the data) for a smaller number of epochs to make sure things work. |\n",
    "| `eval_strategy` | When to evaluate the model on the evaluation data. If `eval_strategy=\"epoch\"`, the model will be evaluated every epoch. See the documentation for more options. **Note:** This was previously called `evaluation_strategy` but was shortened in `transformers==4.46`. |\n",
    "| `save_strategy` | When to save a model checkpoint. If `save_strategy=\"epoch\"`, a checkpoint will be saved every epoch. See the documentation for more save options. |\n",
    "| `save_total_limit` | Number of total amount of checkpoints to save (so we don't save `num_train_epochs` checkpoints). For example, can limit to 3 saves so the total number of saves are the 3 most recent as well as the best performing checkpoint (as per `load_best_model_at_end`). |\n",
    "| `use_cpu` | Set to `False` by default, will use CUDA GPU (`torch.device(\"cuda\")`) or MPS device (`torch.device(\"mps\")`, for Mac) if available. This is because training is generally faster on an accelerator device. |\n",
    "| `seed` | Set to `42` by default for reproducibility. Meaning that subsequent runs with the same setup should achieve the same results. |\n",
    "| `load_best_model_at_end` | When set to `True`, makes sure that the best model found during training is loaded when training finishes. This will mean the best model checkpoint gets saved regardless of what epoch it happened on. This is set to `False` by default. |\n",
    "| `logging_strategy` | When to log the training results and metrics. For example, if `logging_strategy=\"epoch\"`, results will be logged as outputs every epoch. See the documentation for more logging options.   |\n",
    "| `report_to` | Log experiments to various experiment tracking services. For example, you can log to Weights & Biases using `report_to=\"wandb\"`. We'll turn this off for now and keep logging to a local directory by setting `report_to=\"none\"`. |\n",
    "| `push_to_hub` | Automatically upload the model to the Hugging Face Hub every time the model is saved. We'll set `push_to_hub=False` as we'll see how to do this manually later on. See the documentation for more options on saving models to the Hugging Face Hub. |\n",
    "| `hub_token` | Add your Hugging Face Hub token to push a model to the Hugging Face Hub with `push_to_hub` (will default to [`huggingface-cli login`](https://huggingface.co/docs/huggingface_hub/en/guides/cli) details). |\n",
    "| `hub_private_repo` | Whether or not to make the Hugging Face Hub repository private or public, defaults to `False` (e.g. set to `True` if you want the repository to be private). |\n",
    "\n",
    "::: {.callout-note}\n",
    "To get more familiar with the `transformers.TrainingArguments` class, I'd highly recommend [reading the documentation](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments) for 15-20 minutes. Perhaps over a couple of sessions. There are quite a large number of parameters which will be helpful to be aware of.\n",
    ":::\n",
    "\n",
    "Phew!\n",
    "\n",
    "That was a lot to take in.\n",
    "\n",
    "But let's now practice setting up our own instance of `transformers.TrainingArguments`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "sqNdIIJd6QQO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving model checkpoints to: models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "print(f\"[INFO] Saving model checkpoints to: {model_save_dir}\")\n",
    "\n",
    "# Create training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_save_dir,\n",
    "    learning_rate=0.0001,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\", # was previously \"evaluation_strategy\"\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3, # limit the total amount of save checkpoints (so we don't save num_epochs checkpoints)\n",
    "    use_cpu=False, # set to False by default, will use CUDA GPU or MPS device if available\n",
    "    seed=42, # set to 42 by default for reproducibility\n",
    "    load_best_model_at_end=True, # load the best model when finished training\n",
    "    logging_strategy=\"epoch\", # log training results every epoch\n",
    "    report_to=\"none\", # optional: log experiments to Weights & Biases/other similar experimenting tracking services (we'll turn this off for now) \n",
    "    # push_to_hub=True # optional: automatically upload the model to the Hub (we'll do this manually later on)\n",
    "    # hub_token=\"your_token_here\" # optional: add your Hugging Face Hub token to push to the Hub (will default to huggingface-cli login)\n",
    "    hub_private_repo=False # optional: make the uploaded model private (defaults to False)\n",
    ")\n",
    "\n",
    "# Optional: Print out training_args to inspect (warning, it is quite a long output)\n",
    "# training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training arguments created!\n",
    "\n",
    "Let's put them to work in an instance of `transformers.Trainer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TK - Setting up an instance of Trainer\n",
    "\n",
    "Time for step 4!\n",
    "\n",
    "1. Create and preprocess data (done ✅).\n",
    "2. Define the model we'd like use with [`transformers.AutoModelForSequenceClassification`](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForSequenceClassification) (or another similar model class) (done ✅). \n",
    "3. Define training arguments (these are hyperparameters for our model) with [`transformers.TrainingArguments`](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments). (done ✅)\n",
    "4. Pass `TrainingArguments` from 3 and target datasets to an instance of [`transformers.Trainer`](https://huggingface.co/docs/transformers/en/main_classes/trainer).\n",
    "5. Train the model by calling [`Trainer.train()`](https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/trainer#transformers.Trainer.train).\n",
    "6. Save the model.\n",
    "7. Evaluate results.\n",
    "\n",
    "The [`transformers.Trainer`](https://huggingface.co/docs/transformers/en/main_classes/trainer#trainer) class allows you to train models.\n",
    "\n",
    "It's built on PyTorch so it gets to leverage all of the powerful PyTorch toolkit.\n",
    "\n",
    "But since it also works closely with the `transformers.TrainingArguments` class, it offers many helpful features.\n",
    "\n",
    "::: {.callout-note}\n",
    "`transformers.Trainer` can work with `torch.nn.Module` models, however, it is designed to work best with [`transformers.PreTrainedModel`](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel)'s from the `transformers` library.\n",
    "\n",
    "This is not a problem for us as we're using `transformers.AutoModelForSequenceClassification.from_pretrained` which loads a `transformers.PreTrainedModel`.\n",
    "\n",
    "See the [`transformers.Trainer` documentation](https://huggingface.co/docs/transformers/en/main_classes/trainer#trainer) for tips on how to make sure your model is compatible.\n",
    ":::\n",
    "\n",
    "| Parameter | Explanation |\n",
    "|:-----|:-----|\n",
    "| `model` | The model we'd like to train. Works best with an instance of `transformers.PreTrainedModel`. Most models loaded using `from_pretrained` will be of this type. |\n",
    "| `args` | Instance of `transformers.TrainingArguments`. We'll use the `training_args` object we defined earlier. But if this is not set, it will default to the default settings for `transformers.TrainingArguments`. |\n",
    "| `train_dataset`| Dataset to use during training. We can use our `tokenized_dataset[\"train\"]` as it has already been preprocessed. |\n",
    "| `eval_dataset` | Dataset to use during evaluation. We can use our `tokenized_dataset[\"test\"]` as it has already been preprocessed.  |\n",
    "| `tokenizer` | The `tokenizer` which was used to preprocess the data. Passing a tokenizer will also pad the inputs to maximum length when batching them. It will also be saved with the model so future re-runs are easier. |\n",
    "| `compute_metrics` | An evaluation function to evaluate a model during training and evaluation steps. In our case, we'll use the `compute_accuracy` function we defined earlier. |\n",
    "\n",
    "With all this being said, let's build our `Trainer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "7vXcfL6q7Fqs"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Setup Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer, # Pass tokenizer to the Trainer for dynamic padding (padding as the training happens) (see \"data_collator\" in the Trainer docs)\n",
    "    compute_metrics=compute_accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo! We've created our own `trainer`.\n",
    "\n",
    "We're one step closer to training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TK - Training our text classification model\n",
    "\n",
    "We've done most the hard word setting up our `transformers.TrainingArguments` as well as our `transformers.Trainer`.\n",
    "\n",
    "Now how about we train a model?\n",
    "\n",
    "Following our steps:\n",
    "\n",
    "1. Create and preprocess data (done ✅).\n",
    "2. Define the model we'd like use with [`transformers.AutoModelForSequenceClassification`](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForSequenceClassification) (or another similar model class) (done ✅). \n",
    "3. Define training arguments (these are hyperparameters for our model) with [`transformers.TrainingArguments`](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments). (done ✅)\n",
    "4. Pass `TrainingArguments` from 3 and target datasets to an instance of [`transformers.Trainer`](https://huggingface.co/docs/transformers/en/main_classes/trainer). (done ✅)\n",
    "5. Train the model by calling [`Trainer.train()`](https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/trainer#transformers.Trainer.train).\n",
    "6. Save the model.\n",
    "7. Evaluate results.\n",
    "\n",
    "Looks like all we have to do is call `transformers.Trainer.train()`.\n",
    "\n",
    "We'll be sure to save the results of the training to a variable `results` so we can inspect them later.\n",
    "\n",
    "Let's try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "MEzkvmO6-Whg",
    "outputId": "22937d9f-a794-4579-bc67-70ba6cb05dcd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 00:08, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.444800</td>\n",
       "      <td>0.125465</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>0.016850</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.027745</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train a text classification model\n",
    "results = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woahhhh!!!\n",
    "\n",
    "How cool is that!\n",
    "\n",
    "We just trained a text classification model!\n",
    "\n",
    "And it looks like the training went pretty quick (thanks to our smaller dataset and relatively small model, for larger datasets, training would likely take longer).\n",
    "\n",
    "How about we check some of the metrics?\n",
    "\n",
    "We can do so using the `results.metrics` attribute (this returns a Python dictionary with stats from our training run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_runtime: 8.7734\n",
      "train_samples_per_second: 227.962\n",
      "train_steps_per_second: 7.979\n",
      "total_flos: 18110777160000.0\n",
      "train_loss: 0.05121807770005294\n",
      "epoch: 10.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect training metrics\n",
    "for key, value in results.metrics.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!\n",
    "\n",
    "Looks like our overall training runtime is low because of our small dataset.\n",
    "\n",
    "And looks like our `trainer` was able to process a fair few samples per second.\n",
    "\n",
    "If we were to 1000x the size of our dataset (e.g. ~250 samples -> ~250,000 samples), it seems our training time still wouldn't take too long.\n",
    "\n",
    "The `total_flos` stands for \"[floating point operations](https://en.wikipedia.org/wiki/FLOPS)\" (also referred to as FLOPS), this is the total number of calculations our model has performed to find patterns in the data. And as you can see, it's quite a large number! \n",
    "\n",
    "::: {.callout-note}\n",
    "\n",
    "Depending on the hardware you're using, the results with respect to `train_runtime`, `train_samples_per_second` and `train_steps_per_second` will likely be different.\n",
    "\n",
    "The faster your accelerator hardware (e.g. NVIDIA GPU or Mac GPU), the lower your runtime and higher your samples/steps per second will be.\n",
    "\n",
    "For reference, on my local NVIDIA RTX 4090, I get a `train_runtime` of 8-9 seconds, `train_samples_per_second` of 230-250 and `train_steps_per_second` of 8.565.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TK - Save the model for later use\n",
    "\n",
    "Now our model has been trained, let's save it for later use.\n",
    "\n",
    "We'll save it locally first and push it to the Hugging Face Hub later.\n",
    "\n",
    "We can save our model using the [`transformers.Trainer.save_model`](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.save_model) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving model to models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "print(f\"[INFO] Saving model to {model_save_dir}\")\n",
    "trainer.save_model(output_dir=model_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model saved locally! Before we save it to the Hugging Face Hub, let's check out its metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TK - Inspecting the model training metrics\n",
    "\n",
    "We can get a log of our model's training state using `trainer.state.log_history`.\n",
    "\n",
    "This will give us a collection of metrics per epoch (as long as we set `logging_strategy=\"epoch\"` in `transformers.TrainingArguments`), in particular, it will give us a loss value per epoch.\n",
    "\n",
    "We can extract these values and inspect them visually for a better understanding our model training.\n",
    "\n",
    "Let's get the training history and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 0.4448,\n",
       "  'grad_norm': 1.788326621055603,\n",
       "  'learning_rate': 9e-05,\n",
       "  'epoch': 1.0,\n",
       "  'step': 7},\n",
       " {'eval_loss': 0.12546488642692566,\n",
       "  'eval_accuracy': 0.98,\n",
       "  'eval_runtime': 0.0193,\n",
       "  'eval_samples_per_second': 2588.246,\n",
       "  'eval_steps_per_second': 103.53,\n",
       "  'epoch': 1.0,\n",
       "  'step': 7},\n",
       " {'loss': 0.0531,\n",
       "  'grad_norm': 0.19492699205875397,\n",
       "  'learning_rate': 8e-05,\n",
       "  'epoch': 2.0,\n",
       "  'step': 14},\n",
       " {'eval_loss': 0.016849540174007416,\n",
       "  'eval_accuracy': 1.0,\n",
       "  'eval_runtime': 0.0214,\n",
       "  'eval_samples_per_second': 2340.049,\n",
       "  'eval_steps_per_second': 93.602,\n",
       "  'epoch': 2.0,\n",
       "  'step': 14}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get training history \n",
    "trainer_history_all = trainer.state.log_history \n",
    "trainer_history_metrics = trainer_history_all[:-1] # get everything except the training time metrics (we've seen these already)\n",
    "trainer_history_training_time = trainer_history_all[-1] # this is the same value as results.metrics from above\n",
    "\n",
    "# View the first 4 metrics from the training history\n",
    "trainer_history_metrics[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, looks like the metrics are logged every epochs in a list Python dictionaries with interleaving `loss` (this is the training set loss) and `eval_loss` values.\n",
    "\n",
    "How about we write some code to separate the training set metrics and the evaluation set metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] First two items in training set:\n",
      "[{'epoch': 1.0,\n",
      "  'grad_norm': 1.788326621055603,\n",
      "  'learning_rate': 9e-05,\n",
      "  'loss': 0.4448,\n",
      "  'step': 7},\n",
      " {'epoch': 2.0,\n",
      "  'grad_norm': 0.19492699205875397,\n",
      "  'learning_rate': 8e-05,\n",
      "  'loss': 0.0531,\n",
      "  'step': 14}]\n",
      "\n",
      "[INFO] First two items in evaluation set:\n",
      "[{'epoch': 1.0,\n",
      "  'eval_accuracy': 0.98,\n",
      "  'eval_loss': 0.12546488642692566,\n",
      "  'eval_runtime': 0.0193,\n",
      "  'eval_samples_per_second': 2588.246,\n",
      "  'eval_steps_per_second': 103.53,\n",
      "  'step': 7},\n",
      " {'epoch': 2.0,\n",
      "  'eval_accuracy': 1.0,\n",
      "  'eval_loss': 0.016849540174007416,\n",
      "  'eval_runtime': 0.0214,\n",
      "  'eval_samples_per_second': 2340.049,\n",
      "  'eval_steps_per_second': 93.602,\n",
      "  'step': 14}]\n"
     ]
    }
   ],
   "source": [
    "import pprint # import pretty print for nice printing of lists\n",
    "\n",
    "# Extract training and evaluation metrics\n",
    "trainer_history_training_set = []\n",
    "trainer_history_eval_set = []\n",
    "\n",
    "# Loop through metrics and filter for training and eval metrics\n",
    "for item in trainer_history_metrics:\n",
    "    item_keys = list(item.keys())\n",
    "    # Check to see if \"eval\" is in the keys of the item\n",
    "    if any(\"eval\" in item for item in item_keys):\n",
    "        trainer_history_eval_set.append(item)\n",
    "    else:\n",
    "        trainer_history_training_set.append(item)\n",
    "\n",
    "# Show the first two items in each metric set\n",
    "print(f\"[INFO] First two items in training set:\")\n",
    "pprint.pprint(trainer_history_training_set[:2])\n",
    "\n",
    "print(f\"\\n[INFO] First two items in evaluation set:\")\n",
    "pprint.pprint(trainer_history_eval_set[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful!\n",
    "\n",
    "How about we take it a step further and turn our metrics into pandas DataFrames so we can view them easier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>grad_norm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4448</td>\n",
       "      <td>1.788327</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0531</td>\n",
       "      <td>0.194927</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.049827</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.027272</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>4.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.020451</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>5.0</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     loss  grad_norm  learning_rate  epoch  step\n",
       "0  0.4448   1.788327        0.00009    1.0     7\n",
       "1  0.0531   0.194927        0.00008    2.0    14\n",
       "2  0.0069   0.049827        0.00007    3.0    21\n",
       "3  0.0022   0.027272        0.00006    4.0    28\n",
       "4  0.0013   0.020451        0.00005    5.0    35"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create pandas DataFrames for the training and evaluation metrics\n",
    "trainer_history_training_df = pd.DataFrame(trainer_history_training_set)\n",
    "trainer_history_eval_df = pd.DataFrame(trainer_history_eval_set)\n",
    "\n",
    "trainer_history_training_df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!\n",
    "\n",
    "And the evaluation DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.125465</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>2588.246</td>\n",
       "      <td>103.530</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016850</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>2340.049</td>\n",
       "      <td>93.602</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.027745</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.0296</td>\n",
       "      <td>1690.693</td>\n",
       "      <td>67.628</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001385</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>2116.198</td>\n",
       "      <td>84.648</td>\n",
       "      <td>4.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000903</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0175</td>\n",
       "      <td>2850.050</td>\n",
       "      <td>114.002</td>\n",
       "      <td>5.0</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eval_loss  eval_accuracy  eval_runtime  eval_samples_per_second  \\\n",
       "0   0.125465           0.98        0.0193                 2588.246   \n",
       "1   0.016850           1.00        0.0214                 2340.049   \n",
       "2   0.027745           0.98        0.0296                 1690.693   \n",
       "3   0.001385           1.00        0.0236                 2116.198   \n",
       "4   0.000903           1.00        0.0175                 2850.050   \n",
       "\n",
       "   eval_steps_per_second  epoch  step  \n",
       "0                103.530    1.0     7  \n",
       "1                 93.602    2.0    14  \n",
       "2                 67.628    3.0    21  \n",
       "3                 84.648    4.0    28  \n",
       "4                114.002    5.0    35  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_history_eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, we'll have follow the data explorer's motto of *visualize, visualize, visualize!* and inspect our loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and evaluation loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(trainer_history_training_df[\"epoch\"], trainer_history_training_df[\"loss\"], label=\"Training loss\")\n",
    "plt.plot(trainer_history_eval_df[\"epoch\"], trainer_history_eval_df[\"eval_loss\"], label=\"Evaluation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Text classification with DistilBert training and evaluation loss over time\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B-e-a-utiful!\n",
    "\n",
    "That is exactly what we wanted.\n",
    "\n",
    "Training and evaluation loss going down over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TK - Pushing our model to the Hugging Face Hub (TK - should I reorder this to be: save to local -> save to Hugging Face Hub -> inspect metrics)\n",
    "\n",
    "We've saved our model locally but how about we push it to the Hugging Face Hub?\n",
    "\n",
    "The Hugging Face Hub is one of the best sources of machine learning models on the internet.\n",
    "\n",
    "And we can add our model there so others can use it or we can access it in the future (we could also keep it private on the Hugging Face Hub so only people from our organization can use it).\n",
    "\n",
    "Sharing models on Hugging Face is also a great way to showcase your skills as a machine learning engineer, it gives you something to show potential employers and say \"here's what I've done\".\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "Before sharing a model to the Hugging Face Hub, be sure to go through the following steps: \n",
    "\n",
    "1. Setup a Hugging Face token using the [`huggingface-cli login` command](https://huggingface.co/docs/huggingface_hub/en/guides/cli).\n",
    "2. Read through the [user access tokens guide](https://huggingface.co/docs/hub/en/security-tokens).\n",
    "3. Set up an access token via [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) (ensure it has \"write\" access).\n",
    "\n",
    "If you are using Google Colab, you can add your token under the \"Secrets\" tab on the left.\n",
    "\n",
    "On my local computer, my token is saved to `/home/daniel/.cache/huggingface/token` (thanks to running `huggingface-cli login` on the command line).\n",
    "\n",
    "And for more on sharing models to the Hugging Face Hub, be sure to check out the [model sharing documentation](https://huggingface.co/docs/transformers/en/model_sharing).\n",
    "\n",
    ":::\n",
    "\n",
    "We can push our model, tokenizer and other assosciated files to the Hugging Face Hub using the [`transformers.Trainer.push_to_hub`](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.push_to_hub) method. \n",
    "\n",
    "We can also optionally do the following:\n",
    "\n",
    "* Add a [model card](https://huggingface.co/docs/hub/en/model-cards) (something that describes how the model was created and what it can be used for) using [`transformers.Trainer.create_model_card`](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.create_model_card).\n",
    "* Add a custom `README.md` file to the model repository to explain more details about the model using [`huggingface_hub.HfApi.upload_file`](https://huggingface.co/docs/huggingface_hub/en/guides/upload#upload-a-file). This method is similar to model card creation method above but with more customization.\n",
    "\n",
    "Let's save our model to the Hub!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44aede0167f2432bb387a9bea0ac7905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee3e5e5c5334586a24e4c5879d33c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc592d78831948afab5b80ba0e5d450a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased/commit/5cbdf11a03cf05cd5a76c5441f84a810e87f96b2', commit_message='Uploading food not food text classifier model', commit_description='', oid='5cbdf11a03cf05cd5a76c5441f84a810e87f96b2', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save our model to the Hugging Face Hub\n",
    "# This will be public, since we set hub_private_repo=False in our TrainingArguments\n",
    "trainer.push_to_hub(\n",
    "    commit_message=\"Uploading food not food text classifier model\",\n",
    "    # token=\"YOUR_HF_TOKEN_HERE\" # This will default to the token you have saved in your Hugging Face config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model pushed to the Hugging Face Hub!\n",
    "\n",
    "::: {.callout-note} \n",
    "\n",
    "You may see the following error:\n",
    "\n",
    "> 403 Forbidden: You don't have the rights to create a model under the namespace \"mrdbourke\".\n",
    "> Cannot access content at: https://huggingface.co/api/repos/create.\n",
    "> If you are trying to create or update content, make sure you have a token with the `write` role.\n",
    "\n",
    "In this case, be sure to go through the [setup steps above](https://huggingface.co/docs/hub/en/security-tokens) to make sure you have a Hugging Face access token with \"write\" access.\n",
    "\n",
    ":::\n",
    "\n",
    "And since it's public (by default), you can see it at [https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased](https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased) (it gets saved to the same name as our target local directory). \n",
    "\n",
    "You can now share and interact with this model online.\n",
    "\n",
    "As well as download it for use in your own applications.\n",
    "\n",
    "* TK image - model on Hugging Face Hub ready to use/example of using the model already\n",
    "\n",
    "But before we make an application with it, let's keep evaluating it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TK - Evaluating predictions on the test data\n",
    "\n",
    "Model trained, let's now evaluate it on the test data.\n",
    "\n",
    "A reminder that the test data is data that our model has never seen before.\n",
    "\n",
    "So it will be a good estimate of how our model will do in a production setting.\n",
    "\n",
    "We can make predictions on the test dataset using [`transformers.Trainer.predict`](https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/trainer#transformers.Trainer.predict).\n",
    "\n",
    "And then we can get the prediction values with the `predictions` attribute and assosciated metrics with the `metrics` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Prediction metrics on the test data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.0005322981742210686,\n",
       " 'test_accuracy': 1.0,\n",
       " 'test_runtime': 0.0181,\n",
       " 'test_samples_per_second': 2763.374,\n",
       " 'test_steps_per_second': 110.535}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform predictions on the test set\n",
    "predictions_all = trainer.predict(tokenized_dataset[\"test\"])\n",
    "prediction_values = predictions_all.predictions\n",
    "prediction_metrics = predictions_all.metrics\n",
    "\n",
    "print(f\"[INFO] Prediction metrics on the test data:\")\n",
    "prediction_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah!\n",
    "\n",
    "Looks like our model did an outstanding job!\n",
    "\n",
    "And it was *very* quick too.\n",
    "\n",
    "This is one of the benefits of using a smaller pretrained model and customizing it to your own dataset.\n",
    "\n",
    "You can achieve outstanding results in a very quick time as well as have a model capable of performing thousands of predictions per second.\n",
    "\n",
    "We can also calculate the accuracy by hand by comparing the prediction labels to the test labels.\n",
    "\n",
    "To do so, we'll:\n",
    "\n",
    "1. Calculate the prediction probabilities (though this is optional) by passing the `prediction_values` to [`torch.softmax`](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html).\n",
    "2. Find the index of the prediction value with the highest value (the index will be equivalent to the predicted label) using [`torch.argmax`](https://pytorch.org/docs/stable/generated/torch.argmax.html) (we could also use [`np.argmax`](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) here) to find the predicted labels. \n",
    "3. Get the true labels from the test dataset using `dataset[\"test\"][\"label\"]`.\n",
    "4. Compare the predicted labels from 2 to the true labels from 3 using [`sklearn.metrics.accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) to find the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Test accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Get prediction probabilities (this is optional, could get the same results with step 2 onwards)\n",
    "pred_probs = torch.softmax(torch.tensor(prediction_values), dim=1)\n",
    "\n",
    "# 2. Get the predicted labels\n",
    "pred_labels = torch.argmax(pred_probs, dim=1)\n",
    "\n",
    "# 3. Get the true labels\n",
    "true_labels = dataset[\"test\"][\"label\"]\n",
    "\n",
    "# 4. Compare predicted labels to true labels to get the test accuracy\n",
    "test_accuracy = accuracy_score(y_true=true_labels, \n",
    "                               y_pred=pred_labels)\n",
    "\n",
    "print(f\"[INFO] Test accuracy: {test_accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah!\n",
    "\n",
    "Looks like our model performs really well on our test set.\n",
    "\n",
    "It will be interesting to see how it goes on real world samples.\n",
    "\n",
    "We'll test this later on.\n",
    "\n",
    "How about we make a pandas DataFrame out of our test samples, predicted labels and predicted probabilities to further inspect our results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>pred_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A slice of pepperoni pizza with a layer of mel...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Red brick fireplace with a mantel serving as a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A bowl of sliced bell peppers with a sprinkle ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Set of mugs hanging on a hook</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Standing floor lamp providing light next to an...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  true_label  pred_label  \\\n",
       "0  A slice of pepperoni pizza with a layer of mel...           1           1   \n",
       "1  Red brick fireplace with a mantel serving as a...           0           0   \n",
       "2  A bowl of sliced bell peppers with a sprinkle ...           1           1   \n",
       "3                      Set of mugs hanging on a hook           0           0   \n",
       "4  Standing floor lamp providing light next to an...           0           0   \n",
       "\n",
       "   pred_prob  \n",
       "0   0.999443  \n",
       "1   0.999520  \n",
       "2   0.999445  \n",
       "3   0.999556  \n",
       "4   0.999556  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a DataFrame of test predictions\n",
    "test_predictions_df = pd.DataFrame({\n",
    "    \"text\": dataset[\"test\"][\"text\"],\n",
    "    \"true_label\": true_labels,\n",
    "    \"pred_label\": pred_labels,\n",
    "    \"pred_prob\": torch.max(pred_probs, dim=1).values\n",
    "})\n",
    "\n",
    "test_predictions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the examples with the lowest prediction probability to see where the model is unsure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>pred_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Set of muffin tins stacked together</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>A bowl of cherries with a sprig of mint for ga...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>A close-up shot of a cheesy pizza slice being ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Boxes of apples, pears, pineapple, manadrins a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>A fruit platter with a variety of exotic fruit...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Two handfuls of bananas in a fruit bowl with g...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Zucchini in a bowl, sprinkled with basil and s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Luxurious coconut shrimp curry on a generous p...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Plate of sushi served with pickled ginger and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Traditional Japanese flavored sushi roll with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  true_label  pred_label  \\\n",
       "43                Set of muffin tins stacked together           0           0   \n",
       "40  A bowl of cherries with a sprig of mint for ga...           1           1   \n",
       "11  A close-up shot of a cheesy pizza slice being ...           1           1   \n",
       "42  Boxes of apples, pears, pineapple, manadrins a...           1           1   \n",
       "26  A fruit platter with a variety of exotic fruit...           1           1   \n",
       "14  Two handfuls of bananas in a fruit bowl with g...           1           1   \n",
       "16  Zucchini in a bowl, sprinkled with basil and s...           1           1   \n",
       "35  Luxurious coconut shrimp curry on a generous p...           1           1   \n",
       "49  Plate of sushi served with pickled ginger and ...           1           1   \n",
       "18  Traditional Japanese flavored sushi roll with ...           1           1   \n",
       "\n",
       "    pred_prob  \n",
       "43   0.999358  \n",
       "40   0.999407  \n",
       "11   0.999418  \n",
       "42   0.999420  \n",
       "26   0.999423  \n",
       "14   0.999424  \n",
       "16   0.999430  \n",
       "35   0.999432  \n",
       "49   0.999435  \n",
       "18   0.999437  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show 10 examples with low prediction probability\n",
    "test_predictions_df.sort_values(\"pred_prob\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, it looks like our model has quite a high prediction probability for almost all samples.\n",
    "\n",
    "We can further evalaute our model by making predictions on new custom data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGndCPKMAG-p"
   },
   "source": [
    "## TK - Making and inspecting predictions on custom text data\n",
    "\n",
    "We've seen how our model performs on the test dataset (quite well).\n",
    "\n",
    "But how might we check its performance on our own custom data?\n",
    "\n",
    "For example, text captions from the wild.\n",
    "\n",
    "Well, we've got two ways to load our model now too:\n",
    "\n",
    "1. Load model locally from our computer (e.g. via `models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased`).\n",
    "2. Load model from Hugging Face Hub (e.g. via `mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased`).\n",
    "\n",
    "Either way of loading the model results in the same outcome: being able to make predictions on given data.\n",
    "\n",
    "So how about we start by setting up our model paths for both local loading and loading from the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup local model path\n",
    "local_model_path = \"models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n",
    "\n",
    "# Setup Hugging Face model path (see: https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased)\n",
    "huggingface_model_path = \"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TK - Discussing ways to make predictions\n",
    "\n",
    "When we've loaded our trained model, because of the way we've set it up, there are two main ways to make predictions on custom data:\n",
    "\n",
    "1. **Pipeline mode** using [`transformers.pipeline`](https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/pipelines#pipelines) and passing it our target model, this allows us to preprocess custom data and make predictions in one step.\n",
    "2. **PyTorch mode** using a combination of [`transformers.AutoTokenizer`](https://huggingface.co/docs/transformers/v4.42.0/en/model_doc/auto#transformers.AutoTokenizer) and [`transformers.AutoModelForSequenceClassification`](https://huggingface.co/docs/transformers/v4.42.0/en/model_doc/auto#transformers.AutoModelForSequenceClassification) and passing each our target model, this requires us to preprocess our data before passing to a model, however, it offers the most customization.\n",
    "\n",
    "Each method supports predictions:\n",
    "\n",
    "1. Predictions one at a time (batch size of 1), for example, one person using the app at a time.\n",
    "2. Predictions at a time (batch size of `n` where `n` can be any number, e.g. `8`, `16`, `32`), for example, many people using a service simultaneously such as a voice chat and needing to filter comments (predicting on batches of size `n` is usually much faster than batches of 1).\n",
    "\n",
    "Whichever method we choose, we'll have to set the target device we'd like the operations to happen on.\n",
    "\n",
    "In general, it's best to make predictions on the most powerful accelerator you have available.\n",
    "\n",
    "And in most cases that will be a NVIDIA GPU > Mac GPU > CPU.\n",
    "\n",
    "So let's write a small function to pick the target device for us in that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "def set_device():\n",
    "    \"\"\"\n",
    "    Set device to CUDA if available, else MPS (Mac), else CPU.\n",
    "\n",
    "    This defaults to using the best available device (usually).\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "DEVICE = set_device()\n",
    "print(f\"[INFO] Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target device set!\n",
    "\n",
    "Let's start predicting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJ8jAbX6Aa7D"
   },
   "source": [
    "### TK - Making predictions with pipeline\n",
    "\n",
    "The [`transformers.pipeline`](https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/pipelines#transformers.pipeline) method creates a machine learning pipeline.\n",
    "\n",
    "Data goes in one end and predictions come out the other end.\n",
    "\n",
    "You can create pipelines for many different tasks, such as, text classification, image classification, object detection, text generation and more.\n",
    "\n",
    "Let's see how we can create a pipeline for our text classification model.\n",
    "\n",
    "To do so we'll:\n",
    "\n",
    "1. Instantiate an instance of `transformers.pipeline`.\n",
    "2. Pass in the `task` parameter of `text-classification` (we can do this because our model is already formatted for text classification thanks to using `transformers.AutoModelForSequenceClassification`).\n",
    "3. Setup the `model` parameter to be `local_model_path` (though we could also use `huggingface_model_path`).\n",
    "4. Set the target device using the `device` parameter.\n",
    "5. Set `top_k=1` to get to the top prediction back (e.g. either `\"food\"` or `\"not_food\"`, could set this higher to get more labels back).\n",
    "6. Set the `BATCH_SIZE=32` so we can pass to the `batch_size` parameter. This will allow our model to make predictions on up to `32` samples at a time. Predicting on batches of data is usually much faster than single samples at a time, however, this often saturates at a point (e.g. predicting on batches of size 64 may be the same speed as 32 due to memory contraints).\n",
    "\n",
    "::: {.callout-note}\n",
    "\n",
    "There are many more pipelines available in the [Hugging Face documentation](https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/pipelines#transformers.pipeline).\n",
    "\n",
    "As an exericse, I'd spend 10-15 minutes reading through the pipeline documentation to get familiar with what's available.\n",
    "\n",
    ":::\n",
    "\n",
    "Let's setup our pipeline! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B44pOUBKAcvh",
    "outputId": "fb71c373-8dc5-42c3-9749-c58ab59253c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x7fdd74f23410>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Set the batch size for predictions\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create an instance of transformers.pipeline\n",
    "food_not_food_classifier = pipeline(task=\"text-classification\", # we can use this because our model is an instance of AutoModelForSequenceClassification\n",
    "                                    model=local_model_path, # could also pass in huggingface_model_path\n",
    "                                    device=DEVICE, # set the target device\n",
    "                                    top_k=1, # only return the top predicted value\n",
    "                                    batch_size=BATCH_SIZE) # perform predictions on up to BATCH_SIZE number of samples at a time \n",
    "\n",
    "food_not_food_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created an instance of [`transformers.pipelines.text_classification.TextClassificationPipeline`](https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/pipelines#transformers.TextClassificationPipeline)!\n",
    "\n",
    "Now let's test it out by passing it a string of text about food.\n",
    "\n",
    "UPTOHERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'food', 'score': 0.9994196891784668}]]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text_food = \"A delicious photo of a plate of scrambled eggs, bacon and toast\"\n",
    "food_not_food_classifier(sample_text_food)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TK - How about a string not about food?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8YP2KDxPFQ8C",
    "outputId": "7d9f1336-883e-474a-cc9a-aa605ba2afc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'not_food', 'score': 0.9994813799858093}]]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text_not_food = \"A yellow tractor driving over the hill\"\n",
    "food_not_food_classifier(sample_text_not_food)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we passed in random text?\n",
    "\n",
    "TK - this is a use case to think about when building ML-powered apps in the wild, you will often have many inputs to your model you didn't actually expect (e.g. Nutrify's food not food model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'not_food', 'score': 0.9978036284446716}]]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_not_food_classifier(\"cvnhertiejhwgdjshdfgh394587\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'food', 'score': 0.9981549382209778}]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pipeline also works with remote models (will have to laod the model locally first)\n",
    "food_not_food_classifier_remote = pipeline(task=\"text-classification\", \n",
    "                                           model=huggingface_model_path,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           device=DEVICE)\n",
    "\n",
    "food_not_food_classifier_remote(\"This is some new text about bananas and pancakes and ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TK - Batch prediction\n",
    "\n",
    "* TK - what is batch prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ypr_Y65CFjFb",
    "outputId": "3a50b522-c155-49fd-8c0e-27541486bca4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'not_food', 'score': 0.9410305619239807},\n",
       " {'label': 'not_food', 'score': 0.9650871753692627},\n",
       " {'label': 'not_food', 'score': 0.9215793609619141},\n",
       " {'label': 'not_food', 'score': 0.9115400910377502},\n",
       " {'label': 'not_food', 'score': 0.9625208377838135},\n",
       " {'label': 'not_food', 'score': 0.9476941823959351},\n",
       " {'label': 'not_food', 'score': 0.9451109170913696},\n",
       " {'label': 'not_food', 'score': 0.9027702808380127},\n",
       " {'label': 'not_food', 'score': 0.9954429864883423},\n",
       " {'label': 'food', 'score': 0.7653573155403137}]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting works with lists\n",
    "# Can find the examples with highest confidence and keep those\n",
    "sentences = [\n",
    "    \"I whipped up a fresh batch of code, but it seems to have a syntax error.\",\n",
    "    \"We need to marinate these ideas overnight before presenting them to the client.\",\n",
    "    \"The new software is definitely a spicy upgrade, taking some time to get used to.\",\n",
    "    \"Her social media post was the perfect recipe for a viral sensation.\",\n",
    "    \"He served up a rebuttal full of facts, leaving his opponent speechless.\",\n",
    "    \"The team needs to simmer down a bit before tackling the next challenge.\",\n",
    "    \"Our budget is a bit thin, so we'll have to use budget-friendly materials for this project.\",\n",
    "    \"The presentation was a delicious blend of humor and information, keeping the audience engaged.\",\n",
    "    \"Daniel Bourke is really cool :D\",\n",
    "    \"My favoruite food is biltong!\"\n",
    "]\n",
    "\n",
    "food_not_food_classifier(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TK - Time our model across larger sample sizes\n",
    "\n",
    "* TK - our model is fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Number of sentences: 100\n",
      "[INFO] Inference time for 100 sentences: 0.07726 seconds.\n",
      "[INFO] Avg inference time per sentence: 0.0007726 seconds.\n",
      "\n",
      "[INFO] Number of sentences: 1000\n",
      "[INFO] Inference time for 1000 sentences: 0.32344 seconds.\n",
      "[INFO] Avg inference time per sentence: 0.00032344 seconds.\n",
      "\n",
      "[INFO] Number of sentences: 10000\n",
      "[INFO] Inference time for 10000 sentences: 1.43834 seconds.\n",
      "[INFO] Avg inference time per sentence: 0.00014383 seconds.\n",
      "\n",
      "[INFO] Number of sentences: 100000\n",
      "[INFO] Inference time for 100000 sentences: 14.4585 seconds.\n",
      "[INFO] Avg inference time per sentence: 0.00014459 seconds.\n",
      "\n",
      "CPU times: user 15.8 s, sys: 552 ms, total: 16.3 s\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "for i in [10, 100, 1000, 10_000]:\n",
    "    sentences_big = sentences * i\n",
    "    print(f\"[INFO] Number of sentences: {len(sentences_big)}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    food_not_food_classifier(sentences_big)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"[INFO] Inference time for {len(sentences_big)} sentences: {round(end_time - start_time, 5)} seconds.\")\n",
    "    print(f\"[INFO] Avg inference time per sentence: {round((end_time - start_time) / len(sentences_big), 8)} seconds.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBdDyzEvAvwK"
   },
   "source": [
    "### PyTorch mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "TEl6YIR6Azbq"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"learn_hf_food_not_food_text_classifier_model\")\n",
    "inputs = tokenizer(sample_text_food, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "hsDs8QhXBHPJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"learn_hf_food_not_food_text_classifier_model\")\n",
    "with torch.no_grad():\n",
    "  logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpETXjJHBD8k",
    "outputId": "bfc30a2d-9a47-441f-f2e9-7cee7fd4989d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: A delicious photo of a plate of scrambled eggs, bacon and toast\n",
      "Predicted label: food\n"
     ]
    }
   ],
   "source": [
    "# Get predicted class\n",
    "predicted_class_id = logits.argmax().item()\n",
    "print(f\"Text: {sample_text_food}\")\n",
    "print(f\"Predicted label: {model.config.id2label[predicted_class_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TK - Turning our model into a demo\n",
    "\n",
    "* TK - why build a demo?\n",
    "    * - try our model in the wild, see samples which don't work properly, e.g. use cases we didn't think of... \"pie\"/\"tea\" (short words), \"hjflasdjhfhwerr\" (gibberish)\n",
    "* TK - build a demo with Gradio, see it here: https://www.gradio.app/guides/quickstart \n",
    "* TK - requires `pip install gradio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'not_food', 'score': 0.9977033734321594},\n",
       " {'label': 'food', 'score': 0.002296620048582554}]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set top_k=2 to get top 2 predictions (in our case, food and not_food)\n",
    "food_not_food_classifier(\"Testing the pipeline\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TK - Creating a simple function to perform inference\n",
    "\n",
    "* TK - this is required for gradio -> output a dict of {\"label_1\": probability_1, \"label_2\": probability_2...}\n",
    "* 2 options:\n",
    "    * Local demo (for our own inspection)\n",
    "    * Hosted demo on Hugging Face Spaces (for sharing with others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'food': 0.7966588139533997, 'not_food': 0.20334114134311676}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def food_not_food_classifier(text):\n",
    "    food_not_food_classifier = pipeline(task=\"text-classification\", \n",
    "                                        model=local_model_path,\n",
    "                                        batch_size=64,\n",
    "                                        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                                        top_k=None) # return all possible scores (not just top-1)\n",
    "    \n",
    "    # Get outputs from pipeline (as a list of dicts)\n",
    "    outputs = food_not_food_classifier(text)[0]\n",
    "\n",
    "    # Format output for Gradio (e.g. {\"label_1\": probability_1, \"label_2\": probability_2})\n",
    "    output_dict = {}\n",
    "\n",
    "    for item in outputs:\n",
    "        output_dict[item[\"label\"]] = item[\"score\"]\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "food_not_food_classifier(\"My lunch today was bacon and eggs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=food_not_food_classifier, \n",
    "    inputs=\"text\", \n",
    "    outputs=gr.Label(num_top_classes=2), # show top 2 classes (that's all we have)\n",
    "    title=\"Food or Not Food Classifier\",\n",
    "    description=\"A text classifier to determine if a sentence is about food or not food.\",\n",
    "    examples=[[\"I whipped up a fresh batch of code, but it seems to have a syntax error.\"],\n",
    "              [\"A delicious photo of a plate of scrambled eggs, bacon and toast.\"]])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TK - Uploading/running the demo\n",
    "\n",
    "Options:\n",
    "* Uploading manually to Hugging Face Spaces - hf.co/new-space \n",
    "* Uploading programmatically to Hugging Face Spaces - https://www.gradio.app/guides/using-hugging-face-integrations#hosting-your-gradio-demos-on-spaces\n",
    "* Running the demo locally - `Interface.launch()` (only works if you have Gradio installed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a directory for demos\n",
    "demos_dir = Path(\"../demos\")\n",
    "demos_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create a folder for the food_not_food_text_classifer demo\n",
    "food_not_food_text_classifier_demo_dir = Path(demos_dir, \"food_not_food_text_classifier\")\n",
    "food_not_food_text_classifier_demo_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../demos/food_not_food_text_classifier/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../demos/food_not_food_text_classifier/app.py\n",
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "def food_not_food_classifier(text):\n",
    "    # Set up text classification pipeline\n",
    "    food_not_food_classifier = pipeline(task=\"text-classification\", \n",
    "                                        model=\"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\", # link to model on HF Hub\n",
    "                                        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                                        top_k=None) # return all possible scores (not just top-1)\n",
    "    \n",
    "    # Get outputs from pipeline (as a list of dicts)\n",
    "    outputs = food_not_food_classifier(text)[0]\n",
    "\n",
    "    # Format output for Gradio (e.g. {\"label_1\": probability_1, \"label_2\": probability_2})\n",
    "    output_dict = {}\n",
    "    for item in outputs:\n",
    "        output_dict[item[\"label\"]] = item[\"score\"]\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "description = \"\"\"\n",
    "A text classifier to determine if a sentence is about food or not food.\n",
    "\n",
    "TK - See source code:\n",
    "\"\"\"\n",
    "\n",
    "demo = gr.Interface(fn=food_not_food_classifier, \n",
    "             inputs=\"text\", \n",
    "             outputs=gr.Label(num_top_classes=2), # show top 2 classes (that's all we have)\n",
    "             title=\"🍗🚫🥑 Food or Not Food Text Classifier\",\n",
    "             description=description,\n",
    "             examples=[[\"I whipped up a fresh batch of code, but it seems to have a syntax error.\"],\n",
    "                       [\"A delicious photo of a plate of scrambled eggs, bacon and toast.\"]])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TK - note: you will often need a requirements.txt file\n",
    "\n",
    "```\n",
    "===== Application Startup at 2024-06-13 05:37:21 =====\n",
    "\n",
    "Traceback (most recent call last):\n",
    "  File \"/home/user/app/app.py\", line 1, in <module>\n",
    "    import torch\n",
    "ModuleNotFoundError: No module named 'torch'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../demos/food_not_food_text_classifier/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../demos/food_not_food_text_classifier/requirements.txt\n",
    "gradio\n",
    "torch\n",
    "transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `README.md` file with metadata instructions (these are specific to Hugging Face Spaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../demos/food_not_food_text_classifier/README.md\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../demos/food_not_food_text_classifier/README.md\n",
    "---\n",
    "title: Food Not Food Text Classifier\n",
    "emoji: 🍗🚫🥑\n",
    "colorFrom: blue\n",
    "colorTo: yellow\n",
    "sdk: gradio\n",
    "sdk_version: 4.36.1\n",
    "app_file: app.py\n",
    "pinned: false\n",
    "license: apache-2.0\n",
    "---\n",
    "\n",
    "# 🍗🚫🥑 Food Not Food Text Classifier\n",
    "\n",
    "Small demo to showcase a text classifier to determine if a sentence is about food or not food.\n",
    "\n",
    "DistillBERT model fine-tuned on a small synthetic dataset of 250 generated [Food or Not Food image captions](https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions).\n",
    "\n",
    "TK - see the demo notebook on how to create this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating repo: learn_hf_food_not_food_text_classifier_demo\n",
      "[INFO] Full repo name: mrdbourke/learn_hf_food_not_food_text_classifier_demo\n",
      "[INFO] Uploading ../demos/food_not_food_text_classifier to repo: mrdbourke/learn_hf_food_not_food_text_classifier_demo\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import (\n",
    "    create_repo,\n",
    "    get_full_repo_name,\n",
    "    upload_file, # for uploading a single file\n",
    "    upload_folder # for uploading multiple files (in a folder)\n",
    ")\n",
    "\n",
    "path_to_demo_folder = \"../demos/food_not_food_text_classifier\"\n",
    "repo_type = \"space\" # we're creating a Hugging Face Space\n",
    "\n",
    "# Create a repo on Hugging Face\n",
    "# see docs: https://huggingface.co/docs/huggingface_hub/v0.23.3/en/package_reference/hf_api#huggingface_hub.HfApi.create_repo\n",
    "target_space_name = \"learn_hf_food_not_food_text_classifier_demo\"\n",
    "print(f\"[INFO] Creating repo: {target_space_name}\")\n",
    "create_repo(\n",
    "    repo_id=target_space_name,\n",
    "    #token=\"YOUR_HF_TOKEN\"\n",
    "    private=False, # set to True if you want the repo to be private\n",
    "    repo_type=repo_type, # create a Hugging Face Space\n",
    "    space_sdk=\"gradio\", # we're using Gradio to build our demo \n",
    "    exist_ok=True, # set to False if you want to create the repo even if it already exists            \n",
    ")\n",
    "\n",
    "# Get the full repo name (e.g. \"mrdbourke/learn_hf_food_not_food_text_classifier_demo\")\n",
    "full_repo_name = get_full_repo_name(model_id=target_space_name)\n",
    "print(f\"[INFO] Full repo name: {full_repo_name}\")\n",
    "\n",
    "# Upload a file\n",
    "# see docs: https://huggingface.co/docs/huggingface_hub/v0.23.3/en/package_reference/hf_api#huggingface_hub.HfApi.upload_file \n",
    "print(f\"[INFO] Uploading {path_to_demo_folder} to repo: {full_repo_name}\")\n",
    "file_url = upload_folder(\n",
    "    folder_path=path_to_demo_folder,\n",
    "    path_in_repo=\".\", # save to the root of the repo\n",
    "    repo_id=full_repo_name,\n",
    "    repo_type=repo_type,\n",
    "    #token=\"YOUR_HF_TOKEN\"\n",
    "    commit_message=\"Uploading food not food text classifier demo app.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TK - see the demo link here: https://huggingface.co/spaces/mrdbourke/learn_hf_food_not_food_text_classifier_demo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TK - Testing the live demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe\n",
       "\tsrc=\"https://mrdbourke-learn-hf-food-not-food-text-classifier-demo.hf.space\"\n",
       "\tframeborder=\"0\"\n",
       "\twidth=\"850\"\n",
       "\theight=\"450\"\n",
       "></iframe>     \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "# You can get embeddable HTML code for your demo by clicking the \"Embed\" button on the demo page\n",
    "HTML('''\n",
    "<iframe\n",
    "\tsrc=\"https://mrdbourke-learn-hf-food-not-food-text-classifier-demo.hf.space\"\n",
    "\tframeborder=\"0\"\n",
    "\twidth=\"850\"\n",
    "\theight=\"450\"\n",
    "></iframe>     \n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TK - Exercises and Extensions\n",
    "\n",
    "* Reading: \n",
    "    * Spend 15 minutes reading the TrainingArguments documentation - https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments \n",
    "    * Spend 10 minutes reading the Trainer documentation - https://huggingface.co/docs/transformers/en/main_classes/trainer#trainer \n",
    "    * Spend 10 minutes reading the Hugging Face model sharing documentation - https://huggingface.co/docs/transformers/en/model_sharing \n",
    "    * Spend 10-15 minutes reading the Hugging Face `transformers.pipeline` documentation - https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/pipelines#pipelines \n",
    "        * What does a pipeline do?\n",
    "        * Name 3 different kinds of pipelines and describe what they do in a sentence\n",
    "* Where does our model fail? E.g. what kind of sentences does it struggle with? How could you fix this? \n",
    "    * Make an extra 10-50 examples of these and add them to the dataset and then retrain the model\n",
    "    * See here: https://discuss.huggingface.co/t/how-do-i-add-things-rows-to-an-already-saved-dataset/27423 \n",
    "* Build your own text classifier on a different dataset/your own custom dataset\n",
    "* How might we make our dataset multi-class? (e.g. more than 2 classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TK - Extra resources\n",
    "\n",
    "* Hugging Face guide on text classification: https://huggingface.co/docs/transformers/en/tasks/sequence_classification \n",
    "* Hugging Face documentation on padding and truncation - https://huggingface.co/docs/transformers/en/pad_truncation \n",
    "* For more on Transformers (the architecture) as well as the DistilBert model:\n",
    "    - Read [*Transformers from scratch*](https://peterbloem.nl/blog/transformers) by Peter Bloem.\n",
    "    - Watch [Andrej Karpathy's lecture on Transformers and their history](https://www.youtube.com/watch?v=XfpMkf4rD6E).\n",
    "    - Read the original [*Attention is all you need*](https://arxiv.org/abs/1706.03762) paper (the paper that introduced the Transformer architecture).\n",
    "    - Read the [*DistilBert paper*](https://arxiv.org/abs/1910.01108) from the Hugging Face team (paper that introduced the DistilBert architecture and training setup).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "016181d9f51a4abea666046fb95f2aee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "01f82250d2e14f198dd0f561969d54ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0233456210f8469f85e648ba1c1f83ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2afc2c8167ad4d15ab6a35c1d1f1f3cb",
      "max": 152322,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_070785d29788406db9369ce39e24b7cf",
      "value": 152322
     }
    },
    "054e759a27a442afa8c666fc3d4c3f2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0673f25c77b04212bc2b63f56454a5d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "070785d29788406db9369ce39e24b7cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0eaf7b7abd71413597e0533efa3feda2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c56c3c1ff2a040b0952b5bac655b71dc",
      "placeholder": "​",
      "style": "IPY_MODEL_3d5b841a59a3489ea5b4112182d4ae70",
      "value": " 48.0/48.0 [00:00&lt;00:00, 3.86kB/s]"
     }
    },
    "103ff0dd04da40a69d85763bbf571490": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "13b1494b02194773a5fafe39b211c88a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1aa8fdd087ce42bdb16ae19a38e79b27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_80ef815be4ac4b64b3b3d99a6c43fadb",
      "placeholder": "​",
      "style": "IPY_MODEL_d1cc110825034545a2532fc567b5325e",
      "value": " 232k/232k [00:00&lt;00:00, 3.49MB/s]"
     }
    },
    "1b61ce44fb554db2bb906fb39b41b7e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c5d45fa6fb14097bf36584ec7ef651c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ccbc146f4ad4382879b5d8b526cac4e",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fda99413adc8497cba92a9c6416f77ad",
      "value": 231508
     }
    },
    "20a883d10ea945a9889429c5efb76a43": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2afc2c8167ad4d15ab6a35c1d1f1f3cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2cb043cfb0fc448e81a34710d65bb691": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2f44328dc8a04f84a59e26d08d343c77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "31cf1f106af54f7986b4d2f462125cca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_506baa608274455eae12e54e1d7b7dd5",
      "placeholder": "​",
      "style": "IPY_MODEL_103ff0dd04da40a69d85763bbf571490",
      "value": "vocab.txt: 100%"
     }
    },
    "347ce846f3564c58ac1eb26ae2f3adc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a20bdb762700449097d5010ce7d29921",
      "max": 4203,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2cb043cfb0fc448e81a34710d65bb691",
      "value": 4203
     }
    },
    "34edb9945bb54a69868d62ecd5b1ae56": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35205e647f814b97a2e977af48cc7a17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8580f622bac8422a992b56b5f39b3693",
      "max": 48,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca74fdefa7844f11b1d9052269efd585",
      "value": 48
     }
    },
    "37cede645a5b48a9b89df987b9ef91e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39f3c0ec5b4446cda93df7a999d4fd41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d5b841a59a3489ea5b4112182d4ae70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "470f2248b2444869914197c7481e10ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54e1a02cf71743ec9da128ff6c882f34",
      "placeholder": "​",
      "style": "IPY_MODEL_6429d7801ba14ed4861726bd7832ecee",
      "value": "config.json: 100%"
     }
    },
    "495cd338f7614f7a9a68fc3dc3916dcf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b6f81607c31496a83d5964983fed168": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c92f17cff2944a9a2a49be56649d81e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4df9f85ae9e943ceb3ed1a5d9e143f5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4ef091cf8c084af1acb95d62ffe1f0f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_495cd338f7614f7a9a68fc3dc3916dcf",
      "placeholder": "​",
      "style": "IPY_MODEL_dc17c88c62244e2f9a574ef895132a83",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "4ef76858d67f4ee6a99b305edb682a43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4f8713c60a0947ae86ee9d1d4de8ab3a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "506baa608274455eae12e54e1d7b7dd5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "522f6c1e5e7340039b7bbf700704699f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6294e85e97a8495bb18d11dff1991a79",
      "placeholder": "​",
      "style": "IPY_MODEL_91ee20aae0474a4c90a26463f72bacf1",
      "value": " 30465/30465 [00:00&lt;00:00, 31103.17 examples/s]"
     }
    },
    "54e1a02cf71743ec9da128ff6c882f34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5616046a1d1347cfbbe82fa67b79af1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "593b4aaa5c9e4e1898cc4d5064940770": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b1fd51b695b4c81ab351ed5b1f92966": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "60e84d7134b646f6b291b0439d9ec890": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6294e85e97a8495bb18d11dff1991a79": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63e64e93abca400293b7b7742bd94aaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b791095b95214d82af1e344a2b5898a2",
      "placeholder": "​",
      "style": "IPY_MODEL_a3fa870efa0f4463b1e60fa106f782f1",
      "value": " 268M/268M [00:01&lt;00:00, 287MB/s]"
     }
    },
    "6429d7801ba14ed4861726bd7832ecee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "64513b9e43254d5a9bbbffc9cb9a054c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6a7b459551104d47a90527c885647830": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f8713c60a0947ae86ee9d1d4de8ab3a",
      "placeholder": "​",
      "style": "IPY_MODEL_4b6f81607c31496a83d5964983fed168",
      "value": " 483/483 [00:00&lt;00:00, 39.6kB/s]"
     }
    },
    "6cdf8319c7184ee5b107cea1718b03c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4ef091cf8c084af1acb95d62ffe1f0f2",
       "IPY_MODEL_35205e647f814b97a2e977af48cc7a17",
       "IPY_MODEL_0eaf7b7abd71413597e0533efa3feda2"
      ],
      "layout": "IPY_MODEL_34edb9945bb54a69868d62ecd5b1ae56"
     }
    },
    "72df691ec7784ae7b0542171a7789555": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39f3c0ec5b4446cda93df7a999d4fd41",
      "placeholder": "​",
      "style": "IPY_MODEL_bb7f6dabc3ad4779a87b211227040606",
      "value": " 466k/466k [00:00&lt;00:00, 2.36MB/s]"
     }
    },
    "79144b8d1ec44b6bb3dcc381f7e829ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b91cb6b9b5e4ba891a6cfaab2d5948b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "80995fb6ed6f4557b1037dea2798b2fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb256ba48acd446bbe6b832c9558f7c6",
      "placeholder": "​",
      "style": "IPY_MODEL_4ef76858d67f4ee6a99b305edb682a43",
      "value": " 121857/121857 [00:04&lt;00:00, 25895.88 examples/s]"
     }
    },
    "80ef815be4ac4b64b3b3d99a6c43fadb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8580f622bac8422a992b56b5f39b3693": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c3f85da75764955a572261460db619b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e2f32fe9433d458dbc278da438f27449",
      "max": 121857,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_64513b9e43254d5a9bbbffc9cb9a054c",
      "value": 121857
     }
    },
    "8fc36b3359d24eb987c4be9eaf5d4e95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d6b2772488464e80b333332d6cd46eeb",
      "placeholder": "​",
      "style": "IPY_MODEL_5616046a1d1347cfbbe82fa67b79af1a",
      "value": "model.safetensors: 100%"
     }
    },
    "90455578408b4f33bc9a9c732d431236": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a2e056a63fd34329a21330a4fc69429d",
       "IPY_MODEL_347ce846f3564c58ac1eb26ae2f3adc4",
       "IPY_MODEL_9a01bb7c8f5e4940ba2444fa63a3db6f"
      ],
      "layout": "IPY_MODEL_b8beefe086284c7297004fcad9303617"
     }
    },
    "91ee20aae0474a4c90a26463f72bacf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9455512d944c418b87e4d1fe324619e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b93a51136d1146e08914db125d0259fb",
       "IPY_MODEL_c74948f6d29e4e0e868a3ccf836d2377",
       "IPY_MODEL_522f6c1e5e7340039b7bbf700704699f"
      ],
      "layout": "IPY_MODEL_0673f25c77b04212bc2b63f56454a5d2"
     }
    },
    "985174675f354a9bb2580a70ad2b7625": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b9e83e2c1d2f4970a0905efde0055e3d",
       "IPY_MODEL_8c3f85da75764955a572261460db619b",
       "IPY_MODEL_80995fb6ed6f4557b1037dea2798b2fb"
      ],
      "layout": "IPY_MODEL_016181d9f51a4abea666046fb95f2aee"
     }
    },
    "9a01bb7c8f5e4940ba2444fa63a3db6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b8d8eac2274a4040a4163663896753f7",
      "placeholder": "​",
      "style": "IPY_MODEL_01f82250d2e14f198dd0f561969d54ff",
      "value": " 4.20k/4.20k [00:00&lt;00:00, 379kB/s]"
     }
    },
    "9ccbc146f4ad4382879b5d8b526cac4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a20bdb762700449097d5010ce7d29921": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2e056a63fd34329a21330a4fc69429d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_593b4aaa5c9e4e1898cc4d5064940770",
      "placeholder": "​",
      "style": "IPY_MODEL_4df9f85ae9e943ceb3ed1a5d9e143f5c",
      "value": "Downloading builder script: 100%"
     }
    },
    "a3fa870efa0f4463b1e60fa106f782f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a5076ac1d4ad4c09aa1e6f12449915af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae6d2d013c03448887d53f8a0760e77d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b6c20dee590f40e5b2c96beefc1643e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b6cb26e5df334ac980005863a38581a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b791095b95214d82af1e344a2b5898a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8beefe086284c7297004fcad9303617": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8d8eac2274a4040a4163663896753f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b93a51136d1146e08914db125d0259fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_054e759a27a442afa8c666fc3d4c3f2e",
      "placeholder": "​",
      "style": "IPY_MODEL_e4633e912a57492497bc2fbfad02351c",
      "value": "Map: 100%"
     }
    },
    "b9e83e2c1d2f4970a0905efde0055e3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b1fd51b695b4c81ab351ed5b1f92966",
      "placeholder": "​",
      "style": "IPY_MODEL_a5076ac1d4ad4c09aa1e6f12449915af",
      "value": "Map: 100%"
     }
    },
    "bb7f6dabc3ad4779a87b211227040606": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "beb76be168f840d79c950efc4bdfc8d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8d378a142934482aa04a608ada9acdb",
      "max": 267954768,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b6c20dee590f40e5b2c96beefc1643e1",
      "value": 267954768
     }
    },
    "c0028b925edf40ecb94ee219a26dfe36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b6cb26e5df334ac980005863a38581a1",
      "placeholder": "​",
      "style": "IPY_MODEL_ae6d2d013c03448887d53f8a0760e77d",
      "value": " 152322/152322 [00:06&lt;00:00, 24189.50 examples/s]"
     }
    },
    "c3faf065f2d2435697fb730bf04ff1cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8fc36b3359d24eb987c4be9eaf5d4e95",
       "IPY_MODEL_beb76be168f840d79c950efc4bdfc8d1",
       "IPY_MODEL_63e64e93abca400293b7b7742bd94aaf"
      ],
      "layout": "IPY_MODEL_d41ef095cc5b4c23b31236b501dcf8c1"
     }
    },
    "c4055bbdeb6c4833b4fb50970248a154": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_31cf1f106af54f7986b4d2f462125cca",
       "IPY_MODEL_1c5d45fa6fb14097bf36584ec7ef651c",
       "IPY_MODEL_1aa8fdd087ce42bdb16ae19a38e79b27"
      ],
      "layout": "IPY_MODEL_ef3b72dea05d404db610152cb17ba8f9"
     }
    },
    "c45933bed8084b53b562621699870695": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37cede645a5b48a9b89df987b9ef91e2",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_13b1494b02194773a5fafe39b211c88a",
      "value": 466062
     }
    },
    "c56c3c1ff2a040b0952b5bac655b71dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6cbefbc4f4d4cfe98400aa05370e3e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0c86013f8194633be0ac27ca9b8876c",
      "max": 483,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7b91cb6b9b5e4ba891a6cfaab2d5948b",
      "value": 483
     }
    },
    "c74948f6d29e4e0e868a3ccf836d2377": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79144b8d1ec44b6bb3dcc381f7e829ce",
      "max": 30465,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_60e84d7134b646f6b291b0439d9ec890",
      "value": 30465
     }
    },
    "ca74fdefa7844f11b1d9052269efd585": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ca79aac952ca41c096241ad371c055fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b61ce44fb554db2bb906fb39b41b7e0",
      "placeholder": "​",
      "style": "IPY_MODEL_dcc6949983954dfb9b7f8bd0ad6cd6d8",
      "value": "Map: 100%"
     }
    },
    "cf51cae23d2b4026a4f890069983da4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ca79aac952ca41c096241ad371c055fe",
       "IPY_MODEL_0233456210f8469f85e648ba1c1f83ff",
       "IPY_MODEL_c0028b925edf40ecb94ee219a26dfe36"
      ],
      "layout": "IPY_MODEL_d4a7a1f7a10040ccb07704d845de6cd6"
     }
    },
    "d0080133b35c40578eba5db18dc82db6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0c86013f8194633be0ac27ca9b8876c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1cc110825034545a2532fc567b5325e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d41ef095cc5b4c23b31236b501dcf8c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4a7a1f7a10040ccb07704d845de6cd6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6b2772488464e80b333332d6cd46eeb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc17c88c62244e2f9a574ef895132a83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dcc6949983954dfb9b7f8bd0ad6cd6d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e2f32fe9433d458dbc278da438f27449": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4633e912a57492497bc2fbfad02351c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb256ba48acd446bbe6b832c9558f7c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec74087b5c9d4e538675956af72ae138": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fe87ce116db84d5ba3d00e4a8c249dc1",
       "IPY_MODEL_c45933bed8084b53b562621699870695",
       "IPY_MODEL_72df691ec7784ae7b0542171a7789555"
      ],
      "layout": "IPY_MODEL_4c92f17cff2944a9a2a49be56649d81e"
     }
    },
    "ef3b72dea05d404db610152cb17ba8f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1fa4e09b42c4904b9be033d33943e05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_470f2248b2444869914197c7481e10ae",
       "IPY_MODEL_c6cbefbc4f4d4cfe98400aa05370e3e7",
       "IPY_MODEL_6a7b459551104d47a90527c885647830"
      ],
      "layout": "IPY_MODEL_d0080133b35c40578eba5db18dc82db6"
     }
    },
    "f8d378a142934482aa04a608ada9acdb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fda99413adc8497cba92a9c6416f77ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fe87ce116db84d5ba3d00e4a8c249dc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20a883d10ea945a9889429c5efb76a43",
      "placeholder": "​",
      "style": "IPY_MODEL_2f44328dc8a04f84a59e26d08d343c77",
      "value": "tokenizer.json: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
