[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I‚Äôd like to learn the Hugging Face ecosystem more.\nSo this website is dedicated to documenting my learning journey + creating usable resources and tutorials for others.\nIt‚Äôs made by Daniel Bourke and will be in a similiar style to learnpytorch.io.\nYou can see more of my tutorials on:\n\nYouTube\nGitHub"
  },
  {
    "objectID": "extras/todo.html",
    "href": "extras/todo.html",
    "title": "Learn Hugging Face ü§ó",
    "section": "",
    "text": "Move todo.md into index.md for easier navigation (one file is enough)\nAdd ‚Äúgetting setup‚Äù file to get started locally with the required dependencies\nAdd text classification dataset creation notebook (so people can see where the data comes from)\nAdd a Hugging Face ecosystem overview (transformers, datasets, tokenizers, torch, Hugging Face Hub, Hugging Face Spaces, etc.)\nAdd a fav icon\nMake code-only versions of notebooks? e.g.¬†text stripped away and only code is left"
  },
  {
    "objectID": "extras/todo.html#quarto-misc",
    "href": "extras/todo.html#quarto-misc",
    "title": "Learn Hugging Face ü§ó",
    "section": "Quarto misc",
    "text": "Quarto misc\n\nCreate share cards - https://quarto.org/docs/websites/website-tools.html#twitter-cards\n\nSee here for share image - https://quarto.org/docs/websites/website-tools.html#preview-images"
  },
  {
    "objectID": "extras/setup.html",
    "href": "extras/setup.html",
    "title": "Getting setup for the Hugging Face ecosystem",
    "section": "",
    "text": "The following steps are to help you get started with the Hugging Face ecosystem.\nBest to follow the ‚ÄúStart here‚Äù steps and then go through the other setup steps as necessary.\n\n\n\nCreate a free Hugging Face account at https://huggingface.co/join.\nCreate a Hugging Face access token with read and write access at https://huggingface.co/settings/tokens.\n\nYou can create a read/write token using the fine-grained settings and selecting all the appropriate options.\nRead more on Hugging Face access tokens at https://huggingface.co/docs/hub/en/security-tokens.\n\n\n\n\n\n\nTo read from and write to your Hugging Face Hub account, you‚Äôll need to set up an access token. You can have one token for reading and one for writing. However, I personally use a single token for reading and writing.\n\n\nNote: Do not share your token with others. Always keep it private and avoid saving it in raw text format.\n\n\n\nNote: If you‚Äôre unfamiliar with Google Colab, I‚Äôd recommend going through Sam Witteveen‚Äôs video Colab 101 and then Advanced Colab to learn more.\n\nFollow the steps in Start here.\nAdd your Hugging Face read/write token as a Secret in Google Colab.\n\nNaming this Secret HF_TOKEN will mean that Hugging Face libraries automatically recognize your token for future use.\n\n\n\n\n\n\nFor accessing models and datasets from the Hugging Face Hub (both read and write) inside Google Colab, you‚Äôll need to add your Hugging Face token as a Secret in Google Colab. Once you give your Google Colab notebook access to the token, it can be used by Hugging Face libraries to interact with the Hugging Face Hub.\n\n\nAlternatively, if you need to force relogin for a notebook session, you can run:\nimport huggingface_hub # requires !pip install huggingface_hub\n\n# Login to Hugging Face\nhuggingface_hub.login()\nAnd enter your token in the box that appears (note: this token will only be active for the current notebook session and will delete when your Google Colab instance terminates).\n\n\n\n\nFollow the steps in Start here.\nInstall the Hugging Face CLI with pip install -U \"huggingface_hub[cli]\".\nFollow the setup steps mentioned in https://huggingface.co/docs/huggingface_hub/en/guides/cli.\n\n\n\n\nWe‚Äôll need to install the following libraries from the Hugging Face ecosystem:\n\ntransformers - comes pre-installed on Google Colab but if you‚Äôre running on your local machine, you can install it via pip install transformers.\ndatasets - a library for accessing and manipulating datasets on and off the Hugging Face Hub, you can install it via pip install datasets.\nevaluate - a library for evaluating machine learning model performance with various metrics, you can install it via pip install evaluate.\naccelerate - a library for training machine learning models faster, you can install it via pip install accelerate.\ngradio - a library for creating interactive demos of machine learning models, you can install it via pip install gradio."
  },
  {
    "objectID": "extras/setup.html#tk---start-here-universal-steps",
    "href": "extras/setup.html#tk---start-here-universal-steps",
    "title": "Getting setup for the Hugging Face ecosystem",
    "section": "",
    "text": "Create a free Hugging Face account at https://huggingface.co/join.\nCreate a Hugging Face access token with read and write access at https://huggingface.co/settings/tokens.\n\nYou can create a read/write token using the fine-grained settings and selecting all the appropriate options.\nRead more on Hugging Face access tokens at https://huggingface.co/docs/hub/en/security-tokens.\n\n\n\n\n\n\nTo read from and write to your Hugging Face Hub account, you‚Äôll need to set up an access token. You can have one token for reading and one for writing. However, I personally use a single token for reading and writing. Note: Do not share your token with others. Always keep it private and avoid saving it in raw text format."
  },
  {
    "objectID": "extras/setup.html#tk---getting-setup-on-google-colab",
    "href": "extras/setup.html#tk---getting-setup-on-google-colab",
    "title": "Getting setup for the Hugging Face ecosystem",
    "section": "",
    "text": "Follow the steps in Start here.\nAdd your Hugging Face read/write token as a Secret in Google Colab.\n\nNaming this Secret HF_TOKEN will mean that Hugging Face libraries automatically recognize your token for future use.\n\n\n\n\n\n\nFor accessing models and datasets from the Hugging Face Hub (both read and write) inside Google Colab, you‚Äôll need to add your Hugging Face token as a Secret in Google Colab. Once you give your Google Colab notebook access to the token, it can be used by Hugging Face libraries to interact with the Hugging Face Hub.\n\n\nAlternatively, if you need to force relogin for a notebook session, you can run:\nimport huggingface_hub # requires !pip install huggingface_hub\n\n# Login to Hugging Face\nhuggingface_hub.login()\nAnd enter your token in the box that appears (note: this token will only be active for the current notebook session and will delete when your Google Colab instance terminates)."
  },
  {
    "objectID": "extras/setup.html#tk---getting-started-locally",
    "href": "extras/setup.html#tk---getting-started-locally",
    "title": "Getting setup for the Hugging Face ecosystem",
    "section": "",
    "text": "Follow the steps in Start here.\nInstall the Hugging Face CLI with pip install -U \"huggingface_hub[cli]\".\nFollow the setup steps mentioned in https://huggingface.co/docs/huggingface_hub/en/guides/cli."
  },
  {
    "objectID": "extras/setup.html#installing-hugging-face-libraries",
    "href": "extras/setup.html#installing-hugging-face-libraries",
    "title": "Getting setup for the Hugging Face ecosystem",
    "section": "",
    "text": "We‚Äôll need to install the following libraries from the Hugging Face ecosystem:\n\ntransformers - comes pre-installed on Google Colab but if you‚Äôre running on your local machine, you can install it via pip install transformers.\ndatasets - a library for accessing and manipulating datasets on and off the Hugging Face Hub, you can install it via pip install datasets.\nevaluate - a library for evaluating machine learning model performance with various metrics, you can install it via pip install evaluate.\naccelerate - a library for training machine learning models faster, you can install it via pip install accelerate.\ngradio - a library for creating interactive demos of machine learning models, you can install it via pip install gradio."
  },
  {
    "objectID": "extras/resources.html",
    "href": "extras/resources.html",
    "title": "Learn Hugging Face ü§ó",
    "section": "",
    "text": "See the Pytorch extra resources for some ideas: https://www.learnpytorch.io/pytorch_extra_resources/\nHugging Face NLP course: https://huggingface.co/learn/nlp-course/chapter0/1\nHugging Face forum: https://discuss.huggingface.co/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "",
    "text": "Website dedicated to teaching the Hugging Face ecosystem with practical examples.\nEach example will include an end-to-end approach of starting with a dataset (custom or existing), building and evaluating a model and creating a demo to share.\nTeaching style:\nA machine learning cooking show! üë®‚Äçüç≥\nMottos:\nProject style:\nData, model, demo!\nThis will be our (rough) workflow:"
  },
  {
    "objectID": "index.html#faq",
    "href": "index.html#faq",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "FAQ",
    "text": "FAQ\n\nIs this an official Hugging Face website?\n\nNo, it‚Äôs a personal project by myself (Daniel Bourke) to learn and help others learn the Hugging Face ecosystem.\n\nHow is this website made?\n\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "index.html#how-is-this-website-made",
    "href": "index.html#how-is-this-website-made",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "How is this website made?",
    "text": "How is this website made?\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "extras/glossary.html",
    "href": "extras/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Glossary\nThis is messy and a work in progress. Will tidy up later.\n\nPredictive vs generative AI = predictive AI -&gt; machine readable outputs, generative AI -&gt; human readable outputs. Predictive style models take in data and map it to a fixed output space (e.g.¬†a text classification model predicting whether an email is spam or not). Generative AI models take in data and generate an unbounded response (though theorectically this response is bounded by the training distribution), such as, a chat system taking in natural language instructions and producing natural language as output. Generative AI models can be turned into predictive-style models, for example a generative LLM could produce JSON outputs if instructed/constrained to do so.\nTransformer = A deep learning model that adopts the attention mechanism to draw global dependencies between input and output\nTokenization = turning a series of data (text or image) into a series of tokens, where a token is a numerical representation of the input data, for example, in the case of text, tokenization could mean turning the words in a sentence into numbers (e.g.¬†‚Äúhello world‚Äù -&gt; [101, 102])\nTokens = a token is a letter, word or word-piece (word) that a model uses to represent input data, for example, in the case of text, a token could be a word (e.g.¬†‚Äúhello‚Äù) or a word-piece (e.g.¬†‚Äúhell‚Äù and ‚Äúo‚Äù), see: https://platform.openai.com/tokenizer for an example\ntransformers = A Python library by Hugging Face that provides a wide range of pre-trained transformer models, fine-tuning tools, and utilities to use them\ndatasets = A Python library by Hugging Face that provides a wide range of datasets for NLP and CV tasks\ntokenizers = A Python library by Hugging Face that provides a wide range of tokenizers for NLP tasks\nevaluate = A Python library by Hugging Face with premade evaluation functions for various tasks\ntorch = PyTorch, an open-source machine learning library\ntransformers.pipeline = an abstraction to get a machine learning pipeline up and running in a few lines of code, handles data preprocessing and device placement behind the scences. For example, transformers.pipeline(\"text-classification\") can be used to tokenize input text and classify it.\ntransfer learning = taking what one model has learned and applying it to another task (e.g.¬†a model which has learned across many millions of words of text from the internet and then adjusting it to work with your smaller dataset)\nfine-tuning = a type of transfer learning where you take the existing patterns of one model (usually trained on a very large dataset) and customize them to work for your smaller dataset\nhyperparameters = values you can set to adjust training settings, for example, learning rate is a hyperparameter that is adjustable\nHugging Face Hub (or Hub for short) = Place to store datasets, models, and other resources of your own + find existing datasets, models & scripts others have shared. If you are familiar with GitHub, Hugging Face is like the GitHub of machine learning.\nAuto Classes = A series of classes in transformers which enables automatic loading of preprocessor or model classes based on the name or path of the model. For example you can load the processor for microsoft/conditional-detr-resnet-50 with transformers.AutoImageProcessor(microsoft/conditional-detr-resnet-50) or the model architecture with transformers.AutoModelForObjectDetection(microsoft/conditional-detr-resnet-50).\nHugging Face Spaces = A platform to share and run machine learning apps/demos, these can be built with HTML, Gradio or Streamlit\nHF = Hugging Face\nNLP = Natural Language Processing\nCV = Computer Vision\nImage classification = Classify an image in a single or multiple classes (classifying something as multiple items or labels such as [warm, well lit, sunset] is also known as tagging or more specifically, image tagging), for example, is a photo of food or not food.\nObject detection = Detect and locate an item in an image or series of images (e.g.¬†a video). An item can be almost anything in an image, for example, a licence plate, a person, a weed in a garden or a small bug on the body of a bee.\nBounding box = A box, often rectangular in nature, drawn around an item in an image to indicate its location. Can come in several different forms such as XYXY, XYWH and CXCYWH (see more in A Guide to Bounding Box Formats and How to Draw Them).\nTPU = Tensor Processing Unit\nGPU = Graphics Processing Unit\nLearning rate = Often the most important hyperparameter to tune. It is proportional with the amount an optimizer will update a model‚Äôs parameters every update step. A higher amount means larger updates (though sometimes too large) a lower amount means smaller updates (though sometimes not enough). The most ideal learning rate is experimental. Common values include 0.001, 0.0001, 0.0005, 0.00001, 0.00005 (though the learning rate can be any value). Many optimizers have decent default learning rates. For example, the Adam optimizer (a common and generally well performing optimizer) in PyTorch (torch.optim.Adam) has a default learning rate of 0.001. For fine-tuning an already trained model a learning rate of 10x smaller than the default is a good rule of thumb (e.g.¬†if a model was trained with a learning rate of 0.001, fine-tuning with 0.0001 is common). The learning rate does not have to be static and can change dynamcially during training, this practice is referred to as learning rate scheduling.\nInference = using a trained (or untrained) model to make predictions on a given piece of data. The model infers what the output should be based on the inputs. Inference is often much faster than training on a sample per sample basis because no weights get updated during inference. Though, when compared to training, inference can often take more compute in the long run. Because a model can be trained once but then used for inference millions of times (or more) over the next several months (or longer).\nPrediction probability = the probability of a model‚Äôs prediction for a given input, is a score between 0 and 1 with 1 being the highest, for example, a model may have a prediction probability of 0.95, this would mean it‚Äôs quite confident with its prediction but it doesn‚Äôt mean it‚Äôs correct. A good way to inspect potential issues with a dataset is to show examples in the test set which have a high prediction probability but are wrong (e.g.¬†pred prob = 0.98 but the prediction was incorrect).\nHugging Face Pipeline (pipeline) = A high-level API for using model for various tasks (e.g.¬†text-classification, audio-classification, image-classification, object-detection and more), see the docs: https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/pipelines#transformers.pipeline\nloss value = Measure of how wrong your model is by a given metric. A perfect model will have a loss value of 0 (it is able to predict the data perfectly), though this is highly unlikely in practice (there are no perfect models). Ideally, the loss value will go down (towards 0) as training goes on. If the loss value on the training set is lower than the loss value on the test set, the model is likely overfitting (memorizing the training set too well rather than learning generalizable patterns to unseen data). To fix overfitting, introduce more regularization. To fix underfitting (loss not going down), introduce more learning capacity (more data, more parameters in the model, longer training). Machine learning is a constant battle between overfitting and underfitting.\nRandom seed = Value to flavour the randomness of an operation. For example, if you set a random seed to 42 the numbers produced by a random generator will be random but flavoured by the seed. This means if the seed stays at 42, subsequent calls of the same operation will return the same values. Not setting a random seed will result in different random values each time. Setting a random seed is done to ensure reproducibility of an operation. This is helpful when performing experiments and you do not want the outputs to be random each time.\nSynthetic data generation = using a model such as a generative Large Language Model (LLM) to generate synthetic pieces of data for a specific problem. For example, getting an LLM to generate food and not food image captions to create a binary text classification model. Synthetic data is very helpful when bootstrapping a machine learning problem. Though it is advised to only train on synthetic data and to evaluate on real data whenever possible.\nPre-trained models = models which have already been trained on a large dataset, for example, text-based models which have gone through many millions of words of text (e.g.¬†all of Wikipedia and 1000s of books) or image-based models which have seen millions of images (e.g.¬†models trained on ImageNet). In essence, any model which has already spent a large amount of time learning patterns in data. These patterns can then be adjusted for your own sample problems, often with much much smaller amounts of data for excellent results. The process of customizing a pre-trained model for a specific problem is called transfer learning (transferring what an existing model knows to your own problem).\nTraining/test split = One of the most important concepts in machine learning. Train models on the training data and evaluate them on the test data. The test data should never be seen by a model during training. Think of the test data as the final exam in a university course. A model should be able to learn enough patterns in the training set to perform well on the test set. Just like a student should be able to learn enough on course materials to do well on the final exam. If a model performs well on the training set but not well on the test set, this is known as overfitting, as in, the model memorizes the training set rather than learning generalizable patterns to unseen data. If a model performs poorly on both the training set and the test set, this is known as underfitting.\n\nPrediction probabilities = a value assigned to a model‚Äôs prediction on a certain sample after its output logits have passed through an activation function such as Softmax or Sigmoid. For example, in a binary classification problem of whether an image is of food or not of food, a model could assign a prediction probability of the image being 0.98 food and 0.02 not food. Prediction probabilities do not indicate how right a prediction is, more so, how confident a model is in that prediction. The closer a prediction probability to 1, the higher the model‚Äôs confidence in the prediction. A good evaluation step is to inspect samples with low prediction probabilities (the model seems to get confused on them) or inspect test samples where the model has a high prediction probability but the prediction is wrong (these predictions are often referred to as most wrong predictions).\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html",
    "href": "notebooks/hugging_face_text_classification_tutorial.html",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "",
    "text": "Source code on GitHub | Online book version | Setup guide | Video Course (step by step walkthrough)",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#overview",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#overview",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "1 Overview",
    "text": "1 Overview\nWelcome to the Learn Hugging Face Text Classificaiton project!\nThis tutorial is hands-on and focused on writing resuable code.\nWe‚Äôll start with a text dataset, build a model to classify text samples and then share our model as a demo others can use.\nTo do so, we‚Äôll be using a handful of helpful open-source tools from the Hugging Face ecosystem.\n\n\n\n\nWe‚Äôre going to put on our internship hats and build a food not food text classification model using tools from the Hugging Face ecosystem.\n\n\n\n\n\n\n\n\nNote\n\n\n\nFeel to keep reading through the notebook but if you‚Äôd like to run the code yourself, be sure to go through the setup guide first.\n\n\n\n1.1 What we‚Äôre going to build\nWe‚Äôre going to be bulding a food/not_food text classification model.\nGiven a piece of a text (such as an image caption), our model will be able to predict if it‚Äôs about food or not.\nThis is the same kind of model I use in my own work on Nutrify (an app to help people learn about food).\nMore specifically, we‚Äôre going to follow the following steps:\n\nData: Problem defintion and dataset preparation - Getting a dataset/setting up the problem space.\nModel: Finding, training and evaluating a model - Finding a text classification model suitable for our problem on Hugging Face and customizing it to our own dataset.\nDemo: Creating a demo and put our model into the real world - Sharing our trained model in a way others can access and use.\n\nBy the end of this project, you‚Äôll have a trained model and demo on Hugging Face you can share with others:\n\nfrom IPython.display import HTML \n\nHTML(\"\"\"\n&lt;iframe\n    src=\"https://mrdbourke-learn-hf-food-not-food-text-classifier-demo.hf.space\"\n    frameborder=\"0\"\n    width=\"850\"\n    height=\"650\"\n&gt;&lt;/iframe&gt;\n\"\"\")\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote this is a hands-on project, so we‚Äôll be focused on writing reusable code and building a model that can be used in the real world. If you are looking for explainers to the theory of what we‚Äôre doing, I‚Äôll leave links in the extra-curriculum section.\n\n\n\n\n1.2 What is text classification?\nText classification is the process of assigning a category to a piece of text.\nWhere a category can be almost anything and a piece of text can be a word, phrase, sentence, paragraph or entire document.\nExample text classification problems include:\n\n\n\n\n\n\n\n\nProblem\nDescription\nProblem Type\n\n\n\n\nSpam/phishing email detection\nIs an email spam or not spam? Or is it a phishing email or not?\nBinary classification (one thing or another)\n\n\nSentiment analysis\nIs a piece of text positive, negative or neutral? Such as classifying product reviews into good/bad/neutral.\nMulti-class classification (one thing from many)\n\n\nLanguage detection\nWhat language is a piece of text written in?\nMulti-class classification (one thing from many)\n\n\nTopic classification\nWhat topic(s) does a news article belong to?\nMulti-label classification (one or more things from many)\n\n\nHate speech detection\nIs a comment hateful or not hateful?\nBinary classification (one thing or another)\n\n\nProduct categorization\nWhat categories does a product belong to?\nMulti-label classification (one or more things from many)\n\n\nBusiness email classification\nWhich category should this email go to?\nMulti-class classification (one thing from many)\n\n\n\nText classification is a very common problem in many business settings.\nFor example, a project I‚Äôve worked on previously as a machine learning engineer was building a text classification model to classify different insurance claims into claimant_at_fault/claimant_not_at_fault for a large insurance company.\nIt turns out the deep learning-based model we built was very good (98%+ accuracy on the test dataset).\n\n\n\n\nAn example text classification problem I once worked on to classify insurance claim texts into at fault or not fault. This result of the model would send the claim to a different department in the insurance company.\n\n\nSpeaking of models, there are several different kinds of models you can use for text classification.\nAnd each will have its pros and cons depending on the problem you‚Äôre working on.\nExample text classification models include:\n\n\n\nModel\nDescription\nPros\nCons\n\n\n\n\nRule-based\nUses a set of rules to classify text (e.g.¬†if text contains ‚Äúsad‚Äù -&gt; sentiment = low)\nSimple, easy to understand\nRequires manual creation of rules\n\n\nBag of Words\nCounts the frequency of words in a piece of text\nSimple, easy to understand\nDoesn‚Äôt capture word order\n\n\nTF-IDF\nWeighs the importance of words in a piece of text\nSimple, easy to understand\nDoesn‚Äôt capture word order\n\n\nDeep learning-based models\nUses neural networks to learn patterns in text\nCan learn complex patterns at scale\nCan require large amounts of data/compute power to run, not as easy to understand (can be hard to debug)\n\n\n\nFor our project, we‚Äôre going to go with a deep learning model.\nWhy?\nBecause Hugging Face helps us do so.\nAnd in most cases, with a quality dataset, a deep learning model will often perform better than a rule-based or other model.\n\n\n1.3 Why train your own text classification models?\nYou can customize pre-trained models for text classification as well as API-powered models and LLMs such as GPT, Gemini, Claude or Mistral.\nDepending on your requirements, there are several pros and cons for using your own model versus using an API.\nTraining/fine-tuning your own model:\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nControl: Full control over model lifecycle.\nCan be complex to get setup.\n\n\nNo usage limits (aside from compute constraints).\nRequires dedicated compute resources for training/inference.\n\n\nCan train once and deploy everywhere/whenever you want (for example, Tesla deploying a model to all self-driving cars).\nRequires maintenance over time to ensure performance remains up to par.\n\n\nPrivacy: Data can be kept in-house/app and doesn‚Äôt need to go to a third party.\nCan require longer development cycles compared to using existing APIs.\n\n\nSpeed: Customizing a small model for a specific use case often means it runs much faster.\n\n\n\n\nUsing a pre-built model API (e.g.¬†GPT, Gemini, Claude, Mistral):\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nEase of use: often can be setup within a few lines of code.\nIf the model API goes down, your service goes down.\n\n\nNo maintenance of compute resources.\nData is required to be sent to a third-party for processing.\n\n\nAccess to the most advanced models.\nThe API may have usage limits per day/time period.\n\n\nCan scale if usage increases.\nCan be much slower than using dedicated models due to requiring an API call.\n\n\n\nFor this project, we‚Äôre going to focus on fine-tuning our own model.\n\n\n1.4 Workflow we‚Äôre going to follow\nOur motto is data, model, demo!\nSo we‚Äôre going to follow the rough workflow of:\n\nCreate and preprocess data.\nDefine the model we‚Äôd like use with transformers.AutoModelForSequenceClassification (or another similar model class).\nDefine training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\nPass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\nTrain the model by calling Trainer.train().\nSave the model (to our local machine or to the Hugging Face Hub).\nEvaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.\n\nI say rough because machine learning projects are often non-linear in nature.\nAs in, because machine learning projects involve many experiments, they can kind of be all over the place.\nBut this worfklow will give us some good guidelines to follow.\n\n\n\n\nA general Hugging Face workflow from idea to shared model and demo using tools from the Hugging Face ecosystem. You‚Äôll notice some of the steps don‚Äôt match with our workflow outline above. This is because the text-based workflow outline above breaks some of the steps down for educational purposes. These kind of workflows are not set in stone and are more of guide than specific directions. See information on each of the tools in the Hugging Face documentation.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#importing-necessary-libraries",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#importing-necessary-libraries",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "2 Importing necessary libraries",
    "text": "2 Importing necessary libraries\nLet‚Äôs get started!\nFirst, we‚Äôll import the required libraries.\nIf you‚Äôre running on your local computer, be sure to check out the getting setup guide to make sure you have everything you need.\nIf you‚Äôre using Google Colab, many of them the following libraries will be installed by default.\nHowever, we‚Äôll have to install a few extras to get everything working.\n\n\n\n\n\n\nNote\n\n\n\nIf you‚Äôre running on Google Colab, this notebook will work best with access to a GPU. To enable a GPU, go to Runtime ‚û°Ô∏è Change runtime type ‚û°Ô∏è Hardware accelerator ‚û°Ô∏è GPU.\n\n\nWe‚Äôll need to install the following libraries from the Hugging Face ecosystem:\n\ntransformers - comes pre-installed on Google Colab but if you‚Äôre running on your local machine, you can install it via pip install transformers.\ndatasets - a library for accessing and manipulating datasets on and off the Hugging Face Hub, you can install it via pip install datasets.\nevaluate - a library for evaluating machine learning model performance with various metrics, you can install it via pip install evaluate.\naccelerate - a library for training machine learning models faster, you can install it via pip install accelerate.\ngradio - a library for creating interactive demos of machine learning models, you can install it via pip install gradio.\n\nWe can also check the versions of our software with package_name.__version__.\n\n# Install dependencies (this is mostly for Google Colab, as the other dependences are available by default in Colab)\ntry:\n  import datasets, evaluate, accelerate\n  import gradio as gr\nexcept ModuleNotFoundError:\n  !pip install -U datasets evaluate accelerate gradio # -U stands for \"upgrade\" so we'll get the latest version by default\n  import datasets, evaluate, accelerate\n  import gradio as gr\n\nimport random\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport transformers\n\nprint(f\"Using transformers version: {transformers.__version__}\")\nprint(f\"Using datasets version: {datasets.__version__}\")\nprint(f\"Using torch version: {torch.__version__}\")\n\nUsing transformers version: 4.43.2\nUsing datasets version: 2.20.0\nUsing torch version: 2.4.0+cu121\n\n\nWonderful, as long as your versions are the same or higher to the versions above, you should be able to run the code below.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#getting-a-dataset",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#getting-a-dataset",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "3 Getting a dataset",
    "text": "3 Getting a dataset\nOkay, now we‚Äôre got the required libraries, let‚Äôs get a dataset.\nGetting a dataset is one of the most important things a machine learning project.\nThe dataset you often determines the type of model you use as well as the quality of the outputs of that model.\nMeaning, if you have a high quality dataset, chances are, your future model could also have high quality outputs.\nIt also means if your dataset is of poor quality, your model will likely also have poor quality outputs.\nFor a text classificaiton problem, your dataset will likely come in the form of text (e.g.¬†a paragraph, sentence or phrase) and a label (e.g.¬†what category the text belongs to).\n\n\n\n\nOur  food not food image caption dataset on the Hugging Face Hub.\n\n\nIn our case, our dataset comes in the form of a collection of synthetic image captions and their corresponding labels (food or not food).\nThis is a dataset I‚Äôve created earlier to help us practice building a text classification model.\nYou can find it on Hugging Face under the name mrdbourke/learn_hf_food_not_food_image_captions.\n\n\n\n\n\n\nFood Not Food Image Caption Dataset Creation\n\n\n\nYou can see how the Food Not Food image caption dataset was created in the example Google Colab notebook.\nA Large Language Model (LLM) was asked to generate various image caption texts about food and not food.\nGetting another model to create data for a problem is known as synthetic data generation and is a very good way of bootstrapping towards creating a model.\nOne workflow would be to use real data wherever possible and use synthetic data to boost when needed.\nNote that it‚Äôs always advised to evaluate/test models on real-life data as opposed to synthetic data.\n\n\n\n3.1 Where can you get more datasets?\nThe are many different places you can get datasets for text-based problems.\nOne of the best places is on the Hugging Face Hub, specifically huggingface.co/datasets.\nHere you can find many different kinds of problem specific data such as text classification.\nThere are also many more datasets available on Kaggle Datasets.\nAnd thanks to the power of LLMs (Large Language Models), you can also now create your own text classifications by generating samples (this is how I created the dataset for this project).\n\n\n\n\nHugging Face Datasets and Kaggle Datasets are two of the best places on the internet to find all kinds of different datasets. If you can‚Äôt find an existing dataset related to your problem you can either use your own data or potentially generate synthetic data samples with an LLM. For more on synthetic data generation, see the Creating Synthetic Data article by NVIDIA.\n\n\n\n\n3.2 Loading the dataset\nOnce we‚Äôve found/prepared a dataset on the Hugging Face Hub, we can use the Hugging Face datasets library to load it.\nTo load a dataset we can use the datasets.load_dataset(path=NAME_OR_PATH_OF_DATASET) function and pass it the name/path of the dataset we want to load.\nIn our case, our dataset name is mrdbourke/learn_hf_food_not_food_image_captions (you can also change this for your own dataset).\nAnd since our dataset is hosted on Hugging Face, when we run the following code for the first time, it will download it.\nIf your target dataset is quite large, this download may take a while.\nHowever, once the dataset is downloaded, subsequent reloads will be mush faster.\n\n# Load the dataset from Hugging Face Hub\ndataset = datasets.load_dataset(path=\"mrdbourke/learn_hf_food_not_food_image_captions\")\n\n# Inspect the dataset\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 250\n    })\n})\n\n\nDataset loaded!\nLooks like our dataset has two features, text and label.\nAnd 250 total rows (the number of examples in our dataset).\nWe can check the column names with dataset.column_names.\n\n# What features are there?\ndataset.column_names\n\n{'train': ['text', 'label']}\n\n\nLooks like our dataset comes with a train split already (the whole dataset).\nWe can access the train split with dataset[\"train\"] (some datasets also come with built-in \"test\" splits too).\n\n# Access the training split\ndataset[\"train\"]\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 250\n})\n\n\nHow about we check out a single sample?\nWe can do so with indexing.\n\ndataset[\"train\"][0]\n\n{'text': 'Creamy cauliflower curry with garlic naan, featuring tender cauliflower in a rich sauce with cream and spices, served with garlic naan bread.',\n 'label': 'food'}\n\n\nNice! We get back a dictionary with the keys text and label.\nThe text key contains the text of the image caption and the label key contains the label (food or not food).\n\n\n3.3 Inspect random examples from the dataset\nAt 250 total samples, our dataset isn‚Äôt too large.\nSo we could sit here and explore the samples one by one.\nBut whenever I interact with a new dataset, I like to view a bunch of random examples and get a feel of the data.\nDoing so is inline with the data explorer‚Äôs motto: visualize, visualize, visualize!\nAs a rule of thumb, I like to view at least 20-100 random examples when interacting with a new dataset.\nLet‚Äôs write some code to view 5 random indexes of our data and their corresponding text and labels at a time.\n\nimport random\n\nrandom_indexs = random.sample(range(len(dataset[\"train\"])), 5)\nrandom_samples = dataset[\"train\"][random_indexs]\n\nprint(f\"[INFO] Random samples from dataset:\\n\")\nfor item in zip(random_samples[\"text\"], random_samples[\"label\"]):\n    print(f\"Text: {item[0]} | Label: {item[1]}\")\n\n[INFO] Random samples from dataset:\n\nText: Set of spatulas kept in a holder | Label: not_food\nText: Mouthwatering paneer tikka masala, featuring juicy paneer in a rich tomato-based sauce, garnished with fresh coriander leaves. | Label: food\nText: Pair of reading glasses left open on a book | Label: not_food\nText: Set of board games stacked on a shelf | Label: not_food\nText: Two handfuls of bananas in a fruit bowl with grapes on the side, the fruit bowl is blue | Label: food\n\n\nBeautiful! Looks like our data contains a mix of shorter and longer sentences (between 5 and 20 words) of texts about food and not food.\nWe can get the unique labels in our dataset with dataset[\"train\"].unique(\"label\").\n\n# Get unique label values\ndataset[\"train\"].unique(\"label\")\n\n['food', 'not_food']\n\n\nIf our dataset is small enough to fit into memory, we can count the number of different labels with Python‚Äôs collections.Counter (a method for counting objects in an iterable or mapping).\n\n# Check number of each label\nfrom collections import Counter\n\nCounter(dataset[\"train\"][\"label\"])\n\nCounter({'food': 125, 'not_food': 125})\n\n\nExcellent, looks like our dataset is well balanced with 125 samples of food and 125 samples of not food.\nIn a binary classification case, this is ideal.\nIf the classes were dramatically unbalanced (e.g.¬†90% food and 10% not food) we might have to consider collecting/creating more data.\nBut best to train a model and see how it goes before making any drastic dataset changes.\nBecause our dataset is small, we could also inspect it via a pandas DataFrame (however, this may not be possible for extremely large datasets).\n\n# Turn our dataset into a DataFrame and get a random sample\nfood_not_food_df = pd.DataFrame(dataset[\"train\"])\nfood_not_food_df.sample(7)\n\n\n\n\n\n\n\n\ntext\nlabel\n\n\n\n\n142\nA slice of pizza with a generous amount of shr...\nfood\n\n\n6\nPair of reading glasses left open on a book\nnot_food\n\n\n97\nTelescope positioned on a balcony\nnot_food\n\n\n60\nA close-up of a family playing a board game wi...\nnot_food\n\n\n112\nRich and spicy lamb rogan josh with yogurt gar...\nfood\n\n\n181\nA steaming bowl of fiery chicken curry, infuse...\nfood\n\n\n197\nPizza with a stuffed crust, oozing with cheese\nfood\n\n\n\n\n\n\n\n\n# Get the value counts of the label column\nfood_not_food_df[\"label\"].value_counts()\n\nlabel\nfood        125\nnot_food    125\nName: count, dtype: int64",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#preparing-data-for-text-classification",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#preparing-data-for-text-classification",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "4 Preparing data for text classification",
    "text": "4 Preparing data for text classification\nWe‚Äôve got our data ready but there are a few steps we‚Äôll need to take before we can model it.\nThe main two being:\n\nTokenization - turning our text into a numerical representation (machines prefer numbers rather than words), for example, {\"a\": 0, \"b\": 1, \"c\": 2...}.\nCreating a train/test split - right now our data is in a training split only but we‚Äôll create a test set to evaluate our model‚Äôs performance.\n\nThese don‚Äôt necessarily have to be in order either.\nBefore we get to them, let‚Äôs create a small mapping from our labels to numbers.\nIn the same way we need to tokenize our text into numerical representation, we also need to do the same for our labels.\n\n4.1 Creating a mapping from labels to numbers\nOur machine learning model will want to see all numbers (people do well with text, computers do well with numbers).\nThis goes for text as well as label input.\nSo let‚Äôs create a mapping from our labels to numbers.\nSince we‚Äôve only got a couple of labels (\"food\" and \"not_food\"), we can create a dictionary to map them to numbers, however, if you‚Äôve got a fair few labels, you may want to make this mapping programmatically.\nWe can use these dictionaries later on for our model training as well as evaluation.\n\n# Create mapping from id2label and label2id\nid2label = {0: \"not_food\", 1: \"food\"}\nlabel2id = {\"not_food\": 0, \"food\": 1}\n\nprint(f\"Label to ID mapping: {label2id}\")\nprint(f\"ID to Label mapping: {id2label}\")\n\nLabel to ID mapping: {'not_food': 0, 'food': 1}\nID to Label mapping: {0: 'not_food', 1: 'food'}\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn a binary classification task (such as what we‚Äôre working on), the positive class, in our case \"food\", is usually given the label 1 and the negative class (\"not_food\") is given the label 0.\n\n\nRather than hard-coding our label to ID maps, we can also create them programmatically from the dataset (this is helpful if you have many classes).\n\n# Create mappings programmatically from dataset\nid2label = {idx: label for idx, label in enumerate(dataset[\"train\"].unique(\"label\")[::-1])} # reverse sort list to have \"not_food\" first\nlabel2id = {label: idx for idx, label in id2label.items()}\n\nprint(f\"Label to ID mapping: {label2id}\")\nprint(f\"ID to Label mapping: {id2label}\")\n\nLabel to ID mapping: {'not_food': 0, 'food': 1}\nID to Label mapping: {0: 'not_food', 1: 'food'}\n\n\nWith our dictionary mappings created, we can update the labels of our dataset to be numeric.\nWe can do this using the datasets.Dataset.map method and passing it a function to apply to each example.\nLet‚Äôs create a small function which turns an example label into a number.\n\n# Turn labels into 0 or 1 (e.g. 0 for \"not_food\", 1 for \"food\")\ndef map_labels_to_number(example):\n  example[\"label\"] = label2id[example[\"label\"]]\n  return example\n\nexample_sample = {\"text\": \"This is a sentence about my favourite food: honey.\", \"label\": \"food\"}\n\n# Test the function\nmap_labels_to_number(example_sample)\n\n{'text': 'This is a sentence about my favourite food: honey.', 'label': 1}\n\n\nLooks like our function works!\nHow about we map it to the whole dataset?\n\n# Map our dataset labels to numbers\ndataset = dataset[\"train\"].map(map_labels_to_number)\ndataset[:5]\n\n{'text': ['Creamy cauliflower curry with garlic naan, featuring tender cauliflower in a rich sauce with cream and spices, served with garlic naan bread.',\n  'Set of books stacked on a desk',\n  'Watching TV together, a family has their dog stretched out on the floor',\n  'Wooden dresser with a mirror reflecting the room',\n  'Lawn mower stored in a shed'],\n 'label': [1, 0, 0, 0, 0]}\n\n\nNice! Looks like our labels are all numerical now.\nWe can check a few random samples using dataset.shuffle() and indexing for the first few.\n\n# Shuffle the dataset and view the first 5 samples (will return different results each time) \ndataset.shuffle()[:5]\n\n{'text': ['Set of oven mitts hanging on a hook',\n  'Set of cookie cutters collected in a jar',\n  'Pizza with a dessert twist, featuring a sweet Nutella base and fresh strawberries on top',\n  'Set of binoculars placed on a table',\n  'Two handfuls of bananas in a fruit bowl with grapes on the side, the fruit bowl is blue'],\n 'label': [0, 0, 1, 0, 1]}\n\n\n\n\n4.2 Split the dataset into training and test sets\nRight now our dataset only has a training split.\nHowever, we‚Äôd like to create a test split so we can evaluate our model.\nIn essence, our model will learn patterns (the relationship between text captions and their labels of food/not_food) on the training data.\nAnd we will evaluate those learned patterns on the test data.\nWe can split our data using the datasets.Dataset.train_test_split method.\nWe can use the test_size parameter to define the percentage of data we‚Äôd like to use in our test set (e.g.¬†test_size=0.2 would mean 20% of the data goes to the test set).\n\n# Create train/test splits\ndataset = dataset.train_test_split(test_size=0.2, seed=42) # note: seed isn't needed, just here for reproducibility, without it you will get different splits each time you run the cell\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 200\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50\n    })\n})\n\n\nPerfect!\nOur dataset has been split into 200 training examples and 50 testing examples.\nLet‚Äôs visualize a few random examples to make sure they still look okay.\n\nrandom_idx_train = random.randint(0, len(dataset[\"train\"]))\nrandom_sample_train = dataset[\"train\"][random_idx_train]\n\nrandom_idx_test = random.randint(0, len(dataset[\"test\"]))\nrandom_sample_test = dataset[\"test\"][random_idx_test]\n\nprint(f\"[INFO] Random sample from training dataset:\")\nprint(f\"Text: {random_sample_train['text']}\\nLabel: {random_sample_train['label']} ({id2label[random_sample_train['label']]})\\n\")\nprint(f\"[INFO] Random sample from testing dataset:\")\nprint(f\"Text: {random_sample_test['text']}\\nLabel: {random_sample_test['label']} ({id2label[random_sample_test['label']]})\")\n\n[INFO] Random sample from training dataset:\nText: Set of dumbbells stacked in a gym\nLabel: 0 (not_food)\n\n[INFO] Random sample from testing dataset:\nText: Two handfuls of bananas in a fruit bowl with grapes on the side, the fruit bowl is blue\nLabel: 1 (food)\n\n\n\n\n4.3 Tokenizing text data\nLabels numericalized, dataset split, time to turn our text into numbers.\nHow?\nTokenization.\nWhat‚Äôs tokenization?\nTokenization is the process of converting a non-numerical data source into numbers.\nWhy?\nBecause machines (especially machine learning models) prefer numbers to human-style data.\nIn the case of the text \"I love pizza\" a very simple method of tokenization might be to convert each word to a number.\nFor example, {\"I\": 0, \"love\": 1, \"pizza\": 2}.\nHowever, for most modern machine learning models, the tokenization process is a bit more nuanced.\nFor example, the text \"I love pizza\" might be tokenized into something more like [101, 1045, 2293, 10733, 102].\n\n\n\n\nAlthough it may seem like you can type text directly to machine learning models, behind the scenes they are converting it to numbers first. This happens for all kinds of data being passed to machine learning models. It goes from its raw form (e.g.¬†text, image, audio) and gets turned into a numerical representation (often called tokenization) before it is processed by the model. Exactly how data gets turned into numbers will often be different depending on the model. This example shows the use of OpenAI‚Äôs GPT-3.5 & GPT-4 tokenizer.\n\n\n\n\n\n\n\n\nNote\n\n\n\nDepending on the model you use, the tokenization process could be different.\nFor example, one model might turn \"I love pizza\" into [40, 3021, 23317], where as another model might turn it into [101, 1045, 2293, 10733, 102].\nTo deal with this, Hugging Face models often pair models and tokenizers together by name.\nSuch is the case with distilbert/distilbert-base-uncased (there is a tokenizer.json file as well as a tokenizer_config.json file which contains all of the tokenizer implementation details).\nFor more examples of tokenization, you can see OpenAI‚Äôs tokenization visualizer tool as well as their open-source library tiktoken, Google also have an open-source tokenization library called sentencepiece, finally Hugging Face‚Äôs tokenizers library is also a great resource (this is what we‚Äôll be using behind the scenes).\n\n\nMany of the text-based models on Hugging Face come paired with their own tokenizer.\nFor example, the distilbert/distilbert-base-uncased model is paired with the distilbert/distilbert-base-uncased tokenizer.\nWe can load the tokenizer for a given model using the transformers.AutoTokenizer.from_pretrained method and passing it the name of the model we‚Äôd like to use.\nThe transformers.AutoTokenizer class is part of a series of Auto Classes (such as AutoConfig, AutoModel, AutoProcessor) which automatically loads the correct configuration settings for a given model ID.\nLet‚Äôs load the tokenizer for the distilbert/distilbert-base-uncased model and see how it works.\n\n\n\n\n\n\nNote\n\n\n\nWhy use the distilbert/distilbert-base-uncased model?\nThe short answer is that I‚Äôve used it before and it works well (and fast) on various text classification tasks.\nIt also performed well in the original research paper which introduced it.\nThe longer answer is that Hugging Face has many available open-source models for many different problems available at https://huggingface.co/models.\nNavigating these models can take some practice.\nAnd several models may be suited for the same task (though with various tradeoffs such as size and speed).\nHowever, overtime and with adequate experimentation, you‚Äôll start to build an intuition on which models are good for which problems.\n\n\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n                                          use_fast=True) # uses fast tokenization (backed by tokenziers library and implemented in Rust) by default, if not available will default to Python implementation\n\ntokenizer\n\nDistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n\n\nNice!\nThere‚Äôs our tokenizer!\nIt‚Äôs an instance of the transformers.DistilBertTokenizerFast class.\nYou can read more about it in the documentation.\nFor now, let‚Äôs try it out by passing it a string of text.\n\n# Test out tokenizer\ntokenizer(\"I love pizza\")\n\n{'input_ids': [101, 1045, 2293, 10733, 102], 'attention_mask': [1, 1, 1, 1, 1]}\n\n\n\n# Try adding a \"!\" at the end\ntokenizer(\"I love pizza!\")\n\n{'input_ids': [101, 1045, 2293, 10733, 999, 102], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n\n\nWoohoo!\nOur text gets turned into numbers (or tokens).\nNotice how with even a slight change in the text, the tokenizer produces different results?\nThe input_ids are our tokens.\nAnd the attention_mask (in our case, all [1, 1, 1, 1, 1, 1]) is a mask which tells the model which tokens to use or not.\nTokens with a mask value of 1 get used and tokens with a mask value of 0 get ignored.\nThere are several attributes of the tokenizer we can explore.\n\ntokenizer.vocab will return the vocabulary of the tokenizer or in other words, the unique words/word pieces the tokenizer is capable of converting into numbers.\ntokenizer.model_max_length will return the maximum length of a sequence the tokenizer can process, pass anything longer than this and the sequence will be truncated.\n\n\n# Get the length of the vocabulary \nlength_of_tokenizer_vocab = len(tokenizer.vocab)\nprint(f\"Length of tokenizer vocabulary: {length_of_tokenizer_vocab}\")\n\n# Get the maximum sequence length the tokenizer can handle\nmax_tokenizer_input_sequence_length = tokenizer.model_max_length\nprint(f\"Max tokenizer input sequence length: {max_tokenizer_input_sequence_length}\")\n\nLength of tokenizer vocabulary: 30522\nMax tokenizer input sequence length: 512\n\n\nWoah, looks like our tokenizer has a vocabulary of 30,522 different words and word pieces.\nAnd it can handle a sequence length of up to 512 (any sequence longer than this will be automatically truncated from the end).\nLet‚Äôs check out some of the vocab.\nCan I find my own name?\n\n# Does \"daniel\" occur in the vocab?\ntokenizer.vocab[\"daniel\"]\n\n3817\n\n\nOooh, looks like my name is 3817 in the tokenizer‚Äôs vocab.\nCan you find your own name? (note: there may be an error if the token doesn‚Äôt exist, we‚Äôll get to this)\nHow about ‚Äúpizza‚Äù?\n\ntokenizer.vocab[\"pizza\"]\n\n10733\n\n\nWhat if a word doesn‚Äôt exist in the vocab?\n\ntokenizer.vocab[\"akash\"]\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[26], line 1\n----&gt; 1 tokenizer.vocab[\"akash\"]\n\nKeyError: 'akash'\n\n\n\nDam, we get a KeyError.\nNot to worry, this is okay, since when calling the tokenizer on the word, it will automatically split the word into word pieces or subwords.\n\ntokenizer(\"akash\")\n\n{'input_ids': [101, 9875, 4095, 102], 'attention_mask': [1, 1, 1, 1]}\n\n\nIt works!\nWe can check what word pieces \"akash\" got broken into with tokenizer.convert_ids_to_tokens(input_ids).\n\ntokenizer.convert_ids_to_tokens(tokenizer(\"akash\").input_ids)\n\n['[CLS]', 'aka', '##sh', '[SEP]']\n\n\nAhhh, it seems \"akash\" was split into two tokens, [\"aka\", \"##sh\"].\nThe \"##\" at the start of \"##sh\" means that the sequence is part of a larger sequence.\nAnd the \"[CLS]\" and \"[SEP]\" tokens are special tokens indicating the start and end of a sequence.\nNow, since tokenizers can deal with any text, what if there was an unknown token?\nFor example, rather than \"pizza\" someone used the pizza emoji üçï?\nLet‚Äôs try!\n\n# Try to tokenize an emoji\ntokenizer.convert_ids_to_tokens(tokenizer(\"üçï\").input_ids)\n\n['[CLS]', '[UNK]', '[SEP]']\n\n\nAhh, we get the special \"[UNK]\" token.\nThis stands for ‚Äúunknown‚Äù.\nThe combination of word pieces and \"[UNK]\" special token means that our tokenizer will be able to turn almost any text into numbers for our model.\n\n\n\n\n\n\nNote\n\n\n\nKeep in mind that just because one tokenizer uses an unknown special token for a particular word or emoji (üçï) doesn‚Äôt mean another will.\n\n\nSince the tokenizer.vocab is a Python dictionary, we can get a sample of the vocabulary using tokenizer.vocab.items().\nHow about we get the first 5?\n\n# Get the first 5 items in the tokenizer vocab\nsorted(tokenizer.vocab.items())[:5]\n\n[('!', 999), ('\"', 1000), ('#', 1001), ('##!', 29612), ('##\"', 29613)]\n\n\nThere‚Äôs our '!' from before! Looks like the first five items are all related to punctuation points.\nHow about a random sample of tokens?\n\nimport random\n\nrandom.sample(sorted(tokenizer.vocab.items()), k=5)\n\n[('##vies', 25929),\n ('responsibility', 5368),\n ('##pm', 9737),\n ('persona', 16115),\n ('rhythm', 6348)]\n\n\n\n\n4.4 Making a preprocessing function to tokenize text\nRather than tokenizing our texts one by one, it‚Äôs best practice to define a preprocessing function which does it for us.\nThis process works regardless of whether you‚Äôre working with text data or other kinds of data such as images or audio.\n\n\n\n\n\n\nTurning data into numbers\n\n\n\nFor any kind of machine learning workflow, an important first step is turning your input data into numbers.\nAs machine learning models are algorithms which find patterns in numbers, before they can find patterns in your data (text, images, audio, tables) it must be numerically encoded first (e.g.¬†tokenizing text).\nTo help with this, transformers has an AutoProcessor class which can preprocess data in a specific format required for a paired model.\n\n\nTo prepare our text data, let‚Äôs create a preprocessing function to take in a dictionary which contains the key \"text\" which has the value of a target string (our data samples come in the form of dictionaries) and then returns the tokenized \"text\".\nWe‚Äôll set the following parameters in our tokenizer:\n\npadding=True - This will make all the sequences in a batch the same length by padding shorter sequences with 0‚Äôs until they equal the longest size in the batch. Why? If there are different size sequences in a batch, you can sometimes run into dimensionality issues.\ntruncation=True - This will shorten sequences longer than the model can handle to the model‚Äôs max input size (e.g.¬†if a sequence is 1000 long and the model can handle 512, it will be shortened to 512 via removing all tokens after 512).\n\nYou can see more parameters available for the tokenizer in the transformers.PreTrainedTokenizer documentation.\n\n\n\n\n\n\nNote\n\n\n\nFor more on padding and truncation (two important concepts in sequence processing), I‚Äôd recommend reading the Hugging Face documentation on Padding and Truncation.\n\n\n\ndef tokenize_text(examples):\n    \"\"\"\n    Tokenize given example text and return the tokenized text.\n    \"\"\"\n    return tokenizer(examples[\"text\"],\n                     padding=True, # pad short sequences to longest sequence in the batch\n                     truncation=True) # truncate long sequences to the maximum length the model can handle\n\nWonderful!\nNow let‚Äôs try it out on an example sample.\n\nexample_sample_2 = {\"text\": \"I love pizza\", \"label\": 1}\n\n# Test the function\ntokenize_text(example_sample_2)\n\n{'input_ids': [101, 1045, 2293, 10733, 102], 'attention_mask': [1, 1, 1, 1, 1]}\n\n\nLooking good!\nHow about we map our tokenize_text function to our whole dataset?\nWe can do so with the datasets.Dataset.map method.\nThe map method allows us to apply a given function to all examples in a dataset.\nBy setting batched=True we can apply the given function to batches of examples (many at a time) to speed up computation time.\nLet‚Äôs create a tokenized_dataset object by calling map on our dataset and passing it our tokenize_text function.\n\n# Map our tokenize_text function to the dataset\ntokenized_dataset = dataset.map(function=tokenize_text, \n                                batched=True, # set batched=True to operate across batches of examples rather than only single examples\n                                batch_size=1000) # defaults to 1000, can be increased if you have a large dataset\n\ntokenized_dataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 200\n    })\n    test: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 50\n    })\n})\n\n\nDataset tokenized!\nLet‚Äôs inspect a pair of samples.\n\n# Get two samples from the tokenized dataset\ntrain_tokenized_sample = tokenized_dataset[\"train\"][0]\ntest_tokenized_sample = tokenized_dataset[\"test\"][0]\n\nfor key in train_tokenized_sample.keys():\n    print(f\"[INFO] Key: {key}\")\n    print(f\"Train sample: {train_tokenized_sample[key]}\")\n    print(f\"Test sample: {test_tokenized_sample[key]}\")\n    print(\"\")\n\n[INFO] Key: text\nTrain sample: Set of headphones placed on a desk\nTest sample: A slice of pepperoni pizza with a layer of melted cheese\n\n[INFO] Key: label\nTrain sample: 0\nTest sample: 1\n\n[INFO] Key: input_ids\nTrain sample: [101, 2275, 1997, 2132, 19093, 2872, 2006, 1037, 4624, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nTest sample: [101, 1037, 14704, 1997, 11565, 10698, 10733, 2007, 1037, 6741, 1997, 12501, 8808, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n[INFO] Key: attention_mask\nTrain sample: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nTest sample: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n\n\nBeautiful! Our samples have been tokenized.\nNotice the zeroes on the end of the inpud_ids and attention_mask values.\nThese are padding tokens to ensure that each sample has the same length as the longest sequence in a given batch.\nWe can now use these tokenized samples later on in our model.\n\n\n4.5 Tokenization takeaways\nWe‚Äôve now seen and used tokenizers in practice.\nA few takeaways before we start to build a model:\n\nTokenizers are used to turn text (or other forms of data such as images and audio) into a numerical representation ready to be used with a machine learning model.\nMany models reuse existing tokenizers and many models have their own specific tokenizer paired with them. Hugging Face‚Äôs transformers.AutoTokenizer, transformers.AutoProcessor and transformers.AutoModel classes make it easy to pair tokenizers and models based on their name (e.g.¬†distilbert/distilbert-base-uncased).",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#setting-up-an-evaluation-metric",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#setting-up-an-evaluation-metric",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "5 Setting up an evaluation metric",
    "text": "5 Setting up an evaluation metric\nAside from training a model, one of the most important steps in machine learning is evaluating a model.\nTo do, we can use evaluation metrics.\nAn evaluation metric attempts to represent a model‚Äôs performance in a single (or series) of numbers (note, I say ‚Äúattempts‚Äù here because evaluation metrics are useful to guage performance but the real test of a machine learning model is in the real world).\nThere are many different kinds of evaluation metrics for various problems.\nBut since we‚Äôre focused on text classification, we‚Äôll use accuracy as our evaluation metric.\nA model which gets 99/100 predictions correct has an accuracy of 99%.\n\\[\n\\text{Accuracy} = \\frac{\\text{correct classifications}}{\\text{all classifications}}\n\\]\nFor some projects, you may have a minimum standard of a metric.\nFor example, when I worked on an insurance claim classification model, the clients required over 98% accuracy on the test dataset for it to be viable to use in production.\nIf needed, we can craft these evaluation metrics ourselves.\nHowever, Hugging Face has a library called evaluate which has various metrics built in ready to use.\nWe can load a metric using evaluate.load(\"METRIC_NAME\").\nLet‚Äôs load in \"accuracy\" and build a function to measure accuracy by comparing arrays of predictions and labels.\n\nimport evaluate\nimport numpy as np\nfrom typing import Tuple\n\naccuracy_metric = evaluate.load(\"accuracy\")\n\ndef compute_accuracy(predictions_and_labels: Tuple[np.array, np.array]):\n  \"\"\"\n  Computes the accuracy of a model by comparing the predictions and labels.\n  \"\"\"\n  predictions, labels = predictions_and_labels\n\n  # Get highest prediction probability of each prediction if predictions are probabilities\n  if len(predictions.shape) &gt;= 2:\n    predictions = np.argmax(predictions, axis=1)\n\n  return accuracy_metric.compute(predictions=predictions, references=labels)\n\nAccuracy function created!\nNow let‚Äôs test it out.\n\n# Create example list of predictions and labels\nexample_predictions_all_correct = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\nexample_predictions_one_wrong = np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\nexample_labels = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n# Test the function\nprint(f\"Accuracy when all predictions are correct: {compute_accuracy((example_predictions_all_correct, example_labels))}\")\nprint(f\"Accuracy when one prediction is wrong: {compute_accuracy((example_predictions_one_wrong, example_labels))}\")\n\nAccuracy when all predictions are correct: {'accuracy': 1.0}\nAccuracy when one prediction is wrong: {'accuracy': 0.9}\n\n\nExcellent, our function works just as we‚Äôd like.\nWhen all predictions are correct, it scores 1.0 (or 100% accuracy) and when 9/10 predictions are correct, it returns 0.9 (or 90% accuracy).\nWe can use this function during training and evaluation of our model.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#setting-up-a-model-for-training",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#setting-up-a-model-for-training",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "6 Setting up a model for training",
    "text": "6 Setting up a model for training\nWe‚Äôve gone through the important steps of setting data up for training (and evaluation).\nNow let‚Äôs prepare a model.\nWe‚Äôll keep going through the following steps:\n\n‚úÖ Create and preprocess data.\nDefine the model we‚Äôd like use with transformers.AutoModelForSequenceClassification (or another similar model class).\nDefine training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\nPass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\nTrain the model by calling Trainer.train().\nSave the model (to our local machine or to the Hugging Face Hub).\nEvaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.\n\nLet‚Äôs start by creating an instance of a model.\nSince we‚Äôre working on text classification, we‚Äôll do so with transformers.AutoModelForSequenceClassification (where sequence classification means a sequence of something, e.g.¬†our sequences of text).\nWe can use the from_pretrained() method to instatiate a pretrained model from the Hugging Face Hub.\n\n\n\n\n\n\nNote\n\n\n\nThe ‚Äúpretrained‚Äù in transformers.AutoModelForSequenceClassification.from_pretrained means acquiring a model which has already been trained on a certain dataset.\nThis is common practice in many machine learning projects and is known as transfer learning.\nThe idea is to take an existing model which works well on a task similar to your target task and then fine-tune it to work even better on your target task.\nIn our case, we‚Äôre going to use the pretrained DistilBERT base model (distilbert/distilbert-base-uncased) which has been trained on many thousands of books as well as a version of the English Wikipedia (millions of words).\nThis training gives it a very good baseline representation of the patterns in language.\nWe‚Äôll take this baseline representation of the patterns in language and adjust it slightly to focus specifically on predicting whether an image caption is about food or not (based on the words it contains).\nThe main two benefits of using transfer learning are:\n\nAbility to get good results with smaller amounts of data (since the main representations are learned on a larger dataset, we only have to show the model a few examples of our specific problem).\nThis process can be repeated acorss various domains and tasks. For example, you can take a computer vision model trained on millions of images and customize it to your own use case. Or an audio model trained on many different nature sounds and customize it specifically for birds.\n\n\n\n\n\nTransfer learning is the process of taking what one model has learned from a (typically large) dataset and applying them to your own custom dataset. This process can be replicated across domains such as computer vision, natural language processing and more.\n\n\nSo when starting a new machine learning project, one of the first questions you should ask is: does an existing pretrained model similar to my task exist and can I fine-tune it for my own task?\nFor an end-to-end example of transfer learning in PyTorch (another popular deep learning framework), see PyTorch Transfer Learning.\n\n\nTime to setup our model instance.\nA few things to note:\n\nWe‚Äôll use transformers.AutoModelForSequenceClassification.from_pretrained, this will create the model architecture we specify with the pretrained_model_name_or_path parameter.\nThe AutoModelForSequenceClassification class comes with a classification head on top of our mdoel (so we can customize this to the number of classes we have with the num_labels parameter).\nUsing from_pretrained will also call the transformers.PretrainedConfig class which will enable us to set id2label and label2id parameters for our fine-tuning task.\n\nLet‚Äôs refresh what our id2label and label2id objects look like.\n\n# Get id and label mappings\nprint(f\"id2label: {id2label}\")\nprint(f\"label2id: {label2id}\")\n\nid2label: {0: 'not_food', 1: 'food'}\nlabel2id: {'not_food': 0, 'food': 1}\n\n\nBeautiful, we can pass these mappings to transformers.AutoModelForSequenceClassification.from_pretrained.\n\nfrom transformers import AutoModelForSequenceClassification\n\n# Setup model for fine-tuning with classification head (top layers of network)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n    num_labels=2, # can customize this to the number of classes in your dataset\n    id2label=id2label, # mappings from class IDs to the class labels (for classification tasks)\n    label2id=label2id\n)\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nModel created!\nYou‚Äôll notice that a warning message gets displayed:\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: [‚Äòclassifier.bias‚Äô, ‚Äòclassifier.weight‚Äô, ‚Äòpre_classifier.bias‚Äô, ‚Äòpre_classifier.weight‚Äô] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\nThis is essentially saying ‚Äúhey, some of the layers in this model are newly initialized (with random patterns) and you should probably customize them to your own dataset‚Äù.\nThis happens because we used the AutoModelForSequenceClassification class.\nWhilst the majority of the layers in our model have already learned patterns from a large corpus of text, the top layers (classifier layers) have been randomly setup so we can customize them on our own.\n\n\n\n\nVarious forms of training paradigms. Typically you‚Äôll start with a model that has been pretrained on a large dataset. For example, a base model could be one that has read all of Wikipedia + 1000‚Äôs of books (like our DistilBert model) and has a good general representation of language data. This representation can then be tailored to a specific use case by customizing the outputs and adjusting the representation slightly by feeding it custom data. This process is often referred to as fine-tuning.\n\n\nLet‚Äôs try and make a prediction with our model and see what happens.\n\n# Try and make a prediction with the loaded model (this will error)\nmodel(**tokenized_dataset[\"train\"][0])\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[40], line 2\n      1 # Try and make a prediction with the loaded model (this will error)\n----&gt; 2 model(**tokenized_dataset[\"train\"][0])\n\nFile ~/miniconda3/envs/learn_hf/lib/python3.11/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1552 else:\n-&gt; 1553     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/envs/learn_hf/lib/python3.11/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)\n   1557 # If we don't have any hooks, we want to skip the rest of the logic in\n   1558 # this function, and just call forward.\n   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1560         or _global_backward_pre_hooks or _global_backward_hooks\n   1561         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1562     return forward_call(*args, **kwargs)\n   1564 try:\n   1565     result = None\n\nTypeError: DistilBertForSequenceClassification.forward() got an unexpected keyword argument 'text'\n\n\n\nOh no! We get an error.\nNot to worry, this is only because our model hasn‚Äôt been trained on our own dataset yet.\nLet‚Äôs take a look at the layers in our model.\n\n# Inspect the model \nmodel\n\nDistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)\n\n\nYou‚Äôll notice that the model comes in 3 main parts (data flows through these sequentially):\n\nembeddings - This part of the model turns the input tokens into a learned representation. So rather than just a list of integers, the values become a learned representation. This learned representation comes from the base model learning how different words and word pieces relate to eachother thanks to its training data. The size of (30522, 768) means the 30,522 words in the vocabulary are all represented by vectors of size 768 (one word gets represented by 768 numbers, these are often not human interpretable).\ntransformer - This is the main body of the model. There are several TransformerBlock layers stacked on top of each other. These layers attempt to learn a deeper representation of the data going through the model. A thorough breakdown of these layers is beyond the scope of this tutorial, however, for and in-depth guide on Transformer-based models, I‚Äôd recommend reading Transformers from scratch by Peter Bloem, going through Andrej Karpathy‚Äôs lecture on Transformers and their history or reading the original Attention is all you need paper (this is the paper that introduced the Transformer architecture).\nclassifier - This is what is going to take the representation of the data and compress it into our number of target classes (notice out_features=2, this means that we‚Äôll get two output numbers, one for each of our classes).\n\nFor more on the entire DistilBert architecture and its training setup, I‚Äôd recommend reading the DistilBert paper from the Hugging Face team.\nRather than breakdown the model itself, we‚Äôre focused on using it for a particular task (classifying text).\n\n6.1 Counting the parameters of our model\nBefore we move into training, we can get another insight into our model by counting its number of parameters.\nLet‚Äôs create a small function to count the number of trainable (these will update during training) and total parameters in our model.\n\ndef count_params(model):\n    \"\"\"\n    Count the parameters of a PyTorch model.\n    \"\"\"\n    trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_parameters = sum(p.numel() for p in model.parameters())\n\n    return {\"trainable_parameters\": trainable_parameters, \"total_parameters\": total_parameters}\n\n# Count the parameters of the model\ncount_params(model)\n\n{'trainable_parameters': 66955010, 'total_parameters': 66955010}\n\n\nNice!\nLooks like our model has a total of 66,955,010 parameters and all of them are trainable.\nA parameter is a numerical value in a model which is capable of being updated to better represent the input data.\nI like to think of them as a small opportunity to learn patterns in the data.\nIf a model has three parameters, it has three small opportunities to learn patterns in the data.\nWhereas, if a model has 60,000,000+ (60M) parameters (like our model), it has 60,000,000+ small opportunities to learn patterns in the data.\nSome models such as Large Language Models (LLMs) like Llama 3 70B have 70,000,000,000+ (70B) parameters (over 1000x our model).\nIn essence, the more parameters a model has, the more opportunities it has to learn (generally).\nMore parameters often results in more capabilities.\nHowever, more parameters also often results in a much larger model size (e.g.¬†many gigabytes versus hundreds of megabytes) as well as a much longer compute time (fewer samples per second).\nFor our use case, a binary text classification task, 60M parameters is more than enough.\n\n\n\n\n\n\nNote\n\n\n\nWhy count the parameters in a model?\nWhile it may be tempting to always go with a model that has the most parameters, there are many considerations to take into account before doing so.\n\nWhat hardware is the model going to run on?\n\nIf you need the model to run on cheap hardware, you‚Äôll likely want a smaller model.\n\nHow fast do you need the model to be?\n\nIf you need 100-1000s of predictions per second, you‚Äôll likely want a smaller model.\n\n‚ÄúI don‚Äôt mind about speed or cost, I just want quality.‚Äù\n\nGo with the biggest model you can.\nHowever, often times you can get really good results by training a small model to do a specific task using quality data than by just always using a large model.\n\n\n\n\n6.2 Create a directory for saving models\nTraining a model can take a while.\nSo we‚Äôll want a place to save our models.\nLet‚Äôs create a directory called \"learn_hf_food_not_food_text_classifier-distilbert-base-uncased\" (it‚Äôs a bit verbose and you can change this if you like but I like to be specific).\n\n# Create model output directory\nfrom pathlib import Path\n\n# Create models directory\nmodels_dir = Path(\"models\")\nmodels_dir.mkdir(exist_ok=True)\n\n# Create model save name\nmodel_save_name = \"learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n# Create model save path\nmodel_save_dir = Path(models_dir, model_save_name)\n\nmodel_save_dir\n\nPosixPath('models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased')\n\n\n\n\n6.3 Setting up training arguments with TrainingArguments\nTime to get our model ready for training!\nWe‚Äôre up to step 3 of our process:\n\n‚úÖ Create and preprocess data.\n‚úÖ Define the model we‚Äôd like use with transformers.AutoModelForSequenceClassification (or another similar model class).\nDefine training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\nPass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\nTrain the model by calling Trainer.train().\nSave the model (to our local machine or to the Hugging Face Hub).\nEvaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.\n\nThe transformers.TrainingArguments class contains a series of helpful items, including hyperparameter settings and model saving strategies to use throughout training.\nIt has many parameters, too many to explain here.\nHowever, the following table breaks down a helpful handful.\nSome of the parameters we‚Äôll set are the same as the defaults (this is on purpose as the defaults are often pretty good), some such as learning_rate are different.\n\n\n\nParameter\nExplanation\n\n\n\n\noutput_dir\nName of output directory to save the model and checkpoints to. For example, learn_hf_food_not_food_text_classifier_model.\n\n\nlearning_rate\nValue of the initial learning rate to use during training. Passed to transformers.AdamW. Initial learning rate because the learning rate can be dynamic during training. The ideal learning is experimental in nature. Defaults to 5e-5 or 0.00001 but we‚Äôll use 0.0001.\n\n\nper_device_train_batch_size\nSize of batches to place on target device during training. For example, a batch size of 32 means the model will look at 32 samples at a time. A batch size too large will result in out of memory issues (e.g.¬†your GPU can‚Äôt handle holding a large number of samples in memory at a time).\n\n\nper_device_eval_batch_size\nSize of batches to place on target device during evaluation. Can often be larger than during training because no gradients are being calculated. For example, training batch size could be 32 where as evaluation batch size may be able to be 128 (4x larger). Though these are only esitmates.\n\n\nnum_train_epochs\nNumber of times to pass over the data to try and learn patterns. For example, if num_train_epochs=10, the model will do 10 full passes of the training data. Because we‚Äôre working with a small dataset, 10 epochs should be fine to begin with. However, if you had a larger dataset, you may want to do a few experiments using less data (e.g.¬†10% of the data) for a smaller number of epochs to make sure things work.\n\n\neval_strategy\nWhen to evaluate the model on the evaluation data. If eval_strategy=\"epoch\", the model will be evaluated every epoch. See the documentation for more options. Note: This was previously called evaluation_strategy but was shortened in transformers==4.46.\n\n\nsave_strategy\nWhen to save a model checkpoint. If save_strategy=\"epoch\", a checkpoint will be saved every epoch. See the documentation for more save options.\n\n\nsave_total_limit\nNumber of total amount of checkpoints to save (so we don‚Äôt save num_train_epochs checkpoints). For example, can limit to 3 saves so the total number of saves are the 3 most recent as well as the best performing checkpoint (as per load_best_model_at_end).\n\n\nuse_cpu\nSet to False by default, will use CUDA GPU (torch.device(\"cuda\")) or MPS device (torch.device(\"mps\"), for Mac) if available. This is because training is generally faster on an accelerator device.\n\n\nseed\nSet to 42 by default for reproducibility. Meaning that subsequent runs with the same setup should achieve the same results.\n\n\nload_best_model_at_end\nWhen set to True, makes sure that the best model found during training is loaded when training finishes. This will mean the best model checkpoint gets saved regardless of what epoch it happened on. This is set to False by default.\n\n\nlogging_strategy\nWhen to log the training results and metrics. For example, if logging_strategy=\"epoch\", results will be logged as outputs every epoch. See the documentation for more logging options.\n\n\nreport_to\nLog experiments to various experiment tracking services. For example, you can log to Weights & Biases using report_to=\"wandb\". We‚Äôll turn this off for now and keep logging to a local directory by setting report_to=\"none\".\n\n\npush_to_hub\nAutomatically upload the model to the Hugging Face Hub every time the model is saved. We‚Äôll set push_to_hub=False as we‚Äôll see how to do this manually later on. See the documentation for more options on saving models to the Hugging Face Hub.\n\n\nhub_token\nAdd your Hugging Face Hub token to push a model to the Hugging Face Hub with push_to_hub (will default to huggingface-cli login details).\n\n\nhub_private_repo\nWhether or not to make the Hugging Face Hub repository private or public, defaults to False (e.g.¬†set to True if you want the repository to be private).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo get more familiar with the transformers.TrainingArguments class, I‚Äôd highly recommend reading the documentation for 15-20 minutes. Perhaps over a couple of sessions. There are quite a large number of parameters which will be helpful to be aware of.\n\n\nPhew!\nThat was a lot to take in.\nBut let‚Äôs now practice setting up our own instance of transformers.TrainingArguments.\n\nfrom transformers import TrainingArguments\n\nprint(f\"[INFO] Saving model checkpoints to: {model_save_dir}\")\n\n# Create training arguments\ntraining_args = TrainingArguments(\n    output_dir=model_save_dir,\n    learning_rate=0.0001,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=10,\n    eval_strategy=\"epoch\", # was previously \"evaluation_strategy\"\n    save_strategy=\"epoch\",\n    save_total_limit=3, # limit the total amount of save checkpoints (so we don't save num_epochs checkpoints)\n    use_cpu=False, # set to False by default, will use CUDA GPU or MPS device if available\n    seed=42, # set to 42 by default for reproducibility\n    load_best_model_at_end=True, # load the best model when finished training\n    logging_strategy=\"epoch\", # log training results every epoch\n    report_to=\"none\", # optional: log experiments to Weights & Biases/other similar experimenting tracking services (we'll turn this off for now) \n    # push_to_hub=True # optional: automatically upload the model to the Hub (we'll do this manually later on)\n    # hub_token=\"your_token_here\" # optional: add your Hugging Face Hub token to push to the Hub (will default to huggingface-cli login)\n    hub_private_repo=False # optional: make the uploaded model private (defaults to False)\n)\n\n# Optional: Print out training_args to inspect (warning, it is quite a long output)\n# training_args\n\n[INFO] Saving model checkpoints to: models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\n\n\nTraining arguments created!\nLet‚Äôs put them to work in an instance of transformers.Trainer.\n\n\n6.4 Setting up an instance of Trainer\nTime for step 4!\n\n‚úÖ Create and preprocess data.\n‚úÖ Define the model we‚Äôd like use with transformers.AutoModelForSequenceClassification (or another similar model class).\n‚úÖ Define training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\nPass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\nTrain the model by calling Trainer.train().\nSave the model (to our local machine or to the Hugging Face Hub).\nEvaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.\n\nThe transformers.Trainer class allows you to train models.\nIt‚Äôs built on PyTorch so it gets to leverage all of the powerful PyTorch toolkit.\nBut since it also works closely with the transformers.TrainingArguments class, it offers many helpful features.\n\n\n\n\n\n\nNote\n\n\n\ntransformers.Trainer can work with torch.nn.Module models, however, it is designed to work best with transformers.PreTrainedModel‚Äôs from the transformers library.\nThis is not a problem for us as we‚Äôre using transformers.AutoModelForSequenceClassification.from_pretrained which loads a transformers.PreTrainedModel.\nSee the transformers.Trainer documentation for tips on how to make sure your model is compatible.\n\n\n\n\n\n\n\n\n\nParameter\nExplanation\n\n\n\n\nmodel\nThe model we‚Äôd like to train. Works best with an instance of transformers.PreTrainedModel. Most models loaded using from_pretrained will be of this type.\n\n\nargs\nInstance of transformers.TrainingArguments. We‚Äôll use the training_args object we defined earlier. But if this is not set, it will default to the default settings for transformers.TrainingArguments.\n\n\ntrain_dataset\nDataset to use during training. We can use our tokenized_dataset[\"train\"] as it has already been preprocessed.\n\n\neval_dataset\nDataset to use during evaluation (our model will not see this data during training). We can use our tokenized_dataset[\"test\"] as it has already been preprocessed.\n\n\ntokenizer\nThe tokenizer which was used to preprocess the data. Passing a tokenizer will also pad the inputs to maximum length when batching them. It will also be saved with the model so future re-runs are easier.\n\n\ncompute_metrics\nAn evaluation function to evaluate a model during training and evaluation steps. In our case, we‚Äôll use the compute_accuracy function we defined earlier.\n\n\n\nWith all this being said, let‚Äôs build our Trainer!\n\nfrom transformers import Trainer\n\n# Setup Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    # Note: the 'tokenizer' parameter will be changed to 'processing_class' in Transformers v5.0.0\n    tokenizer=tokenizer, # Pass tokenizer to the Trainer for dynamic padding (padding as the training happens) (see \"data_collator\" in the Trainer docs)\n    compute_metrics=compute_accuracy\n)\n\nWoohoo! We‚Äôve created our own trainer.\nWe‚Äôre one step closer to training!\n\n\n6.5 Training our text classification model\nWe‚Äôve done most of the hard word setting up our transformers.TrainingArguments as well as our transformers.Trainer.\nNow how about we train a model?\nFollowing our steps:\n\n‚úÖ Create and preprocess data.\n‚úÖ Define the model we‚Äôd like use with transformers.AutoModelForSequenceClassification (or another similar model class).\n‚úÖ Define training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\n‚úÖ Pass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\nTrain the model by calling Trainer.train().\nSave the model (to our local machine or to the Hugging Face Hub).\nEvaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.\n\nLooks like all we have to do is call transformers.Trainer.train().\nWe‚Äôll be sure to save the results of the training to a variable results so we can inspect them later.\nLet‚Äôs try!\n\n# Train a text classification model\nresults = trainer.train()\n\n\n    \n      \n      \n      [70/70 00:07, Epoch 10/10]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\n0.328100\n0.039627\n1.000000\n\n\n2\n0.019200\n0.005586\n1.000000\n\n\n3\n0.003700\n0.002026\n1.000000\n\n\n4\n0.001700\n0.001186\n1.000000\n\n\n5\n0.001100\n0.000858\n1.000000\n\n\n6\n0.000800\n0.000704\n1.000000\n\n\n7\n0.000800\n0.000619\n1.000000\n\n\n8\n0.000700\n0.000571\n1.000000\n\n\n9\n0.000600\n0.000547\n1.000000\n\n\n10\n0.000600\n0.000539\n1.000000\n\n\n\n\n\n\nWoahhhh!!!\nHow cool is that!\nWe just trained a text classification model!\nAnd it looks like the training went pretty quick (thanks to our smaller dataset and relatively small model, for larger datasets, training would likely take longer).\nHow about we check some of the metrics?\nWe can do so using the results.metrics attribute (this returns a Python dictionary with stats from our training run).\n\n# Inspect training metrics\nfor key, value in results.metrics.items():\n    print(f\"{key}: {value}\")\n\ntrain_runtime: 7.5421\ntrain_samples_per_second: 265.177\ntrain_steps_per_second: 9.281\ntotal_flos: 18110777160000.0\ntrain_loss: 0.03574410408868321\nepoch: 10.0\n\n\nNice!\nLooks like our overall training runtime is low because of our small dataset.\nAnd looks like our trainer was able to process a fair few samples per second.\nIf we were to 1000x the size of our dataset (e.g.¬†~250 samples -&gt; ~250,000 samples which is quite a substantial dataset), it seems our training time still wouldn‚Äôt take too long.\nThe total_flos stands for ‚Äúfloating point operations‚Äù (also referred to as FLOPS), this is the total number of calculations our model has performed to find patterns in the data. And as you can see, it‚Äôs quite a large number!\n\n\n\n\n\n\nNote\n\n\n\nDepending on the hardware you‚Äôre using, the results with respect to train_runtime, train_samples_per_second and train_steps_per_second will likely be different.\nThe faster your accelerator hardware (e.g.¬†NVIDIA GPU or Mac GPU), the lower your runtime and higher your samples/steps per second will be.\nFor reference, on my local NVIDIA RTX 4090, I get a train_runtime of 8-9 seconds, train_samples_per_second of 230-250 and train_steps_per_second of 8.565.\n\n\n\n\n6.6 Save the model for later use\nNow our model has been trained, let‚Äôs save it for later use.\nWe‚Äôll save it locally first and push it to the Hugging Face Hub later.\nWe can save our model using the transformers.Trainer.save_model method.\n\n# Save model\nprint(f\"[INFO] Saving model to {model_save_dir}\")\ntrainer.save_model(output_dir=model_save_dir)\n\n[INFO] Saving model to models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\n\n\nModel saved locally! Before we save it to the Hugging Face Hub, let‚Äôs check out its metrics.\n\n\n6.7 Inspecting the model training metrics\nWe can get a log of our model‚Äôs training state using trainer.state.log_history.\nThis will give us a collection of metrics per epoch (as long as we set logging_strategy=\"epoch\" in transformers.TrainingArguments), in particular, it will give us a loss value per epoch.\nWe can extract these values and inspect them visually for a better understanding our model training.\nLet‚Äôs get the training history and inspect it.\n\n# Get training history \ntrainer_history_all = trainer.state.log_history \ntrainer_history_metrics = trainer_history_all[:-1] # get everything except the training time metrics (we've seen these already)\ntrainer_history_training_time = trainer_history_all[-1] # this is the same value as results.metrics from above\n\n# View the first 4 metrics from the training history\ntrainer_history_metrics[:4]\n\n[{'loss': 0.3281,\n  'grad_norm': 0.6938912272453308,\n  'learning_rate': 9e-05,\n  'epoch': 1.0,\n  'step': 7},\n {'eval_loss': 0.03962664306163788,\n  'eval_accuracy': 1.0,\n  'eval_runtime': 0.0135,\n  'eval_samples_per_second': 3707.312,\n  'eval_steps_per_second': 148.292,\n  'epoch': 1.0,\n  'step': 7},\n {'loss': 0.0192,\n  'grad_norm': 0.14873287081718445,\n  'learning_rate': 8e-05,\n  'epoch': 2.0,\n  'step': 14},\n {'eval_loss': 0.005585948005318642,\n  'eval_accuracy': 1.0,\n  'eval_runtime': 0.0147,\n  'eval_samples_per_second': 3399.06,\n  'eval_steps_per_second': 135.962,\n  'epoch': 2.0,\n  'step': 14}]\n\n\nOkay, looks like the metrics are logged every epochs in a list Python dictionaries with interleaving loss (this is the training set loss) and eval_loss values.\nHow about we write some code to separate the training set metrics and the evaluation set metrics?\n\nimport pprint # import pretty print for nice printing of lists\n\n# Extract training and evaluation metrics\ntrainer_history_training_set = []\ntrainer_history_eval_set = []\n\n# Loop through metrics and filter for training and eval metrics\nfor item in trainer_history_metrics:\n    item_keys = list(item.keys())\n    # Check to see if \"eval\" is in the keys of the item\n    if any(\"eval\" in item for item in item_keys):\n        trainer_history_eval_set.append(item)\n    else:\n        trainer_history_training_set.append(item)\n\n# Show the first two items in each metric set\nprint(f\"[INFO] First two items in training set:\")\npprint.pprint(trainer_history_training_set[:2])\n\nprint(f\"\\n[INFO] First two items in evaluation set:\")\npprint.pprint(trainer_history_eval_set[:2])\n\n[INFO] First two items in training set:\n[{'epoch': 1.0,\n  'grad_norm': 0.6938912272453308,\n  'learning_rate': 9e-05,\n  'loss': 0.3281,\n  'step': 7},\n {'epoch': 2.0,\n  'grad_norm': 0.14873287081718445,\n  'learning_rate': 8e-05,\n  'loss': 0.0192,\n  'step': 14}]\n\n[INFO] First two items in evaluation set:\n[{'epoch': 1.0,\n  'eval_accuracy': 1.0,\n  'eval_loss': 0.03962664306163788,\n  'eval_runtime': 0.0135,\n  'eval_samples_per_second': 3707.312,\n  'eval_steps_per_second': 148.292,\n  'step': 7},\n {'epoch': 2.0,\n  'eval_accuracy': 1.0,\n  'eval_loss': 0.005585948005318642,\n  'eval_runtime': 0.0147,\n  'eval_samples_per_second': 3399.06,\n  'eval_steps_per_second': 135.962,\n  'step': 14}]\n\n\nBeautiful!\nHow about we take it a step further and turn our metrics into pandas DataFrames so we can view them easier?\n\n# Create pandas DataFrames for the training and evaluation metrics\ntrainer_history_training_df = pd.DataFrame(trainer_history_training_set)\ntrainer_history_eval_df = pd.DataFrame(trainer_history_eval_set)\n\ntrainer_history_training_df.head() \n\n\n\n\n\n\n\n\nloss\ngrad_norm\nlearning_rate\nepoch\nstep\n\n\n\n\n0\n0.3281\n0.693891\n0.00009\n1.0\n7\n\n\n1\n0.0192\n0.148733\n0.00008\n2.0\n14\n\n\n2\n0.0037\n0.037808\n0.00007\n3.0\n21\n\n\n3\n0.0017\n0.022227\n0.00006\n4.0\n28\n\n\n4\n0.0011\n0.018665\n0.00005\n5.0\n35\n\n\n\n\n\n\n\nNice!\nAnd the evaluation DataFrame?\n\ntrainer_history_eval_df.head()\n\n\n\n\n\n\n\n\neval_loss\neval_accuracy\neval_runtime\neval_samples_per_second\neval_steps_per_second\nepoch\nstep\n\n\n\n\n0\n0.039627\n1.0\n0.0135\n3707.312\n148.292\n1.0\n7\n\n\n1\n0.005586\n1.0\n0.0147\n3399.060\n135.962\n2.0\n14\n\n\n2\n0.002026\n1.0\n0.0136\n3680.635\n147.225\n3.0\n21\n\n\n3\n0.001186\n1.0\n0.0151\n3303.902\n132.156\n4.0\n28\n\n\n4\n0.000858\n1.0\n0.0159\n3146.137\n125.845\n5.0\n35\n\n\n\n\n\n\n\nAnd of course, we‚Äôll have follow the data explorer‚Äôs motto of visualize, visualize, visualize! and inspect our loss curves.\n\n# Plot training and evaluation loss\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.plot(trainer_history_training_df[\"epoch\"], trainer_history_training_df[\"loss\"], label=\"Training loss\")\nplt.plot(trainer_history_eval_df[\"epoch\"], trainer_history_eval_df[\"eval_loss\"], label=\"Evaluation loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Text classification with DistilBert training and evaluation loss over time\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nB-e-a-utiful!\nThat is exactly what we wanted.\nTraining and evaluation loss going down over time.\n\n\n6.8 Pushing our model to the Hugging Face Hub\nWe‚Äôve saved our model locally and confirmed that it seems to be performing well on our training metrics but how about we push it to the Hugging Face Hub?\nThe Hugging Face Hub is one of the best sources of machine learning models on the internet.\nAnd we can add our model there so others can use it or we can access it in the future (we could also keep it private on the Hugging Face Hub so only people from our organization can use it).\nSharing models on Hugging Face is also a great way to showcase your skills as a machine learning engineer, it gives you something to show potential employers and say ‚Äúhere‚Äôs what I‚Äôve done‚Äù.\n\n\n\n\n\n\nNote\n\n\n\nBefore sharing a model to the Hugging Face Hub, be sure to go through the following steps:\n\nSetup a Hugging Face token using the huggingface-cli login command.\nRead through the user access tokens guide.\nSet up an access token via https://huggingface.co/settings/tokens (ensure it has ‚Äúwrite‚Äù access).\n\nIf you are using Google Colab, you can add your token under the ‚ÄúSecrets‚Äù tab on the left.\nOn my local computer, my token is saved to /home/daniel/.cache/huggingface/token (thanks to running huggingface-cli login on the command line).\nAnd for more on sharing models to the Hugging Face Hub, be sure to check out the model sharing documentation.\n\n\nWe can push our model, tokenizer and other assosciated files to the Hugging Face Hub using the transformers.Trainer.push_to_hub method.\nWe can also optionally do the following:\n\nAdd a model card (something that describes how the model was created and what it can be used for) using transformers.Trainer.create_model_card.\nAdd a custom README.md file to the model repository to explain more details about the model using huggingface_hub.HfApi.upload_file. This method is similar to model card creation method above but with more customization.\n\nLet‚Äôs save our model to the Hub!\n\n# Save our model to the Hugging Face Hub\n# This will be public, since we set hub_private_repo=False in our TrainingArguments\nmodel_upload_url = trainer.push_to_hub(\n    commit_message=\"Uploading food not food text classifier model\",\n    # token=\"YOUR_HF_TOKEN_HERE\" # This will default to the token you have saved in your Hugging Face config\n)\nprint(f\"[INFO] Model successfully uploaded to Hugging Face Hub with at URL: {model_upload_url}\")\n\n\n\n\n\n\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased/commit/8a8a8aff5bdee5bc518e31558447dc684d448b8f', commit_message='Uploading food not food text classifier model', commit_description='', oid='8a8a8aff5bdee5bc518e31558447dc684d448b8f', pr_url=None, pr_revision=None, pr_num=None)\n\n\nModel pushed to the Hugging Face Hub!\n\n\n\n\n\n\nNote\n\n\n\nYou may see the following error:\n\n403 Forbidden: You don‚Äôt have the rights to create a model under the namespace ‚Äúmrdbourke‚Äù. Cannot access content at: https://huggingface.co/api/repos/create. If you are trying to create or update content, make sure you have a token with the write role.\n\nOr even:\n\nHfHubHTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-6699c52XXXXXX)\nInvalid username or password.\n\nIn this case, be sure to go through the setup steps above to make sure you have a Hugging Face access token with ‚Äúwrite‚Äù access.\n\n\nAnd since it‚Äôs public (by default), you can see it at https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased (it gets saved to the same name as our target local directory).\nYou can now share and interact with this model online.\nAs well as download it for use in your own applications.\n\n\n\n\nThe Hugging Face Hub allows us to store and share models, datasets and demos. We can set these to be private or public. Models stored on the Hub can easily be accessed via Hugging Face Transformers.\n\n\nBut before we make an application/demo with our trained model, let‚Äôs keep evaluating it.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#making-and-evaluating-predictions-on-the-test-data",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#making-and-evaluating-predictions-on-the-test-data",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "7 Making and evaluating predictions on the test data",
    "text": "7 Making and evaluating predictions on the test data\nModel trained, let‚Äôs now evaluate it on the test data.\nOr step 7 in our workflow:\n\n‚úÖ Create and preprocess data.\n‚úÖ Define the model we‚Äôd like use with transformers.AutoModelForSequenceClassification (or another similar model class).\n‚úÖ Define training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\n‚úÖ Pass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\n‚úÖ Train the model by calling Trainer.train().\n‚úÖ Save the model (to our local machine or to the Hugging Face Hub).\nEvaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.\n\nA reminder that the test data is data that our model has never seen before.\nSo it will be a good estimate of how our model will do in a production setting.\nWe can make predictions on the test dataset using transformers.Trainer.predict.\nAnd then we can get the prediction values with the predictions attribute and assosciated metrics with the metrics attribute.\n\n# Perform predictions on the test set\npredictions_all = trainer.predict(tokenized_dataset[\"test\"])\nprediction_values = predictions_all.predictions\nprediction_metrics = predictions_all.metrics\n\nprint(f\"[INFO] Prediction metrics on the test data:\")\nprediction_metrics\n\n\n\n\n[INFO] Prediction metrics on the test data:\n\n\n{'test_loss': 0.0005385442636907101,\n 'test_accuracy': 1.0,\n 'test_runtime': 0.0421,\n 'test_samples_per_second': 1186.857,\n 'test_steps_per_second': 47.474}\n\n\nWoah!\nLooks like our model did an outstanding job!\nAnd it was very quick too.\nThis is one of the benefits of using a smaller pretrained model and customizing it to your own dataset.\nYou can achieve outstanding results in a very quick time as well as have a model capable of performing thousands of predictions per second.\nWe can also calculate the accuracy by hand by comparing the prediction labels to the test labels.\nTo do so, we‚Äôll:\n\nCalculate the prediction probabilities (though this is optional as we could skip straight to 2 and get the same results) by passing the prediction_values to torch.softmax.\nFind the index of the prediction value with the highest value (the index will be equivalent to the predicted label) using torch.argmax (we could also use np.argmax here) to find the predicted labels.\nGet the true labels from the test dataset using dataset[\"test\"][\"label\"].\nCompare the predicted labels from 2 to the true labels from 3 using sklearn.metrics.accuracy_score to find the accuracy.\n\n\nimport torch\nfrom sklearn.metrics import accuracy_score\n\n# 1. Get prediction probabilities (this is optional, could get the same results with step 2 onwards)\npred_probs = torch.softmax(torch.tensor(prediction_values), dim=1)\n\n# 2. Get the predicted labels\npred_labels = torch.argmax(pred_probs, dim=1)\n\n# 3. Get the true labels\ntrue_labels = dataset[\"test\"][\"label\"]\n\n# 4. Compare predicted labels to true labels to get the test accuracy\ntest_accuracy = accuracy_score(y_true=true_labels, \n                               y_pred=pred_labels)\n\nprint(f\"[INFO] Test accuracy: {test_accuracy*100}%\")\n\n[INFO] Test accuracy: 100.0%\n\n\nWoah!\nLooks like our model performs really well on our test set.\nIt will be interesting to see how it goes on real world samples.\nWe‚Äôll test this later on.\nHow about we make a pandas DataFrame out of our test samples, predicted labels and predicted probabilities to further inspect our results?\n\n# Make a DataFrame of test predictions\ntest_predictions_df = pd.DataFrame({\n    \"text\": dataset[\"test\"][\"text\"],\n    \"true_label\": true_labels,\n    \"pred_label\": pred_labels,\n    \"pred_prob\": torch.max(pred_probs, dim=1).values\n})\n\ntest_predictions_df.head()\n\n\n\n\n\n\n\n\ntext\ntrue_label\npred_label\npred_prob\n\n\n\n\n0\nA slice of pepperoni pizza with a layer of mel...\n1\n1\n0.999369\n\n\n1\nRed brick fireplace with a mantel serving as a...\n0\n0\n0.999662\n\n\n2\nA bowl of sliced bell peppers with a sprinkle ...\n1\n1\n0.999365\n\n\n3\nSet of mugs hanging on a hook\n0\n0\n0.999682\n\n\n4\nStanding floor lamp providing light next to an...\n0\n0\n0.999678\n\n\n\n\n\n\n\nWe can find the examples with the lowest prediction probability to see where the model is unsure.\n\n# Show 10 examples with low prediction probability\ntest_predictions_df.sort_values(\"pred_prob\", ascending=True).head(10)\n\n\n\n\n\n\n\n\ntext\ntrue_label\npred_label\npred_prob\n\n\n\n\n40\nA bowl of cherries with a sprig of mint for ga...\n1\n1\n0.999331\n\n\n11\nA close-up shot of a cheesy pizza slice being ...\n1\n1\n0.999348\n\n\n26\nA fruit platter with a variety of exotic fruit...\n1\n1\n0.999351\n\n\n42\nBoxes of apples, pears, pineapple, manadrins a...\n1\n1\n0.999353\n\n\n46\nA bowl of sliced kiwi with a sprinkle of sugar...\n1\n1\n0.999360\n\n\n37\nClose-up of a sushi roll with avocado, cucumbe...\n1\n1\n0.999360\n\n\n31\nCrunchy sushi roll with tempura flakes or pank...\n1\n1\n0.999360\n\n\n9\nCherry tomatoes and mozzarella balls in a bowl...\n1\n1\n0.999360\n\n\n14\nTwo handfuls of bananas in a fruit bowl with g...\n1\n1\n0.999360\n\n\n44\nSeasonal sushi roll with ingredients like pers...\n1\n1\n0.999361\n\n\n\n\n\n\n\nHmmm, it looks like our model has quite a high prediction probability for almost all samples.\nWe can further evalaute our model by making predictions on new custom data.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#making-and-inspecting-predictions-on-custom-text-data",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#making-and-inspecting-predictions-on-custom-text-data",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "8 Making and inspecting predictions on custom text data",
    "text": "8 Making and inspecting predictions on custom text data\nWe‚Äôve seen how our model performs on the test dataset (quite well).\nBut how might we check its performance on our own custom data?\nFor example, text-based image captions from the wild.\nWell, we‚Äôve got two ways to load our model now too:\n\nLoad model locally from our computer (e.g.¬†via models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased).\nLoad model from Hugging Face Hub (e.g.¬†via mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased).\n\nEither way of loading the model results in the same outcome: being able to make predictions on given data.\nSo how about we start by setting up our model paths for both local loading and loading from the Hugging Face Hub.\n\n# Setup local model path\nlocal_model_path = \"models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n# Setup Hugging Face model path (see: https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased)\n# Note: Be sure to change \"mrdbourke\" to your own Hugging Face username\nhuggingface_model_path = \"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n\n8.1 Discussing ways to make predictions (inference)\nWhen we‚Äôve loaded our trained model, because of the way we‚Äôve set it up, there are two main ways to make predictions on custom data:\n\nPipeline mode using transformers.pipeline and passing it our target model, this allows us to preprocess custom data and make predictions in one step.\nPyTorch mode using a combination of transformers.AutoTokenizer and transformers.AutoModelForSequenceClassification and passing each our target model, this requires us to preprocess our data before passing to a model, however, it offers the most customization.\n\nEach method supports:\n\nPredictions one at a time (batch size of 1), for example, one person using the app at a time.\nBatches of predictions at a time (predictions with a batch size of n where n can be any number, e.g.¬†8, 16, 32), for example, many people using a service simultaneously such as a voice chat and needing to filter comments (predicting on batches of size n is usually much faster than batches of 1).\n\nWhichever method we choose, we‚Äôll have to set the target device we‚Äôd like the operations to happen on.\nIn general, it‚Äôs best to make predictions on the most powerful accelerator you have available.\nAnd in most cases that will be a NVIDIA GPU &gt; Mac GPU &gt; CPU.\nSo let‚Äôs write a small function to pick the target device for us in that order.\n\n\n\n\n\n\nNote\n\n\n\nMaking predictions is also referred to as inference.\nBecause the model is going to infer on some data what the output should be.\nInference is often faster than training on a per sample basis as no model weights are updated (less computation).\nHowever, inference can use more compute than training over the long run because you could train a model once over a few hours (or days or longer) and then use it for inference for several months (or longer), millions of times (or more).\n\n\n\ndef set_device():\n    \"\"\"\n    Set device to CUDA if available, else MPS (Mac), else CPU.\n\n    This defaults to using the best available device (usually).\n    \"\"\"\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n        device = torch.device(\"mps\")\n    else:\n        device = torch.device(\"cpu\")\n    return device\n\nDEVICE = set_device()\nprint(f\"[INFO] Using device: {DEVICE}\")\n\n[INFO] Using device: cuda\n\n\nTarget device set!\nLet‚Äôs start predicting.\n\n\n8.2 Making predictions with pipeline\nThe transformers.pipeline method creates a machine learning pipeline.\nData goes in one end and predictions come out the other end.\nYou can create pipelines for many different tasks, such as, text classification, image classification, object detection, text generation and more.\nLet‚Äôs see how we can create a pipeline for our text classification model.\nTo do so we‚Äôll:\n\nInstantiate an instance of transformers.pipeline.\nPass in the task parameter of text-classification (we can do this because our model is already formatted for text classification thanks to using transformers.AutoModelForSequenceClassification).\nSetup the model parameter to be local_model_path (though we could also use huggingface_model_path).\nSet the target device using the device parameter.\nSet top_k=1 to get to the top prediction back (e.g.¬†either \"food\" or \"not_food\", could set this higher to get more labels back).\nSet the BATCH_SIZE=32 so we can pass to the batch_size parameter. This will allow our model to make predictions on up to 32 samples at a time. Predicting on batches of data is usually much faster than single samples at a time, however, this often saturates at a point (e.g.¬†predicting on batches of size 64 may be the same speed as 32 due to memory contraints).\n\n\n\n\n\n\n\nNote\n\n\n\nThere are many more pipelines available in the Hugging Face documentation.\nAs an exericse, I‚Äôd spend 10-15 minutes reading through the pipeline documentation to get familiar with what‚Äôs available.\n\n\nLet‚Äôs setup our pipeline!\n\nimport torch\nfrom transformers import pipeline\n\n# Set the batch size for predictions\nBATCH_SIZE = 32\n\n# Create an instance of transformers.pipeline\nfood_not_food_classifier = pipeline(task=\"text-classification\", # we can use this because our model is an instance of AutoModelForSequenceClassification\n                                    model=local_model_path, # could also pass in huggingface_model_path\n                                    device=DEVICE, # set the target device\n                                    top_k=1, # only return the top predicted value\n                                    batch_size=BATCH_SIZE) # perform predictions on up to BATCH_SIZE number of samples at a time \n\nfood_not_food_classifier\n\n&lt;transformers.pipelines.text_classification.TextClassificationPipeline at 0x7f2695245950&gt;\n\n\nWe‚Äôve created an instance of transformers.pipelines.text_classification.TextClassificationPipeline!\nNow let‚Äôs test it out by passing it a string of text about food.\n\n# Test our trained model on some example text \nsample_text_food = \"A delicious photo of a plate of scrambled eggs, bacon and toast\"\nfood_not_food_classifier(sample_text_food)\n\n[[{'label': 'food', 'score': 0.9993335604667664}]]\n\n\nNice! Our model gets it right.\nHow about a string not about food?\n\n# Test the model on some more example text\nsample_text_not_food = \"A yellow tractor driving over the hill\"\nfood_not_food_classifier(sample_text_not_food)\n\n[[{'label': 'not_food', 'score': 0.9996254444122314}]]\n\n\nWoohoo!\nCorrect again!\nWhat if we passed in random text?\nAs in, someone types in something random to the model expecting an output.\n\n# Pass in random text to the model\nfood_not_food_classifier(\"cvnhertiejhwgdjshdfgh394587\")\n\n[[{'label': 'not_food', 'score': 0.9985743761062622}]]\n\n\nThe nature of machine learning models is that they are a predictive/generative function.\nIf you input data, they will output something.\n\n\n\n\n\n\nNote\n\n\n\nWhen deploying machine learning models, there are many things to take into consideration.\nOne of the main ones being: ‚Äúwhat data is going to go into the model?‚Äù\nIf this was a public facing model and people could enter any kind of text, they could enter random text rather than a sentence about food or not food.\nSince our main goal of the model is be able to classify image captions into food/not_food, we‚Äôd also have to consider image cpations that are poorly written or contain little text.\nThis is why it‚Äôs important to continually test your models with as much example test/real-world data as you can.\n\n\nOur pipeline can also work with the model we saved to the Hugging Face Hub.\nLet‚Äôs try out the same pipeline with model=hugggingface_model_path.\n\n# Pipeline also works with remote models (will have to laod the model locally first)\nfood_not_food_classifier_remote = pipeline(task=\"text-classification\", \n                                           model=huggingface_model_path, # load the model from Hugging Face Hub (will download the model if it doesn't already exist)\n                                           batch_size=BATCH_SIZE,\n                                           device=DEVICE)\n\nfood_not_food_classifier_remote(\"This is some new text about bananas and pancakes and ice cream\")\n\n\n\n\n\n\n\n[{'label': 'food', 'score': 0.9993208646774292}]\n\n\nBeautiful!\nOur model loaded from Hugging Face gets it right too!\n\n\n8.3 Making multiple predictions at the same time with batch prediction\nWe can make predictions with our model one at a time but it‚Äôs often much faster to do them in batches.\nTo make predictions in batches, we can set up our transformers.pipeline instance with the batch_size parameter greater than 1.\nThen we‚Äôll be able to pass multiple samples at once in the form of a Python list.\n\n# Create batch size (we don't need to do this again but we're doing it for clarity)\nBATCH_SIZE = 32 # this number is experimental and will require testing on your hardware to find the optimal value (e.g. lower if there are memory issues or higher to try speed up inference)\n\n# Setup pipeline to handle batches (we don't need to do this again either but we're doing it for clarity)\nfood_not_food_classifier = pipeline(task=\"text-classification\", \n                                    model=local_model_path,\n                                    batch_size=BATCH_SIZE,\n                                    device=DEVICE)\n\nWonderful, now we‚Äôve set up a pipeline instance capable of handling batches, we can pass it a list of samples and it will make predictions on each.\nHow about we try with a collection of sentences which are a bit tricky?\n\n# Create a list of sentences to make predictions on\nsentences = [\n    \"I whipped up a fresh batch of code, but it seems to have a syntax error.\",\n    \"We need to marinate these ideas overnight before presenting them to the client.\",\n    \"The new software is definitely a spicy upgrade, taking some time to get used to.\",\n    \"Her social media post was the perfect recipe for a viral sensation.\",\n    \"He served up a rebuttal full of facts, leaving his opponent speechless.\",\n    \"The team needs to simmer down a bit before tackling the next challenge.\",\n    \"The presentation was a delicious blend of humor and information, keeping the audience engaged.\",\n    \"A beautiful array of fake wax foods (shokuhin sampuru) in the front of a Japanese restaurant.\",\n    \"Daniel Bourke is really cool :D\",\n    \"My favoruite food is biltong!\"\n]\n\nfood_not_food_classifier(sentences)\n\n[{'label': 'not_food', 'score': 0.9986234903335571},\n {'label': 'not_food', 'score': 0.9993952512741089},\n {'label': 'not_food', 'score': 0.9992876648902893},\n {'label': 'not_food', 'score': 0.9994683861732483},\n {'label': 'not_food', 'score': 0.9993450045585632},\n {'label': 'not_food', 'score': 0.9994571805000305},\n {'label': 'not_food', 'score': 0.9991866946220398},\n {'label': 'food', 'score': 0.9993101358413696},\n {'label': 'not_food', 'score': 0.9995250701904297},\n {'label': 'food', 'score': 0.9966572523117065}]\n\n\nWoah! That was quick!\nAnd it looks like our model performed fairly well.\nThough there was one harder sample which may be deemed as food/not_food, the sentence containing ‚Äúshokuhin sampuru‚Äù (meaning ‚Äúfood model‚Äù in Japanese).\nIs a sentence about food models (fake foods) still about food?\n\n\n8.4 Time our model across larger sample sizes\nWe can say that our model is fast or that making predictions in batches is faster than one at a time.\nBut how about we run some tests to confirm this?\nLet‚Äôs start by making predictions one at a time across 100 sentences (10x our sentences list) and then we‚Äôll write some code to make predictions in batches.\nWe‚Äôll time each and see how they go.\n\nimport time\n\n# Create 1000 sentences\nsentences_1000 = sentences * 100\n\n# Time how long it takes to make predictions on all sentences (one at a time)\nprint(f\"[INFO] Number of sentences: {len(sentences_1000)}\")\nstart_time_one_at_a_time = time.time()\nfor sentence in sentences_1000:\n    # Make a prediction on each sentence one at a time\n    food_not_food_classifier(sentence)\nend_time_one_at_a_time = time.time()\n\nprint(f\"[INFO] Time taken for one at a time prediction: {end_time_one_at_a_time - start_time_one_at_a_time} seconds\")\nprint(f\"[INFO] Avg inference time per sentence: {(end_time_one_at_a_time - start_time_one_at_a_time) / len(sentences_1000)} seconds\")\n\n[INFO] Number of sentences: 1000\n[INFO] Time taken for one at a time prediction: 2.5376925468444824 seconds\n[INFO] Avg inference time per sentence: 0.0025376925468444823 seconds\n\n\nOk, on my local NVIDIA RTX 4090 GPU, it took around 5.5 seconds to make 1000 predictions one at a time.\nThat‚Äôs pretty good!\nBut let‚Äôs see if we can make it faster with batching.\nTo do so, we can increase the size of our sentences_big list and pass the list directly to the model to enable batched prediction.\n\nfor i in [10, 100, 1000, 10_000]:\n    sentences_big = sentences * i\n    print(f\"[INFO] Number of sentences: {len(sentences_big)}\")\n\n    start_time = time.time()\n    # Predict on all sentences in batches \n    food_not_food_classifier(sentences_big)\n    end_time = time.time()\n\n    print(f\"[INFO] Inference time for {len(sentences_big)} sentences: {round(end_time - start_time, 5)} seconds.\")\n    print(f\"[INFO] Avg inference time per sentence: {round((end_time - start_time) / len(sentences_big), 8)} seconds.\")\n    print()\n\n[INFO] Number of sentences: 100\n[INFO] Inference time for 100 sentences: 0.04512 seconds.\n[INFO] Avg inference time per sentence: 0.0004512 seconds.\n\n[INFO] Number of sentences: 1000\n[INFO] Inference time for 1000 sentences: 0.31447 seconds.\n[INFO] Avg inference time per sentence: 0.00031447 seconds.\n\n[INFO] Number of sentences: 10000\n[INFO] Inference time for 10000 sentences: 1.82615 seconds.\n[INFO] Avg inference time per sentence: 0.00018261 seconds.\n\n[INFO] Number of sentences: 100000\n[INFO] Inference time for 100000 sentences: 18.63373 seconds.\n[INFO] Avg inference time per sentence: 0.00018634 seconds.\n\n\n\nWoah!\nIt looks like inference/prediction time is ~10-20x faster when using batched prediction versus predicting one at a time (on my local NVIDIA RTX 4090).\nI ran some more tests with the same model on a different GPU on Google Colab (NVIDIA L4 GPU) and got similar results.\n\n\n\nNumber of Sentences\nTotal Prediction Time\nPrediction Type\n\n\n\n\n100\n0.62\none at a time\n\n\n1000\n6.19\none at a time\n\n\n10000\n61.08\none at a time\n\n\n100000\n605.46\none at a time\n\n\n100\n0.06\nbatch\n\n\n1000\n0.51\nbatch\n\n\n10000\n4.97\nbatch\n\n\n100000\n49.7\nbatch\n\n\n\nTesting the speed of a custom text classifier model on different numbers of sentences with one at a time or batched prediction. Tests conducted on Google Colab with a NVIDIA L4 GPU. See the notebook for code to reproduce.\n\n\n8.5 Making predictions with PyTorch\nWe‚Äôve seen how to make predictions/perform inference with transformers.pipeline, now let‚Äôs see how to do the same with PyTorch.\nPerforming predictions with PyTorch requires an extra step compared to pipeline, we have to prepare our inputs first (turn the text into numbers).\nGood news is, we can prepare our inputs with the tokenizer that got automatically saved with our model.\nAnd since we‚Äôve already trained a model and uploaded it to the Hugging Face Hub, we can load our model and tokenizer with transformers.AutoTokenizer and transformers.AutoModelForSequenceClassification passing it the saved path we used (mine is mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased).\nLet‚Äôs start by loading the tokenizer and see what it looks like to tokenize a piece of sample text.\n\nfrom transformers import AutoTokenizer\n\n# Setup model path (can be local or on Hugging Face)\n# Note: Be sure to change \"mrdbourke\" to your own username\nmodel_path = \"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n# Create an example to predict on\nsample_text_food = \"A delicious photo of a plate of scrambled eggs, bacon and toast\"\n\n# Prepare the tokenizer and tokenize the inputs\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_path)\ninputs = tokenizer(sample_text_food, \n                   return_tensors=\"pt\") # return the output as PyTorch tensors \ninputs\n\n{'input_ids': tensor([[  101,  1037, 12090,  6302,  1997,  1037,  5127,  1997, 13501,  6763,\n          1010, 11611,  1998, 15174,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n\n\nNice!\nText tokenized!\nWe get a dictionary of input_ids (our text in token form) and attention_mask (tells the model which tokens to pay attention to, 1 = pay attention, 0 = no attention).\nNow we can load the model with the same path.\n\nfrom transformers import AutoModelForSequenceClassification\n\n# Load our text classification model\nmodel = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_path)\n\nModel loaded!\nLet‚Äôs make a prediction.\nWe can do so using the context manager torch.no_grad() (because no gradients/weights get updated during inference) and passing our model out inputs dictionary.\n\n\n\n\n\n\nNote\n\n\n\nA little tidbit about using dictionaries as function inputs in Python is the ability to unpack the keys of the dictionary into function arguments.\nThis is possible using **TARGET_DICTIONARY syntax. Where the ** means ‚Äúuse all the keys in the dictionary as function parameters‚Äù.\nFor example, the following two lines are equivalent:\n# Using ** notation\noutputs = model(**inputs)\n\n# Using explicit notation\noutputs = model(input_ids=inputs[\"input_ids\"],\n                attention_mask=inputs[\"attention_mask\"])\n\n\nLet‚Äôs make a prediction with PyTorch!\n\nimport torch\n\nwith torch.no_grad():\n    outputs = model(**inputs) # '**' means input all of the dictionary keys as arguments to the function\n    # outputs = model(input_ids=inputs[\"input_ids\"],\n    #                 attention_mask=inputs[\"attention_mask\"]) # same as above, but explicitly passing in the keys\n\noutputs\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-3.3686,  3.9443]]), hidden_states=None, attentions=None)\n\n\nBeautiful, we‚Äôve got some outputs, which contain logits with two values (one for each class).\nThe index of the higher value is our model‚Äôs predicted class.\nWe can find it by taking the outputs.logits and calling argmax().item() on it.\nWe can also find the prediction probability by passing outputs.logits to torch.softmax.\n\n# Get predicted class and prediction probability\npredicted_class_id = outputs.logits.argmax().item()\nprediction_probability = torch.softmax(outputs.logits, dim=1).max().item()\n\nprint(f\"Text: {sample_text_food}\")\nprint(f\"Predicted label: {model.config.id2label[predicted_class_id]}\")\nprint(f\"Prediction probability: {prediction_probability}\")\n\nText: A delicious photo of a plate of scrambled eggs, bacon and toast\nPredicted label: food\nPrediction probability: 0.9993335604667664\n\n\nBeautiful! A prediction made with pure PyTorch! It looks very much correct too.\nHow about we put it all together?\n\nimport torch\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_path = \"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n# Load the model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_path)\n\n# Make sample text and tokenize it\nsample_text = \"A photo of a broccoli, salmon, rice and radish dish\"\ninputs = tokenizer(sample_text, return_tensors=\"pt\")\n\n# Make a prediction\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# Get predicted class and prediction probability\noutput_logits = outputs.logits\npredicted_class_id = torch.argmax(output_logits, dim=1).item()\npredicted_class_label = model.config.id2label[predicted_class_id]\npredicted_probability = torch.softmax(output_logits, dim=1).max().item()\n\n# Print outputs\nprint(f\"Text: {sample_text}\")\nprint(f\"Predicted class: {predicted_class_label} (prob: {predicted_probability * 100:.2f}%)\")\n\nText: A photo of a broccoli, salmon, rice and radish dish\nPredicted class: food (prob: 99.94%)",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#putting-it-all-together",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#putting-it-all-together",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "9 Putting it all together",
    "text": "9 Putting it all together\nOk, ok, we‚Äôve covered a lot of ground going from dataset to trained model to making predictions on custom samples.\nHow about we put all of the steps we‚Äôve covered so far together in a single code cell (or two)?\nTo do so, we‚Äôll:\n\nImport necessary packages (e.g.¬†datasets, transformers.pipeline, torch and more).\nSetup variables for model training and saving pipeline such as our model name, save directory and dataset name.\nCreate a directory for saving models.\nLoad and preprocess the dataset from Hugging Face Hub using datasets.load_dataset.\nImport a tokenizer with transformers.AutoTokenizer and map it to our dataset with dataset.map.\nSet up an evaluation metric with evaluate & create a function to evaluate our model‚Äôs predictions.\nImport a model with transformers.AutoModelForSequenceClassification and prepare it for training with transformers.TrainingArguments and transformers.Trainer.\nTrain the model on our text dataset by calling transformers.Trainer.train.\nSave the trained model to a local directory.\nPush the model to the Hugging Face Hub.\nEvaluate the model on the test data.\nTest the trained model on a custom sample using transformers.pipeline to make sure it works.\n\nPhew!\nA fair few steps but nothing we can‚Äôt handle!\nLet‚Äôs do it.\n\n# 1. Import necessary packages\nimport pprint\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\n\nimport datasets\nimport evaluate\n\nfrom transformers import pipeline\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# 2. Setup variables for model training and saving pipeline\nDATASET_NAME = \"mrdbourke/learn_hf_food_not_food_image_captions\"\nMODEL_NAME = \"distilbert/distilbert-base-uncased\"\nMODEL_SAVE_DIR_NAME = \"models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\"\n\n# 3. Create a directory for saving models\n# Note: This will override our existing saved model (if there is one)\nprint(f\"[INFO] Creating directory for saving models: {MODEL_SAVE_DIR_NAME}\")\nmodel_save_dir = Path(MODEL_SAVE_DIR_NAME)\nmodel_save_dir.mkdir(parents=True, exist_ok=True)\n\n# 4. Load and preprocess the dataset from Hugging Face Hub\nprint(f\"[INFO] Downloading dataset from Hugging Face Hub, name: {DATASET_NAME}\")\ndataset = datasets.load_dataset(path=DATASET_NAME)\n\n# Create mappings from id2label and label2id (adjust these for your target dataset, can also create these programmatically)\nid2label = {0: \"not_food\", 1: \"food\"}\nlabel2id = {\"not_food\": 0, \"food\": 1}\n\n# Create function to map IDs to labels in dataset\ndef map_labels_to_number(example):\n    example[\"label\"] = label2id[example[\"label\"]]\n    return example\n\n# Map preprocessing function to dataset\ndataset = dataset[\"train\"].map(map_labels_to_number)\n\n# Split the dataset into train/test sets\ndataset = dataset.train_test_split(test_size=0.2, seed=42)\n\n# 5. Import a tokenizer and map it to our dataset\nprint(f\"[INFO] Tokenizing text for model training with tokenizer: {MODEL_NAME}\")\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=MODEL_NAME,\n                                          use_fast=True)\n\n# Create a preprocessing function to tokenize text\ndef tokenize_text(examples):\n    return tokenizer(examples[\"text\"],\n                     padding=True,\n                     truncation=True)\n\ntokenized_dataset = dataset.map(function=tokenize_text,\n                                batched=True,\n                                batch_size=1000)\n\n# 6. Set up an evaluation metric & function to evaluate our model\naccuracy_metric = evaluate.load(\"accuracy\")\n\ndef compute_accuracy(predictions_and_labels):\n    predictions, labels = predictions_and_labels\n\n    if len(predictions.shape) &gt;= 2:\n        predictions = np.argmax(predictions, axis=1)\n    \n    return accuracy_metric.compute(predictions=predictions, references=labels) # note: use \"references\" parameter rather than \"labels\"\n\n\n# 7. Import a model and prepare it for training \nprint(f\"[INFO] Loading model: {MODEL_NAME}\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    pretrained_model_name_or_path=MODEL_NAME,\n    num_labels=2,\n    id2label=id2label,\n    label2id=label2id\n)\nprint(f\"[INFO] Model loading complete!\")\n\n# Setup TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir=model_save_dir,\n    learning_rate=0.0001,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=10,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=3,\n    use_cpu=False,\n    seed=42,\n    load_best_model_at_end=True,\n    logging_strategy=\"epoch\",\n    report_to=\"none\",\n    push_to_hub=False,\n    hub_private_repo=False # Note: if set to False, your model will be publically available\n)\n\n# Create Trainer instance and train model\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_accuracy\n)\n\n# 8. Train the model on our text dataset\nprint(f\"[INFO] Commencing model training...\")\nresults = trainer.train()\n\n# 9. Save the trained model (note: this will overwrite our previous model, this is ok)\nprint(f\"[INFO] Model training complete, saving model to local path: {model_save_dir}\")\ntrainer.save_model(output_dir=model_save_dir)\n\n# 10. Push the model to the Hugging Face Hub\nprint(f\"[INFO] Uploading model to Hugging Face Hub...\")\nmodel_upload_url = trainer.push_to_hub(\n    commit_message=\"Uploading food not food text classifier model\",\n    # token=\"YOUR_HF_TOKEN_HERE\" # requires a \"write\" HF token \n)\nprint(f\"[INFO] Model upload complete, model available at: {model_upload_url}\")\n\n# 11. Evaluate the model on the test data\nprint(f\"[INFO] Performing evaluation on test dataset...\")\npredictions_all = trainer.predict(tokenized_dataset[\"test\"])\nprediction_values = predictions_all.predictions\nprediction_metrics = predictions_all.metrics\n\nprint(f\"[INFO] Prediction metrics on the test data:\")\npprint.pprint(prediction_metrics)\n\n[INFO] Creating directory for saving models: models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\n[INFO] Downloading dataset from Hugging Face Hub, name: mrdbourke/learn_hf_food_not_food_image_captions\n[INFO] Tokenizing text for model training with tokenizer: distilbert/distilbert-base-uncased\n[INFO] Loading model: distilbert/distilbert-base-uncased\n\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n[INFO] Model loading complete!\n[INFO] Commencing model training...\n\n\n\n    \n      \n      \n      [70/70 00:07, Epoch 10/10]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\n0.372500\n0.067892\n1.000000\n\n\n2\n0.028300\n0.009194\n1.000000\n\n\n3\n0.004700\n0.004919\n1.000000\n\n\n4\n0.002000\n0.002121\n1.000000\n\n\n5\n0.001200\n0.001302\n1.000000\n\n\n6\n0.000900\n0.000982\n1.000000\n\n\n7\n0.000800\n0.000839\n1.000000\n\n\n8\n0.000700\n0.000766\n1.000000\n\n\n9\n0.000700\n0.000728\n1.000000\n\n\n10\n0.000700\n0.000715\n1.000000\n\n\n\n\n\n\n[INFO] Model training complete, saving model to local path: models/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\n[INFO] Uploading model to Hugging Face Hub...\n\n\n\n\n\n\n\n\n\n\n\n[INFO] Model upload complete, model available at: https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased/tree/main/\n[INFO] Performing evaluation on test dataset...\n\n\n\n\n\n[INFO] Prediction metrics on the test data:\n{'test_accuracy': 1.0,\n 'test_loss': 0.0007152689504437149,\n 'test_runtime': 0.0507,\n 'test_samples_per_second': 986.278,\n 'test_steps_per_second': 39.451}\n\n\nWoohoo! It all worked!\nNow let‚Äôs make it sure works by turing it into a transformers.pipeline and passing it a custom sample.\n\n# 12. Make sure the model works by testing it on a custom sample\nfood_not_food_classifier = pipeline(task=\"text-classification\",\n                                    model=model_save_dir, # can also use model on Hugging Face Hub path \n                                    device=torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\",\n                                    top_k=1,\n                                    batch_size=32)\n\nfood_not_food_classifier(\"Yo! We just built a food not food sentence classifier model! Good news is, it can be replicated for other kinds of text classification!\")\n\n[[{'label': 'food', 'score': 0.9969706535339355}]]\n\n\nNice!\nLooks like putting all of our code in one cell worked.\nHow about we make our model even more accessible by turning it into a demo?",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#turning-our-model-into-a-demo",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#turning-our-model-into-a-demo",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "10 Turning our model into a demo",
    "text": "10 Turning our model into a demo\nOnce you‚Äôve trained and saved a model, one of the best ways to continue to test it and show/share it with others is to create a demo.\nOr step number 8 in our workflow:\n\n‚úÖ Create and preprocess data.\n‚úÖ Define the model we‚Äôd like use with transformers.AutoModelForSequenceClassification (or another similar model class).\n‚úÖ Define training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\n‚úÖ Pass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\n‚úÖ Train the model by calling Trainer.train().\n‚úÖ Save the model (to our local machine or to the Hugging Face Hub).\n‚úÖ Evaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.\n\nA demo is a small application with the focus of showing the workflow of your model from data in to data out.\nIt‚Äôs also one way to start testing your model in the wild.\nYou may know where it works and where it doesn‚Äôt but chances are someone out there will find a new bug before you do.\nTo build our demo, we‚Äôre going to use an open-source library called Gradio.\nGradio allows you to make machine learning demo apps with Python code and best of all, it‚Äôs part of the Hugging Face ecosystem so you can share your demo to the public directly through Hugging Face.\n\n\n\n\nGoing on the premise of data, model, demo, Gradio helps to create the demo. Once you‚Äôve got a trained model on the Hugging Face Hub, you can setup a Gradio interface to import that model and interact it with it. Gradio interfaces can be deployed on Hugging Face Spaces and shared with others so they can try your model too.\n\n\nGradio works on the premise of input -&gt; function (this could be a model) -&gt; output.\nIn our case:\n\nInput = A string of text.\nFunction = Our trained text classification model.\nOutput = Predicted output of food/not_food with prediction probability.\n\n\n10.1 Creating a simple function to perform inference\nLet‚Äôs create a function to take an input of text, process it with our model and return a dictionary of the predicted labels.\nOur function will:\n\nTake an input of a string of text.\nSetup a text classification pipeline using transformers.pipeline as well as our trained model (this can be from our local machine or loaded from Hugging Face). We‚Äôll return all the probabilities from the output using top_k=None.\nGet the outputs of the text classification pipeline from 2 as a list of dictionaries (e.g.¬†[{'label': 'food', 'score': 0.999105}, {'label': 'not_food', 'score': 0.00089}]).\nFormat and return the list of dictionaries from 3 to be compatible with Gradio‚Äôs gr.Label output (we‚Äôll see this later) which requires a dictionary in the form [{\"label_1\": probability_1, \"label_2\": probability_2}].\n\nOnward!\n\nfrom typing import Dict\n\n# 1. Create a function which takes text as input \ndef food_not_food_classifier(text: str) -&gt; Dict[str, float]:\n    \"\"\"\n    Takes an input string of text and classifies it into food/not_food in the form of a dictionary.\n    \"\"\"\n\n    # 2. Setup the pipeline to use the local model (or Hugging Face model path)\n    food_not_food_classifier = pipeline(task=\"text-classification\", \n                                        model=local_model_path,\n                                        batch_size=32,\n                                        device=\"cuda\" if torch.cuda.is_available() else \"cpu\", # set the device to work in any environment\n                                        top_k=None) # return all possible scores (not just top-1)\n    \n    # 3. Get outputs from pipeline (as a list of dicts)\n    outputs = food_not_food_classifier(text)[0]\n    \n    # 4. Format output for Gradio (e.g. {\"label_1\": probability_1, \"label_2\": probability_2})\n    output_dict = {}\n    for item in outputs:\n        output_dict[item[\"label\"]] = item[\"score\"]\n\n    return output_dict\n\n# Test out the function\nfood_not_food_classifier(\"My lunch today was chicken and salad\")\n\n{'food': 0.9992194175720215, 'not_food': 0.0007805348141118884}\n\n\nBeautiful!\nLooks like our function is working.\n\n\n10.2 Building a small Gradio demo to run locally\nWe‚Äôve got a working function to go from text to predicted labels and probabilities.\nLet‚Äôs now build a Gradio interface to showcase our model.\nWe can do so by:\n\nImporting Gradio (using import gradio as gr).\nCreating an instance of gr.Interface with parameters inputs=\"text\" (for our text-based inputs) called demo and outputs=gr.Label(num_top_classes=2) to display our output dictionary. We can also add some descriptive aspects to our demo with the title, description and examples parameters.\nRunning/launching the demo with gr.Interface.launch().\n\n\n# 1. Import Gradio as the common alias \"gr\"\nimport gradio as gr\n\n# 2. Setup a Gradio interface to accept text and output labels\ndemo = gr.Interface(\n    fn=food_not_food_classifier, \n    inputs=\"text\", \n    outputs=gr.Label(num_top_classes=2), # show top 2 classes (that's all we have)\n    title=\"Food or Not Food Classifier\",\n    description=\"A text classifier to determine if a sentence is about food or not food.\",\n    examples=[[\"I whipped up a fresh batch of code, but it seems to have a syntax error.\"],\n              [\"A delicious photo of a plate of scrambled eggs, bacon and toast.\"]])\n\n# 3. Launch the interface\ndemo.launch()\n\nRunning on local URL:  http://127.0.0.1:7860\n\nTo create a public link, set `share=True` in `launch()`.\n\n\n\n\n\n\n\n\nWoohoo!\nWe‚Äôve made a very clean way of interacting with our model.\nHowever, our model is still only largely accessible to us (except for the model file we‚Äôve uploaded to Hugging Face).\nHow about we make our demo publicly available so it‚Äôs even easier for people to interact with our model?\n\n\n\n\n\n\nNote\n\n\n\nThe gradio.Interface class is full of many different options, I‚Äôd highly recommend reading through the documentation for 10-15 minutes to get an idea of it.\nIf your workflow requires inputs -&gt; function (e.g.¬†a model making predictions on the input) -&gt; output, chances are, you can build it with Gradio.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#making-our-demo-publicly-accessible",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#making-our-demo-publicly-accessible",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "11 Making our demo publicly accessible",
    "text": "11 Making our demo publicly accessible\nOne of the best ways to share your machine learning work is by creating an application.\nAnd one of the best places to share your applications is Hugging Face Spaces.\nHugging Face Spaces allows you to host machine learning (and non-machine learning) applications for free (with optional paid hardware upgrades).\nIf you‚Äôre familiar with GitHub, Hugging Face Spaces works similar to a GitHub repository (each Space is a Git repository itself).\nIf not, that‚Äôs okay, think of Hugging Face Spaces as an online folder where you can upload your files and have them accessed by others.\nCreating a Hugging Face Space can be done in two main ways:\n\nManually - By going to the Hugging Face Spaces website and clicking ‚ÄúCreate new space‚Äù. Or by going directly to https://www.huggingface.co/new-space. Here, you‚Äôll be able to setup a few settings for your Space and choose the framework/runtime (e.g.¬†Streamlit, Gradio, Docker and more).\nProgrammatically - By using the Hugging Face Hub Python API we can write code to directly upload files to the Hugging Face Hub, including Hugging Face Spaces.\n\nBoth are great options but we‚Äôre going to take the second approach.\nThis is so we can create our Hugging Face Space right from this notebook.\nTo do so, we‚Äôll create three files:\n\napp.py - This will be the Python file which will be the main running file on our Hugging Face Space. Inside we‚Äôll include all the code necessary to run our Gradio demo (as above). Hugging Face Spaces will automatically recoginize the app.py file and run it for us.\nrequirements.txt - This text file will include all of the Python packages we need to run our app.py file. Before our Space starts to run, all of the packages in this file will be installed.\nREADME.md - This markdown file will include details about our Space as well as specific Space-related metadata (we‚Äôll see this later on).\n\nWe‚Äôll create these files with the following file structure:\ndemos/\n‚îî‚îÄ‚îÄ food_not_food_text_classifier/\n    ‚îú‚îÄ‚îÄ app.py\n    ‚îú‚îÄ‚îÄ README.md\n    ‚îî‚îÄ‚îÄ requirements.txt\nWhy this way?\nDoing it in the above style means we‚Äôll have a directory which contains all of our demos (demos/) as well as a dedicated directory which contains our food/not_food demo application (food_not_food_text_classifier/).\nThis way, we‚Äôll be able to upload the whole demos/food_not_food_text_classifier/ folder to Hugging Face Spaces.\nLet‚Äôs start by making a directory to store our demo application files.\n\nfrom pathlib import Path\n\n# Make a directory for demos\ndemos_dir = Path(\"../demos\")\ndemos_dir.mkdir(exist_ok=True)\n\n# Create a folder for the food_not_food_text_classifer demo\nfood_not_food_text_classifier_demo_dir = Path(demos_dir, \"food_not_food_text_classifier\")\nfood_not_food_text_classifier_demo_dir.mkdir(exist_ok=True)\n\nDemo directory created, let‚Äôs now create our requried files.\n\n11.1 Making an app file\nOur app.py file will be the main part of our Hugging Face Space.\nThe good news is, we‚Äôve already created most of it when we created our original demo.\nInside the app.py folder we‚Äôll:\n\nImport the required libraries/packages for running our demo app.\nSetup a function for going from text to our trained model‚Äôs predicted outputs. And because our model is already hosted on the Hugging Face Hub, we can pass pipeline our model‚Äôs name (e.g.¬†mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased) and when we upload our app.py file to Hugging Face Spaces, it will load the model directly from the Hub.\n\nNote: Be sure to change ‚Äúmrdbourke‚Äù to your own Hugging Face username.\n\nCreate a demo just as before with gr.Interface.\nLaunch our demo with gr.Interface.launch.\n\nWe can write all of the above in a notebook cell.\nAnd we can turn it into a file by using the %%writefile magic command and passing it our target filepath.\nLet‚Äôs do it!\n\n%%writefile ../demos/food_not_food_text_classifier/app.py\n# 1. Import the required packages\nimport torch\nimport gradio as gr\n\nfrom typing import Dict\nfrom transformers import pipeline\n\n# 2. Define function to use our model on given text \ndef food_not_food_classifier(text: str) -&gt; Dict[str, float]:\n    # Set up text classification pipeline\n    food_not_food_classifier = pipeline(task=\"text-classification\", \n                                        # Because our model is on Hugging Face already, we can pass in the model name directly\n                                        model=\"mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased\", # link to model on HF Hub\n                                        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n                                        top_k=None) # return all possible scores (not just top-1)\n    \n    # Get outputs from pipeline (as a list of dicts)\n    outputs = food_not_food_classifier(text)[0]\n\n    # Format output for Gradio (e.g. {\"label_1\": probability_1, \"label_2\": probability_2})\n    output_dict = {}\n    for item in outputs:\n        output_dict[item[\"label\"]] = item[\"score\"]\n\n    return output_dict\n\n# 3. Create a Gradio interface with details about our app\ndescription = \"\"\"\nA text classifier to determine if a sentence is about food or not food. \n\nFine-tuned from [DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased) on a [small dataset of food and not food text](https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions).\n\nSee [source code](https://github.com/mrdbourke/learn-huggingface/blob/main/notebooks/hugging_face_text_classification_tutorial.ipynb).\n\"\"\"\n\ndemo = gr.Interface(fn=food_not_food_classifier, \n             inputs=\"text\", \n             outputs=gr.Label(num_top_classes=2), # show top 2 classes (that's all we have)\n             title=\"üçóüö´ü•ë Food or Not Food Text Classifier\",\n             description=description,\n             examples=[[\"I whipped up a fresh batch of code, but it seems to have a syntax error.\"],\n                       [\"A delicious photo of a plate of scrambled eggs, bacon and toast.\"]])\n\n# 4. Launch the interface\nif __name__ == \"__main__\":\n    demo.launch()\n\nOverwriting ../demos/food_not_food_text_classifier/app.py\n\n\napp.py file created!\nNow let‚Äôs setup the requirements file.\n\n\n11.2 Making a requirements file\nWhen you upload an app.py file to Hugging Face Spaces, it will attempt to run it automatically.\nAnd just like running the file locally, we need to make sure all of the required packages are available.\nOtherwise our Space will produce an error like the following:\n===== Application Startup at 2024-06-13 05:37:21 =====\n\nTraceback (most recent call last):\n  File \"/home/user/app/app.py\", line 1, in &lt;module&gt;\n    import torch\nModuleNotFoundError: No module named 'torch'\nGood news is, our demo only has three requirements: gradio, torch, transformers.\nLet‚Äôs create a requirements.txt file with the packages we need and save it to the same directory as our app.py file.\n\n%%writefile ../demos/food_not_food_text_classifier/requirements.txt\ngradio\ntorch\ntransformers\n\nOverwriting ../demos/food_not_food_text_classifier/requirements.txt\n\n\nBeautiful!\nHugging Face Spaces will automatically recognize the requirements.txt file and install the listed packages into our Space.\n\n\n11.3 Making a README file\nOur app.py can contain information about our demo, however, we can also use a README.md file to further communicate our work.\n\n\n\n\n\n\nNote\n\n\n\nIt is common practice in Git repositories (including GitHub and Hugging Face Hub) to add a README.md file to your project so people can read more (hence ‚Äúread me‚Äù) about what your project is about.\n\n\nWe can include anything in markdown-style text in the README.md file.\nHowever, Spaces also have a special YAML block at the top of the README.md file in the root directory with configuration details.\nInside the YAML block you can put special metadata details about your Space including:\n\ntitle - The title of your Space (e.g.¬†title: Food Not Food Text Classifier).\nemoji - The emoji to display on your Space (e.g.¬†emoji: üçóüö´ü•ë).\napp_file - The target app file for Spaces to run (set to app_file: app.py by default).\n\nAnd there are plenty more in the Spaces Configuration References documentation.\n\n\n\n\nExample of Hugging Face Spaces README.md file with YAML front matter (front matter is another term for ‚Äúthings at the front/top of the file‚Äù) for formatting the Space.\n\n\nLet‚Äôs create a README.md file with a YAML block at the top detailing some of the metadata about our project.\n\n\n\n\n\n\nNote\n\n\n\nThe YAML block at the top of the README.md can take some practice.\nIf you want to see a demo of how one gets created, try making a Hugging Face Space with the ‚ÄúCreate new Space‚Äù button on the https://huggingface.co/spaces page and seeing what the README.md file starts with (that‚Äôs how I found out what to do!).\n\n\n\n%%writefile ../demos/food_not_food_text_classifier/README.md\n---\ntitle: Food Not Food Text Classifier\nemoji: üçóüö´ü•ë\ncolorFrom: blue\ncolorTo: yellow\nsdk: gradio\napp_file: app.py\npinned: false\nlicense: apache-2.0\n---\n\n# üçóüö´ü•ë Food Not Food Text Classifier\n\nSmall demo to showcase a text classifier to determine if a sentence is about food or not food.\n\nDistillBERT model fine-tuned on a small synthetic dataset of 250 generated [Food or Not Food image captions](https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions).\n\n[Source code notebook](https://github.com/mrdbourke/learn-huggingface/blob/main/notebooks/hugging_face_text_classification_tutorial.ipynb).\n\nOverwriting ../demos/food_not_food_text_classifier/README.md\n\n\nREADME.md created!\nNow let‚Äôs check out the files we have in our demos/food_not_food_text_classifier/ folder.\n\n!ls ../demos/food_not_food_text_classifier\n\nREADME.md  app.py  requirements.txt\n\n\nPerfect!\nLooks like we‚Äôve got all the files we need to create our Space.\nLet‚Äôs upload them to the Hugging Face Hub.\n\n\n11.4 Uploading our demo to Hugging Face Spaces\nWe‚Äôve created all of the files required for our demo, now for the fun part!\nLet‚Äôs upload them to Hugging Face Spaces.\nTo do so programmatically, we can use the Hugging Face Hub Python API.\n\n\n\n\n\n\nNote\n\n\n\nThe Hugging Face Hub Python API has many different options for interacting with the Hugging Face Hub programmatically.\nYou can create repositories, upload files, upload folders, add comments, change permissions and much much more.\nBe sure to explore the documentation for at least 10-15 minutes to get an idea of what‚Äôs possible.\n\n\nTo get our demo hosted on Hugging Face Spaces we‚Äôll go through the following steps:\n\nImport the required methods from the huggingface_hub package, including create_repo, get_full_repo_name, upload_file (optional, we‚Äôll be using upload_folder) and upload_folder.\nDefine the demo folder we‚Äôd like to upload as well as the different parameters for the Hugging Face Space such as repo type (\"space\"), our target Space name, the target Space SDK (\"gradio\"), our Hugging Face token with write access (optional if it already isn‚Äôt setup).\nCreate a repository on Hugging Face Spaces using the huggingface_hub.create_repo method and filling out the appropriate parameters.\nGet the full name of our created repository using the huggingface_hub.get_full_repo_name method (we could hard code this but I like to get it programmatically incase things change).\nUpload the contents of our target demo folder (../demos/food_not_food_text_classifier/) to Hugging Face Hub with huggingface_hub.upload_folder.\nHope it all works and inspect the results! ü§û\n\nA fair few steps but we‚Äôve got this!\n\n# 1. Import the required methods for uploading to the Hugging Face Hub\nfrom huggingface_hub import (\n    create_repo,\n    get_full_repo_name,\n    upload_file, # for uploading a single file (if necessary)\n    upload_folder # for uploading multiple files (in a folder)\n)\n\n# 2. Define the parameters we'd like to use for the upload\nLOCAL_DEMO_FOLDER_PATH_TO_UPLOAD = \"../demos/food_not_food_text_classifier\"\nHF_TARGET_SPACE_NAME = \"learn_hf_food_not_food_text_classifier_demo\"\nHF_REPO_TYPE = \"space\" # we're creating a Hugging Face Space\nHF_SPACE_SDK = \"gradio\"\nHF_TOKEN = \"\" # optional: set to your Hugging Face token (but I'd advise storing this as an environment variable as previously discussed)\n\n# 3. Create a Space repository on Hugging Face Hub \nprint(f\"[INFO] Creating repo on Hugging Face Hub with name: {HF_TARGET_SPACE_NAME}\")\ncreate_repo(\n    repo_id=HF_TARGET_SPACE_NAME,\n    # token=HF_TOKEN, # optional: set token manually (though it will be automatically recognized if it's available as an environment variable)\n    repo_type=HF_REPO_TYPE,\n    private=False, # set to True if you don't want your Space to be accessible to others\n    space_sdk=HF_SPACE_SDK,\n    exist_ok=True, # set to False if you want an error to raise if the repo_id already exists \n)\n\n# 4. Get the full repository name (e.g. {username}/{model_id} or {username}/{space_name})\nfull_hf_repo_name = get_full_repo_name(model_id=HF_TARGET_SPACE_NAME)\nprint(f\"[INFO] Full Hugging Face Hub repo name: {full_hf_repo_name}\")\n\n# 5. Upload our demo folder\nprint(f\"[INFO] Uploading {LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD} to repo: {full_hf_repo_name}\")\nfolder_upload_url = upload_folder(\n    repo_id=full_hf_repo_name,\n    folder_path=LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD,\n    path_in_repo=\".\", # upload our folder to the root directory (\".\" means \"base\" or \"root\", this is the default)\n    # token=HF_TOKEN, # optional: set token manually\n    repo_type=HF_REPO_TYPE,\n    commit_message=\"Uploading food not food text classifier demo app.py\"\n)\nprint(f\"[INFO] Demo folder successfully uploaded with commit URL: {folder_upload_url}\")\n\n[INFO] Creating repo on Hugging Face Hub with name: learn_hf_food_not_food_text_classifier_demo\n[INFO] Full Hugging Face Hub repo name: mrdbourke/learn_hf_food_not_food_text_classifier_demo\n[INFO] Uploading ../demos/food_not_food_text_classifier to repo: mrdbourke/learn_hf_food_not_food_text_classifier_demo\n[INFO] Demo folder successfully uploaded with commit URL: https://huggingface.co/spaces/mrdbourke/learn_hf_food_not_food_text_classifier_demo/tree/main/.\n\n\nExcellent!\nLooks like all of the files in our target demo folder were uploaded!\nOnce this happens, Hugging Face Spaces will take a couple of minutes to build our application.\nIf there are any errors, it will let us know.\nOtherwise, our demo application should be running live and be ready to test at a URL similar to: https://huggingface.co/spaces/mrdbourke/learn_hf_food_not_food_text_classifier_demo (though you may have to swap my username ‚Äúmrdbourke‚Äù for your own as well as the name you chose for the Space).\n\n\n11.5 Testing our hosted demo\nOne of the really cool things about Hugging Face Spaces is that we can share our demo application as a link so others can try it out.\nWe can also embed it right into our notebook.\nTo do so, we can go to the three dots in the top right of our hosted Space and select ‚ÄúEmbed this Space‚Äù.\nWe then have the option to embed our Space using a JavaScript web component, HTML iframe or via the direct URL.\nSince Jupyter notebooks have the ability to render HTML via IPython.display.HTML, let‚Äôs embed our Space with HTML.\n\nfrom IPython.display import HTML\n\n\n# You can get embeddable HTML code for your demo by clicking the \"Embed\" button on the demo page\nHTML(data='''\n&lt;iframe\n    src=\"https://mrdbourke-learn-hf-food-not-food-text-classifier-demo.hf.space\"\n    frameborder=\"0\"\n    width=\"850\"\n    height=\"450\"\n&gt;&lt;/iframe&gt;     \n''')\n\n\n     \n\n\nNow that‚Äôs cool!\nWe can try out our Food Not Food Text Classifier app from right within our notebook!",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#summary",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#summary",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "12 Summary",
    "text": "12 Summary\nYou should be very proud of yourself!\nWe‚Äôve just gone end-to-end on a machine learning workflow with Hugging Face.\nFrom loading a dataset to training a model to deploying that model in the form of a public demo.\nHere are some of the main takeaways from this project.\nThe Hugging Face ecosystem is a collection of powerful and open-source tools for machine learning workflows.\n\nHugging Face datasets helps you to store and preprocess datasets of almost any shape and size.\nHugging Face transformers has many built-in pretrained models for many different use cases and components such as transformers.Trainer help you to tailor those models to your own custom use cases.\nHugging Face tokenizers works closely with transformers and allows the efficient conversion of raw text data into numerical representation (which is required for machine learning models).\nThe Hugging Face Hub is a great place to share your models and machine learning projects. Over time, you can build up a portfolio of machine learning-based projects to show future employers or clients and to help the community grow.\nThere are many more, but I‚Äôll leave these for you to explore as extra-curriculum.\n\nA common machine learning workflow: dataset -&gt; model -&gt; demo.\nBefore a machine learning model is incorporated into a larger application, a very common workflow is to:\n\nFind an existing or create a new dataset for your specific problem.\nTrain/fine-tune and evaluate an existing model on your dataset.\nCreate a small demo application to test your trained model.\n\nWe‚Äôve just gone through all of these steps for text classification!\nText classification is a very common problem in many business settings. If you have a similar problem but a different dataset, you can replicate this workflow.\nBuilding your own model has several advantages over using APIs.\nAPIs are very helpful to try something out.\nHowever, depending on your use case, you may often want to create your own custom model.\nTraining your own model can often result in faster predictions and far less running costs over time.\nThe Hugging Face ecosystem enables the creation of custom models for almost any kind of machine learning problem.",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#exercises-and-extensions",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#exercises-and-extensions",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "13 Exercises and Extensions",
    "text": "13 Exercises and Extensions\nThere‚Äôs no better way to improve other than practicing what you‚Äôve learned.\nThe following exercises and extensions are designed for you to practice the things we‚Äôve covered in this project.\n\nOur text classification model works on food/not_food text samples. How would you create your own binary text classification model on different classes?\n\nCreate ~10 or samples of your own text classes (e.g.¬†10 samples each of spam/not_spam emails) and retrain a text classification model.\nBonus: Share the model you‚Äôve made in a demo just like we did here. Send it to me, I‚Äôd love to see it! My email is on my website.\n\nWe‚Äôve trained our model on two classes (binary classification) but how might we increase that to 3 or more classes (multi-class classification)?\n\nHint: see the num_labels parameter in transformers.AutoModelForSequenceClassification.\n\nOur model seems to work pretty good on our test data and on the few number of examples we tried manually. Can you find any examples where our model fails? For example, what kind of sentences does it struggle with? How could you fix this?\n\nHint: Our model has been trained on examples with at least 5-12 words, does it still work with short sentences? (e.g.¬†‚Äúpie‚Äù).\nBonus: If you find any cases where our model doesn‚Äôt perform well, make an extra 10-20 examples of these and add them to the dataset and then retrain the model (you‚Äôll have to lookup ‚Äúhow to add rows to an existing Hugging Face dataset‚Äù). How does the model perform after adding these additional samples?\n\nDatasets are fundamental to any machine learning project, getting to know how to process and interact with them is a fundamental skill. Spend 1 hour going through the Hugging Face Datasets tutorial.\n\nWrite 5 things you can do with Hugging Face Datasets and where they might come in handy.\n\nThe Hugging Face transformers library has many features. The following readings are to help understand a handful of them.\n\nSpend 10 minutes exploring the transformers.TrainingArguments documentation.\nSpend 10 minutes reading the transformers.Trainer documentation.\n\nSpend 10 minutes reading the Hugging Face model sharing documentation.\n\nSpend 10 minutes reading the Hugging Face transformers.pipeline documentation.\n\nWhat does a pipeline do?\nName 3 different kinds of pipelines and describe what they do in a sentence\n\n\nGradio is a powerful library for making machine learning demos, learning more about it will help you in future creations. Spend 10-15 minutes reading the Gradio quickstart documentation.\n\nWhat are 3 kinds of demos you can create?\nWhat are 3 different inputs and outputs you can make?",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_text_classification_tutorial.html#extra-resources",
    "href": "notebooks/hugging_face_text_classification_tutorial.html#extra-resources",
    "title": "Text Classification with Hugging Face Transformers Tutorial",
    "section": "14 Extra resources",
    "text": "14 Extra resources\nThere are many things we touched over but didn‚Äôt go into much depth in this notebook.\nThe following resources are for those who‚Äôd like to learn a little bit more.\n\nSee how the food not food image caption dataset was created with synthetic text data (image captions generated by a Large Language Model) in the example Google Colab notebook.\nHugging Face have a great guide on sequence classification (it‚Äôs what this notebook was built on).\nFor more on the concept of padding and truncation in sequence processing, I‚Äôd recommend the Hugging Face padding and truncation guide.\nFor more on Transformers (the architecture) as well as the DistilBert model:\n\nRead Transformers from scratch by Peter Bloem.\nWatch Andrej Karpathy‚Äôs lecture on Transformers and their history.\nRead the original Attention is all you need paper (the paper that introduced the Transformer architecture).\nRead the DistilBert paper from the Hugging Face team (paper that introduced the DistilBert architecture and training setup).",
    "crumbs": [
      "Home",
      "Natural Language Processing (NLP)",
      "Build a custom text classification model and demo"
    ]
  },
  {
    "objectID": "extras/setup.html#start-here-universal-steps",
    "href": "extras/setup.html#start-here-universal-steps",
    "title": "Getting setup for the Hugging Face ecosystem",
    "section": "",
    "text": "Create a free Hugging Face account at https://huggingface.co/join.\nCreate a Hugging Face access token with read and write access at https://huggingface.co/settings/tokens.\n\nYou can create a read/write token using the fine-grained settings and selecting all the appropriate options.\nRead more on Hugging Face access tokens at https://huggingface.co/docs/hub/en/security-tokens.\n\n\n\n\n\n\nTo read from and write to your Hugging Face Hub account, you‚Äôll need to set up an access token. You can have one token for reading and one for writing. However, I personally use a single token for reading and writing.\n\n\nNote: Do not share your token with others. Always keep it private and avoid saving it in raw text format."
  },
  {
    "objectID": "extras/setup.html#getting-setup-on-google-colab",
    "href": "extras/setup.html#getting-setup-on-google-colab",
    "title": "Getting setup for the Hugging Face ecosystem",
    "section": "",
    "text": "Note: If you‚Äôre unfamiliar with Google Colab, I‚Äôd recommend going through Sam Witteveen‚Äôs video Colab 101 and then Advanced Colab to learn more.\n\nFollow the steps in Start here.\nAdd your Hugging Face read/write token as a Secret in Google Colab.\n\nNaming this Secret HF_TOKEN will mean that Hugging Face libraries automatically recognize your token for future use.\n\n\n\n\n\n\nFor accessing models and datasets from the Hugging Face Hub (both read and write) inside Google Colab, you‚Äôll need to add your Hugging Face token as a Secret in Google Colab. Once you give your Google Colab notebook access to the token, it can be used by Hugging Face libraries to interact with the Hugging Face Hub.\n\n\nAlternatively, if you need to force relogin for a notebook session, you can run:\nimport huggingface_hub # requires !pip install huggingface_hub\n\n# Login to Hugging Face\nhuggingface_hub.login()\nAnd enter your token in the box that appears (note: this token will only be active for the current notebook session and will delete when your Google Colab instance terminates)."
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "Contents",
    "text": "Contents\nAll code and text will be free/open-source, video step-by-step walkthroughs are available as a paid upgrade.\n\n\n\n\n\n\n\n\n\n\n\nProject\nDescription\nDataset\nModel\nDemo\nVideo Course\n\n\n\n\nText classification (good place to start)\nBuild project ‚ÄúFood Not Food‚Äù, a text classification model to classify image captions into ‚Äúfood‚Äù if they‚Äôre about food or ‚Äúnot_food‚Äù if they‚Äôre not about food. This is the ideal place to get started if you‚Äôve never used the Hugging Face ecosystem.\nDataset\nModel\nDemo\nVideo Course\n\n\nMore to come soon!\nLet me know if you‚Äôd like to see anything specific by leaving an issue on GitHub."
  },
  {
    "objectID": "index.html#who-is-it-for",
    "href": "index.html#who-is-it-for",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "Who is it for?",
    "text": "Who is it for?\nIdeal for:\n\nBeginners who love things explained in detail.\nSomeone who wants to create more of their own end-to-end machine learning projects.\n\nNot ideal for:\n\nPeople with 2-3+ years of machine learning projects & experience^.\n\n^Note: This being said, you may actually find some things helpful along the way. Best to explore and see!"
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "Prerequisites",
    "text": "Prerequisites\n\n3-6 months Python experience.\n1x beginner machine learning or deep learning course (see my begineer-friendly ML course to learn Python + important ML concepts in one).\n\nPyTorch experience is a bonus (see my Learn PyTorch in a Day video or learnpytorch.io)"
  },
  {
    "objectID": "index.html#todo",
    "href": "index.html#todo",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "TODO",
    "text": "TODO\n\nFinish outline of this (index.md) page\n\nCopy a similar version to the README.md for GitHub\nMake share image for the whole thing\n\nMake index of different projects\nEcosystem overview: transformers, datasets, accelerate, Spaces, Hub, models etc\nPractical tutorials\n\nText classification (this will be like a ‚Äústart here‚Äù for the Hugging Face ecosystem)\nMore to come‚Ä¶\n\nWhere to get help? HF forums, HF GitHub, etc\nFinish setup page\n\nLocal setup\n\nFinish deployment to learnhuggingface.com page\nGet started: text classification shows an end-to-end workflow with detailed steps, I‚Äôd advise starting here to get to know the ecosystem a bit\n\nOther projects are more focused on specific tasks with less explanations but still complete code examples"
  },
  {
    "objectID": "index.html#what-is-hugging-face",
    "href": "index.html#what-is-hugging-face",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "What is Hugging Face?",
    "text": "What is Hugging Face?\nHugging Face is a platform that offers access to many different kinds of open-source machine learning models and datasets.\nThey‚Äôre also the creators of the popular transformers library (and many more helpful libraries) which is a Python-based library for working with pre-trained models as well as custom models.\nIf you‚Äôre getting into the world of AI and machine learning, you‚Äôre going to come across Hugging Face.\n\n\n\n\nA handful of pieces from the Hugging Face ecosystem. There are many more available in Hugging Face documentation."
  },
  {
    "objectID": "index.html#why-hugging-face",
    "href": "index.html#why-hugging-face",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "Why Hugging Face?",
    "text": "Why Hugging Face?\nMany of the biggest companies in the world use Hugging Face for their open-source machine learning projects including Apple, Google, Facebook (Meta), Microsoft, OpenAI, ByteDance and more.\nNot only does Hugging Face make it so you can use state-of-the-art machine learning models such as Stable Diffusion (for image generation) and Whipser (for audio transcription) easily, it also makes it so you can share your own models, datasets and resources.\nAside from your own website, consider Hugging Face the homepage of your AI/machine learning profile."
  },
  {
    "objectID": "index.html#updates",
    "href": "index.html#updates",
    "title": "Learn Hugging Face ü§ó (work in progress)",
    "section": "Updates",
    "text": "Updates\n\n1 Oct 2024 - Video course version of text classification is live on ZTM! Inside, we‚Äôll walkthrough every line of code building the text classification project with Hugging Face Datasets, Transformers and Spaces."
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html",
    "href": "notebooks/hugging_face_object_detection_tutorial.html",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "",
    "text": "Details:\nIn progress:\nLater:\nSource code on GitHub | Online book version | Setup guide | Video Course (coming soon)",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#get-modules",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#get-modules",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "2 Get Modules",
    "text": "2 Get Modules\n\nTK - add getting setup note (e.g.¬†for torchmetrics + pycocotools + other dependencies)\nTK - add link to getting setup\n\n\nimport torch\nimport datasets\nimport transformers\nimport torchvision\nimport numpy as np\n\n# TK - Required for evaluation\n# Can install with !pip install torchmetrics[detection]\nimport torchmetrics\nimport pycocotools\n\nprint(transformers.__version__)\nprint(torch.__version__)\nprint(torchvision.__version__)\nprint(torchmetrics.__version__)\n\n4.47.0\n2.6.0+cu124\n0.21.0+cu124\n1.4.1",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#load-data",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#load-data",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "3 Load Data",
    "text": "3 Load Data\nGet help with a function by typing the function and then a question mark.\nE.g. load_dataset?\n\nfrom datasets import load_dataset\n\n# TK - information for loading the dataset\n# load_dataset?\ndataset = load_dataset(path=\"mrdbourke/trashify_manual_labelled_images\")\n\nprint(f\"[INFO] Length of original dataset: {len(dataset['train'])}\")\n\n# Split the data\ndataset_split = dataset[\"train\"].train_test_split(test_size=0.3, seed=42) # split the dataset into 70/30 train/test\ndataset_test_val_split = dataset_split[\"test\"].train_test_split(test_size=0.6, seed=42) # split the test set into 40/60 validation/test\n\n# Create splits\ndataset[\"train\"] = dataset_split[\"train\"]\ndataset[\"validation\"] = dataset_test_val_split[\"train\"]\ndataset[\"test\"] = dataset_test_val_split[\"test\"]\n\ndataset\n\n[INFO] Length of original dataset: 1128\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],\n        num_rows: 789\n    })\n    validation: Dataset({\n        features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],\n        num_rows: 135\n    })\n    test: Dataset({\n        features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],\n        num_rows: 204\n    })\n})\n\n\n\n# Get the categories from the dataset\n# Note: this requires the dataset to have been uploaded with this feature setup\ncategories = dataset[\"train\"].features[\"annotations\"].feature[\"category_id\"]\n\n# Get the names attribute\ncategories.names\n\n['bin', 'hand', 'not_bin', 'not_hand', 'not_trash', 'trash', 'trash_arm']\n\n\n\n# Can also get the features\ndataset[\"train\"].features\n\n{'image': Image(mode=None, decode=True, id=None),\n 'image_id': Value(dtype='int64', id=None),\n 'annotations': Sequence(feature={'file_name': Value(dtype='string', id=None), 'image_id': Value(dtype='int64', id=None), 'category_id': ClassLabel(names=['bin', 'hand', 'not_bin', 'not_hand', 'not_trash', 'trash', 'trash_arm'], id=None), 'bbox': Sequence(feature=Value(dtype='float32', id=None), length=4, id=None), 'iscrowd': Value(dtype='int64', id=None), 'area': Value(dtype='float32', id=None)}, length=-1, id=None),\n 'label_source': Value(dtype='string', id=None),\n 'image_source': Value(dtype='string', id=None)}\n\n\n\n# Boxes come in format: [x, y, w, h] -&gt; [top left corner x, top left corner y, width, height]\n# Where x, y is top left corner of the box and w=width, h=height of the box\ndataset[\"train\"][0]\n\n{'image': &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,\n 'image_id': 69,\n 'annotations': {'file_name': ['c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',\n   'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',\n   'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',\n   'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',\n   'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',\n   'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',\n   'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',\n   'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg'],\n  'image_id': [69, 69, 69, 69, 69, 69, 69, 69],\n  'category_id': [5, 0, 1, 4, 4, 4, 4, 4],\n  'bbox': [[360.20001220703125, 528.5, 177.1999969482422, 261.79998779296875],\n   [298.29998779296875,\n    495.1000061035156,\n    381.1000061035156,\n    505.70001220703125],\n   [81.5999984741211,\n    592.0999755859375,\n    358.79998779296875,\n    316.29998779296875],\n   [1.2999999523162842,\n    776.7000122070312,\n    193.8000030517578,\n    211.89999389648438],\n   [301.1000061035156, 60.79999923706055, 146.89999389648438, 115.0],\n   [501.0, 75.9000015258789, 24.200000762939453, 71.19999694824219],\n   [546.4000244140625,\n    54.70000076293945,\n    130.3000030517578,\n    115.0999984741211],\n   [862.9000244140625,\n    41.099998474121094,\n    75.69999694824219,\n    80.19999694824219]],\n  'iscrowd': [0, 0, 0, 0, 0, 0, 0, 0],\n  'area': [46390.9609375,\n   192722.265625,\n   113488.4375,\n   41066.21875,\n   16893.5,\n   1723.0400390625,\n   14997.5302734375,\n   6071.14013671875]},\n 'label_source': 'manual_prodigy_label',\n 'image_source': 'manual_taken_photo'}\n\n\n\nTK - add image of box coordinates drawn on\nTK - make blog post of different box levels\nTK - see bounding box demo tool - https://huggingface.co/spaces/mrdbourke/bounding-box-demo\nTK - see guide to different bounding box formats - https://www.learnml.io/posts/a-guide-to-bounding-box-formats/\n\n\n3.1 Create id2label mapping\n\nid2label = {i: class_name for i, class_name in enumerate(categories.names)}\nlabel2id = {value: key for key, value in id2label.items()}\n\nid2label, label2id\n\n({0: 'bin',\n  1: 'hand',\n  2: 'not_bin',\n  3: 'not_hand',\n  4: 'not_trash',\n  5: 'trash',\n  6: 'trash_arm'},\n {'bin': 0,\n  'hand': 1,\n  'not_bin': 2,\n  'not_hand': 3,\n  'not_trash': 4,\n  'trash': 5,\n  'trash_arm': 6})\n\n\n\n# Make colour dict\n# \"label_name\" -&gt; colour\ncolour_palette = {\n    'bin': (0, 0, 224),         # Bright Blue (High contrast with greenery)\n    'not_bin': (135, 206, 250), # Light Blue (Lighter for distinction)\n\n    'hand': (148, 0, 211),      # Dark Purple (Contrasts well with skin tones)\n    'not_hand': (218, 112, 214),# Lighter Purple (Soft but visible)\n\n    'trash': (0, 255, 0),       # Bright Green (Common for trash-related items)\n    'not_trash': (144, 238, 144), # Light Green (Still visible but distinct)\n\n    'trash_arm': (255, 140, 0), # Deep Orange (Highly visible)\n}\n\n\n\n3.2 Box conversion methods\nTK - add guide for different bounding boxes (e.g.¬†a table with examples)\n‚Äì\n\nTK - add reference here for different types of box formats:\n\n‚Äòxyxy‚Äô: boxes are represented via corners, x1, y1 being top left and x2, y2 being bottom right. This is the format that torchvision utilities expect.\n\nThis is format that PASCAL VOC dataset comes in: http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/devkit_doc.html#SECTION00053000000000000000 (see section 4.3)\n\n‚Äòxywh‚Äô: boxes are represented via corner, width and height, x1, y2 being top left, w, h being width and height - matplotlib expects these boxes? (not 100% on this)\n\nCOCO (Common Objects in Context) uses this format, see: https://cocodataset.org/#format-data\n\n‚Äòcxcywh‚Äô: boxes are represented via centre, width and height, cx, cy being center of box, w, h being width and height - createml expects these boxes\n\nYOLO format: normalized version of this style (see section 2 of this paper: https://arxiv.org/abs/1506.02640)\n\nSee guide on different kinds of boxes - https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n\nTK - methods for converting boxes:\n\npost_process_object_detection - https://huggingface.co/docs/transformers/v4.44.0/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.post_process_object_detection\ntorchvision.ops.box_convert\n\n\nTK - see docs for torchvision.utils.draw_bounding_boxes - https://pytorch.org/vision/main/generated/torchvision.utils.draw_bounding_boxes.html\n\n# Plot a single image\nimport random\n\nimport torch\n\nfrom torchvision.ops import box_convert\nfrom torchvision.utils import draw_bounding_boxes\n\nfrom torchvision.transforms.functional import pil_to_tensor, to_pil_image \n\nfrom PIL import ImageFont\n\nrandom_index = random.randint(0, len(dataset[\"train\"]))\nprint(f\"[INFO] Showing training sample from index: {random_index}\")\nrandom_sample = dataset[\"train\"][random_index]\n\n# Get box coordinates\nboxes_xywh = torch.tensor(random_sample[\"annotations\"][\"bbox\"])\nprint(f\"Boxes in XYWH format: {boxes_xywh}\")\n\n# Convert boxes from XYWH -&gt; XYXY \n# torchvision.utils.draw_bounding_boxes requires input boxes in XYXY format (X_min, y_min, X_max, y_max)\nboxes_xyxy = box_convert(boxes=boxes_xywh,\n                         in_fmt=\"xywh\",\n                         out_fmt=\"xyxy\")\nprint(f\"Boxes XYXY: {boxes_xyxy}\")\n\n# Get label names of target boxes and colours to match\nrandom_sample_label_names = [categories.int2str(x) for x in random_sample[\"annotations\"][\"category_id\"]]\nrandom_sample_colours = [colour_palette[label_name] for label_name in random_sample_label_names]\nprint(f\"Label names: {random_sample_label_names}\")\nprint(f\"Colour names: {random_sample_colours}\")\n\n# Draw the image as a tensor and then turn it into a PIL image\nto_pil_image(\n    pic=draw_bounding_boxes(\n        image=pil_to_tensor(pic=random_sample[\"image\"]),\n        boxes=boxes_xyxy,\n        colors=random_sample_colours,\n        labels=random_sample_label_names,\n        width=3,\n        # font=font_filename,\n        font_size=30,\n        label_colors=random_sample_colours\n    )\n)\n\n[INFO] Showing training sample from index: 358\nBoxes in XYWH format: tensor([[ 480.0000,  643.7000,  126.0000,  144.3000],\n        [ 419.3000,  743.9000,  368.6000,  461.1000],\n        [ 611.9000, 1147.1000,  260.9000,  127.8000],\n        [  81.5000,  371.1000,  666.6000,  598.4000]])\nBoxes XYXY: tensor([[ 480.0000,  643.7000,  606.0000,  788.0000],\n        [ 419.3000,  743.9000,  787.9000, 1205.0000],\n        [ 611.9000, 1147.1000,  872.8000, 1274.9000],\n        [  81.5000,  371.1000,  748.1000,  969.5000]])\nLabel names: ['trash', 'hand', 'not_trash', 'bin']\nColour names: [(0, 255, 0), (148, 0, 211), (144, 238, 144), (0, 0, 224)]\nPillow/Tests/fonts/NotoSansSymbols-Regular.ttf\n\n\n\n\n\n\n\n\n\n\nrandom_sample_label_names\n\n['trash', 'hand', 'bin', 'bin', 'not_trash']\n\n\n\n# TK - functionize the plotting of boxes and image so we can do input/output with tensors + data augmentations on that (E.g. original: image, augmented: image)",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#setup-model",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#setup-model",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "6 Setup model",
    "text": "6 Setup model\nUPTOHERE - going through different places to get models\nWant to get a model running to start with so we can test it out and see how it goes.\n\nTK - places to get object detection models\nTK - paperswithcode/object detection\nTK - torchvision pretrained models\nTK - Hugging Face Transformers\nTK - mmdetect framework\nTK - detectron2\nTK - YOLO series (various resources such as Ultralytics etc, beware of licencing issues)\n\n\nfrom transformers import AutoModelForObjectDetection, AutoImageProcessor\n\n# Model config - https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrConfig \n# Model docs - https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrModel \nMODEL_NAME = \"microsoft/conditional-detr-resnet-50\"\n\n# Set image size\nIMAGE_SIZE = 640 # other common image sizes include: 300x300, 480x480, 512x512, 640x640, 800x800 (best to experiment and see which works best)\n\n# Get the image processor (this is required for prepraring images)\n# See docs: https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.preprocess\nimage_processor = AutoImageProcessor.from_pretrained(\n    pretrained_model_name_or_path=MODEL_NAME,\n    format=\"coco_detection\", # this is the default\n    do_convert_annotations=True, # defaults to True, converts boxes to (center_x, center_y, width, height)\n    size={\"shortest_edge\": IMAGE_SIZE, \"longest_edge\": IMAGE_SIZE}\n)\n\n# Check out the image processor\nimage_processor\n\nConditionalDetrImageProcessor {\n  \"do_convert_annotations\": true,\n  \"do_normalize\": true,\n  \"do_pad\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"format\": \"coco_detection\",\n  \"image_mean\": [\n    0.485,\n    0.456,\n    0.406\n  ],\n  \"image_processor_type\": \"ConditionalDetrImageProcessor\",\n  \"image_std\": [\n    0.229,\n    0.224,\n    0.225\n  ],\n  \"pad_size\": null,\n  \"resample\": 2,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"longest_edge\": 640,\n    \"shortest_edge\": 640\n  }\n}\n\n\n\n# View the docstring of our image_processor.preprocess function\nimage_processor.preprocess?\n\nSignature:\nimage_processor.preprocess(\n    images: Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), List[ForwardRef('PIL.Image.Image')], List[numpy.ndarray], List[ForwardRef('torch.Tensor')]],\n    annotations: Union[Dict[str, Union[int, str, List[Dict]]], List[Dict[str, Union[int, str, List[Dict]]]], NoneType] = None,\n    return_segmentation_masks: bool = None,\n    masks_path: Union[str, pathlib.Path, NoneType] = None,\n    do_resize: Optional[bool] = None,\n    size: Optional[Dict[str, int]] = None,\n    resample=None,\n    do_rescale: Optional[bool] = None,\n    rescale_factor: Union[int, float, NoneType] = None,\n    do_normalize: Optional[bool] = None,\n    do_convert_annotations: Optional[bool] = None,\n    image_mean: Union[float, List[float], NoneType] = None,\n    image_std: Union[float, List[float], NoneType] = None,\n    do_pad: Optional[bool] = None,\n    format: Union[str, transformers.image_utils.AnnotationFormat, NoneType] = None,\n    return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None,\n    data_format: Union[str, transformers.image_utils.ChannelDimension] = &lt;ChannelDimension.FIRST: 'channels_first'&gt;,\n    input_data_format: Union[transformers.image_utils.ChannelDimension, str, NoneType] = None,\n    pad_size: Optional[Dict[str, int]] = None,\n    **kwargs,\n) -&gt; transformers.feature_extraction_utils.BatchFeature\nDocstring:\nPreprocess an image or a batch of images so that it can be used by the model.\n\nArgs:\n    images (`ImageInput`):\n        Image or batch of images to preprocess. Expects a single or batch of images with pixel values ranging\n        from 0 to 255. If passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n    annotations (`AnnotationType` or `List[AnnotationType]`, *optional*):\n        List of annotations associated with the image or batch of images. If annotation is for object\n        detection, the annotations should be a dictionary with the following keys:\n        - \"image_id\" (`int`): The image id.\n        - \"annotations\" (`List[Dict]`): List of annotations for an image. Each annotation should be a\n          dictionary. An image can have no annotations, in which case the list should be empty.\n        If annotation is for segmentation, the annotations should be a dictionary with the following keys:\n        - \"image_id\" (`int`): The image id.\n        - \"segments_info\" (`List[Dict]`): List of segments for an image. Each segment should be a dictionary.\n          An image can have no segments, in which case the list should be empty.\n        - \"file_name\" (`str`): The file name of the image.\n    return_segmentation_masks (`bool`, *optional*, defaults to self.return_segmentation_masks):\n        Whether to return segmentation masks.\n    masks_path (`str` or `pathlib.Path`, *optional*):\n        Path to the directory containing the segmentation masks.\n    do_resize (`bool`, *optional*, defaults to self.do_resize):\n        Whether to resize the image.\n    size (`Dict[str, int]`, *optional*, defaults to self.size):\n        Size of the image's `(height, width)` dimensions after resizing. Available options are:\n            - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n                Do NOT keep the aspect ratio.\n            - `{\"shortest_edge\": int, \"longest_edge\": int}`: The image will be resized to a maximum size respecting\n                the aspect ratio and keeping the shortest edge less or equal to `shortest_edge` and the longest edge\n                less or equal to `longest_edge`.\n            - `{\"max_height\": int, \"max_width\": int}`: The image will be resized to the maximum size respecting the\n                aspect ratio and keeping the height less or equal to `max_height` and the width less or equal to\n                `max_width`.\n    resample (`PILImageResampling`, *optional*, defaults to self.resample):\n        Resampling filter to use when resizing the image.\n    do_rescale (`bool`, *optional*, defaults to self.do_rescale):\n        Whether to rescale the image.\n    rescale_factor (`float`, *optional*, defaults to self.rescale_factor):\n        Rescale factor to use when rescaling the image.\n    do_normalize (`bool`, *optional*, defaults to self.do_normalize):\n        Whether to normalize the image.\n    do_convert_annotations (`bool`, *optional*, defaults to self.do_convert_annotations):\n        Whether to convert the annotations to the format expected by the model. Converts the bounding\n        boxes from the format `(top_left_x, top_left_y, width, height)` to `(center_x, center_y, width, height)`\n        and in relative coordinates.\n    image_mean (`float` or `List[float]`, *optional*, defaults to self.image_mean):\n        Mean to use when normalizing the image.\n    image_std (`float` or `List[float]`, *optional*, defaults to self.image_std):\n        Standard deviation to use when normalizing the image.\n    do_pad (`bool`, *optional*, defaults to self.do_pad):\n        Whether to pad the image. If `True`, padding will be applied to the bottom and right of\n        the image with zeros. If `pad_size` is provided, the image will be padded to the specified\n        dimensions. Otherwise, the image will be padded to the maximum height and width of the batch.\n    format (`str` or `AnnotationFormat`, *optional*, defaults to self.format):\n        Format of the annotations.\n    return_tensors (`str` or `TensorType`, *optional*, defaults to self.return_tensors):\n        Type of tensors to return. If `None`, will return the list of images.\n    data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n        The channel dimension format for the output image. Can be one of:\n        - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n        - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n        - Unset: Use the channel dimension format of the input image.\n    input_data_format (`ChannelDimension` or `str`, *optional*):\n        The channel dimension format for the input image. If unset, the channel dimension format is inferred\n        from the input image. Can be one of:\n        - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n        - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n        - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n    pad_size (`Dict[str, int]`, *optional*):\n        The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n        provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n        height and width in the batch.\nFile:      ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/conditional_detr/image_processing_conditional_detr.py\nType:      method\n\n\n\n# Try to preprocess this and it will error\nrandom_sample_preprocessed = image_processor.preprocess(images=random_sample[\"image\"],\n                                                        annotations=random_sample[\"annotations\"])\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[14], line 2\n      1 # Try to preprocess this and it will error\n----&gt; 2 random_sample_preprocessed = image_processor.preprocess(images=random_sample[\"image\"],\n      3                                                         annotations=random_sample[\"annotations\"])\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/conditional_detr/image_processing_conditional_detr.py:1422, in ConditionalDetrImageProcessor.preprocess(self, images, annotations, return_segmentation_masks, masks_path, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, do_convert_annotations, image_mean, image_std, do_pad, format, return_tensors, data_format, input_data_format, pad_size, **kwargs)\n   1420 format = AnnotationFormat(format)\n   1421 if annotations is not None:\n-&gt; 1422     validate_annotations(format, SUPPORTED_ANNOTATION_FORMATS, annotations)\n   1424 if (\n   1425     masks_path is not None\n   1426     and format == AnnotationFormat.COCO_PANOPTIC\n   1427     and not isinstance(masks_path, (pathlib.Path, str))\n   1428 ):\n   1429     raise ValueError(\n   1430         \"The path to the directory containing the mask PNG files should be provided as a\"\n   1431         f\" `pathlib.Path` or string object, but is {type(masks_path)} instead.\"\n   1432     )\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/image_utils.py:791, in validate_annotations(annotation_format, supported_annotation_formats, annotations)\n    789 if annotation_format is AnnotationFormat.COCO_DETECTION:\n    790     if not valid_coco_detection_annotations(annotations):\n--&gt; 791         raise ValueError(\n    792             \"Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts \"\n    793             \"(batch of images) with the following keys: `image_id` and `annotations`, with the latter \"\n    794             \"being a list of annotations in the COCO format.\"\n    795         )\n    797 if annotation_format is AnnotationFormat.COCO_PANOPTIC:\n    798     if not valid_coco_panoptic_annotations(annotations):\n\nValueError: Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts (batch of images) with the following keys: `image_id` and `annotations`, with the latter being a list of annotations in the COCO format.\n\n\n\nTK - error preprocessing annotations when they aren‚Äôt in COCO format\n\nValueError: Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts (batch of images) with the following keys: image_id and annotations, with the latter being a list of annotations in the COCO format.\n\nBut we can preprocess our image as a standalone object if we like.\n\n# Preprocess our target sample with the default preprocessing \nrandom_sample_preprocessed_image_only = image_processor.preprocess(images=random_sample[\"image\"])\n\n# The pixel_values key contains our processed image \nrandom_sample_preprocessed_image_only[\"pixel_values\"][0].shape\n\n(3, 640, 480)\n\n\n\nrandom_sample[\"annotations\"]\n\n{'file_name': ['3e85a851-513d-40b8-8b16-240b365132d8.jpeg',\n  '3e85a851-513d-40b8-8b16-240b365132d8.jpeg',\n  '3e85a851-513d-40b8-8b16-240b365132d8.jpeg',\n  '3e85a851-513d-40b8-8b16-240b365132d8.jpeg'],\n 'image_id': [384, 384, 384, 384],\n 'category_id': [5, 1, 0, 0],\n 'bbox': [[452.70001220703125,\n   485.3999938964844,\n   265.29998779296875,\n   174.1999969482422],\n  [625.5, 459.5, 180.1999969482422, 238.10000610351562],\n  [221.3000030517578, 371.8999938964844, 447.8999938964844, 496.3999938964844],\n  [7.699999809265137, 328.0, 301.3999938964844, 440.5]],\n 'iscrowd': [0, 0, 0, 0],\n 'area': [46215.26171875, 42905.62109375, 222337.5625, 132766.703125]}\n\n\n\n6.1 TK Preprocessing our annotations in COCO format\nOur image_processor expects annotations in COCO format.\nTK - See this requirement in the annotations parameter in the preprocess method docs: https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.preprocess\nMore specifically, it expects the format:\n{\n    \"image_id\": int,\n    \"annotations\": List[Dict] # each annotation is in its own dictionary formatted in COCO style\n}\nTK - See link to COCO format: https://cocodataset.org/#format-data\nCOCO format:\n[{\n    \"image_id\": 42,\n    \"annotations\": [{\n        \"id\": 123456,\n        \"category_id\": 1,\n        \"iscrowd\": 0,\n        \"segmentation\": [\n            [42.0, 55.6, ... 99.3, 102.3]\n        ],\n        \"image_id\": 42, # this matches the 'image_id' field above\n        \"area\": 135381.07,\n        \"bbox\": [523.70,\n                 545.09,\n                 402.79,\n                 336.11]\n    },\n    # Next annotation in the same format as the previous one (one annotation per dict)\n    ...]\n}]\nWhere:\n\n\n\n\n\n\n\n\n\nField\nRequirement\nData Type\nDescription\n\n\n\n\nimage_id (top-level)\nRequired\nInteger\nID of the target image.\n\n\nannotations\nRequired\nList[Dict]\nList of dictionaries with one box annotation per dict. Can be empty if there are no boxes.\n\n\nid\nNot required\nInteger\nID of the particular annotation.\n\n\ncategory_id\nRequired\nInteger\nID of the class the box relates to (e.g.¬†{0: 'bin', 1: 'hand', 2: 'not_bin', 3: 'not_hand', 4: 'not_trash', 5: 'trash'}).\n\n\nsegmentation\nNot required\nList or None\nSegmentation mask related to an annotation instance. Focus is on boxes, not segmentation.\n\n\nimage_id (inside annotations field)\nRequired\nInteger\nID of the target image the particular box relates to, should match image_id on the top-level field.\n\n\narea\nNot required\nFloat\nArea of the target bounding box (e.g.¬†box height * width).\n\n\nbbox\nRequired\nList[Float]\nCoordinates of the target bounding box in XYWH ([x, y, width, height]) format. (x, y) are the top left corner coordinates, width and height are dimensions.\n\n\n\nOur annotation data comes in the format:\n{'image': &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,\n 'image_id': 292,\n 'annotations': {'file_name': ['00347467-13f1-4cb9-94aa-4e4369457e0c.jpeg',\n   '00347467-13f1-4cb9-94aa-4e4369457e0c.jpeg'],\n  'image_id': [292, 292],\n  'category_id': [1, 0],\n  'bbox': [[523.7000122070312,\n    545.0999755859375,\n    402.79998779296875,\n    336.1000061035156],\n   [10.399999618530273,\n    163.6999969482422,\n    943.4000244140625,\n    1101.9000244140625]],\n  'iscrowd': [0, 0],\n  'area': [135381.078125, 1039532.4375]},\n 'label_source': 'manual_prodigy_label',\n 'image_source': 'manual_taken_photo'}\n\n\n6.2 TK - Creating a function to format our annotations\nLet‚Äôs write a function to transform our existing annotation data into the format required by image_processor.\n\n# First create a couple of dataclasses to store our data format\nfrom dataclasses import dataclass, asdict\nfrom typing import List, Tuple\n\n@dataclass\nclass SingleCOCOAnnotation:\n    \"An instance of a single COCO annotation. See COCO format: https://cocodataset.org/#format-data\"\n    image_id: int\n    category_id: int\n    bbox: List[float] # bboxes in format [x_top_left, y_top_left, width, height]\n    area: float = 0.0\n    iscrowd: int = 0\n\n@dataclass\nclass ImageCOCOAnnotations:\n    \"A collection of COCO annotations for a given image_id.\"\n    image_id: int\n    annotations: List[SingleCOCOAnnotation]\n\ndef format_image_annotations_as_coco(\n        image_id: int,\n        categories: List[int],\n        areas: List[float],\n        bboxes: List[Tuple[float, float, float, float]] # bboxes in format \n) -&gt; dict:\n    # Turn input lists into a list of dicts\n    coco_format_annotations = [\n        asdict(SingleCOCOAnnotation(\n            image_id=image_id,\n            category_id=category,\n            bbox=list(bbox),\n            area=area,\n        ))\n        for category, area, bbox in zip(categories, areas, bboxes)\n    ]\n\n    # Return dictionary of annotations with format {\"image_id\": ..., \"annotations\": ...}\n    return asdict(ImageCOCOAnnotations(image_id=image_id,\n                                       annotations=coco_format_annotations))\n\n# Let's try it out\nimage_id = 0\nrandom_sample_categories = random_sample[\"annotations\"][\"category_id\"]\nrandom_sample_areas = random_sample[\"annotations\"][\"area\"]\nrandom_sample_bboxes = random_sample[\"annotations\"][\"bbox\"]\n\nrandom_sample_coco_annotations = format_image_annotations_as_coco(image_id=image_id,\n                                                                  categories=random_sample_categories,\n                                                                  areas=random_sample_areas,\n                                                                  bboxes=random_sample_bboxes)\nrandom_sample_coco_annotations\n\n{'image_id': 0,\n 'annotations': [{'image_id': 0,\n   'category_id': 5,\n   'bbox': [452.70001220703125,\n    485.3999938964844,\n    265.29998779296875,\n    174.1999969482422],\n   'area': 46215.26171875,\n   'iscrowd': 0},\n  {'image_id': 0,\n   'category_id': 1,\n   'bbox': [625.5, 459.5, 180.1999969482422, 238.10000610351562],\n   'area': 42905.62109375,\n   'iscrowd': 0},\n  {'image_id': 0,\n   'category_id': 0,\n   'bbox': [221.3000030517578,\n    371.8999938964844,\n    447.8999938964844,\n    496.3999938964844],\n   'area': 222337.5625,\n   'iscrowd': 0},\n  {'image_id': 0,\n   'category_id': 0,\n   'bbox': [7.699999809265137, 328.0, 301.3999938964844, 440.5],\n   'area': 132766.703125,\n   'iscrowd': 0}]}\n\n\n\nSingleCOCOAnnotation?\n\nInit signature:\nSingleCOCOAnnotation(\n    image_id: int,\n    category_id: int,\n    bbox: List[float],\n    area: float = 0.0,\n    iscrowd: int = 0,\n) -&gt; None\nDocstring:      An instance of a single COCO annotation. See COCO format: https://cocodataset.org/#format-data\nType:           type\nSubclasses:     \n\n\n\nImageCOCOAnnotations?\n\nInit signature:\nImageCOCOAnnotations(\n    image_id: int,\n    annotations: List[__main__.SingleCOCOAnnotation],\n) -&gt; None\nDocstring:      A collection of COCO annotations for a given image_id.\nType:           type\nSubclasses:     \n\n\n\n\n6.3 TK - Preprocess annotations and image\nNow we‚Äôve preprocessed our annotations to be in COCO format, we can use them with image_processor.preprocess.\n\n# Preprocess random sample image and assosciated annotations\n# See docs for preprocess: https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.preprocess \nrandom_sample_preprocessed = image_processor.preprocess(images=random_sample[\"image\"],\n                                                        annotations=random_sample_coco_annotations,\n                                                        return_tensors=\"pt\") # can return as tensors or not \n\nTK - Note: You may see a warning of\n\nThe max_size parameter is deprecated and will be removed in v4.26. Please specify in size['longest_edge'] instead.\n\nIf you are not using the max_size parameter and are using a version of transformers &gt; 4.26, you can ignore this.\n\n# Disable warnings about `max_size` parameter being deprecated (this is okay)\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", message=\"The `max_size` parameter is deprecated*\")\n\n\n# Check the keys of our preprocessed example\nrandom_sample_preprocessed.keys()\n\ndict_keys(['pixel_values', 'pixel_mask', 'labels'])\n\n\nTK - break down each of the above\n\npixel_values = preprocessed pixels (the preprocessed image)\npixel_mask = whether or not to mask the pixels (e.g.¬†0 = mask, 1 = no mask)\nlabels = preprocessed labels (the preprocessed annotations)\n\n\nrandom_sample_preprocessed[\"pixel_values\"]\n\ntensor([[[[-2.0665, -2.1008, -2.0665,  ...,  1.5125,  1.5125,  1.5125],\n          [-2.0665, -2.0837, -2.0494,  ...,  1.5125,  1.4954,  1.4954],\n          [-2.0837, -2.1008, -2.0323,  ...,  1.4783,  1.4783,  1.4783],\n          ...,\n          [ 0.0569,  0.3481,  0.2624,  ..., -0.4054,  0.1939,  0.6906],\n          [ 0.1083,  0.1597,  0.1597,  ...,  0.2111,  0.6221,  0.2796],\n          [ 0.0912,  0.2282,  0.1426,  ...,  0.1426,  0.6563, -0.0801]],\n\n         [[-1.8957, -1.9307, -1.8957,  ...,  1.9559,  1.9559,  1.9559],\n          [-1.8957, -1.9307, -1.8782,  ...,  1.9559,  1.9384,  1.9384],\n          [-1.9307, -1.9307, -1.8606,  ...,  1.9209,  1.9209,  1.9209],\n          ...,\n          [-0.2675,  0.1001,  0.0826,  ..., -0.4951,  0.1176,  0.6429],\n          [-0.1450, -0.0399,  0.0126,  ...,  0.1001,  0.5203,  0.1702],\n          [-0.0924,  0.0651,  0.0126,  ..., -0.0049,  0.5203, -0.2500]],\n\n         [[-1.7522, -1.8044, -1.7870,  ...,  2.4308,  2.4308,  2.4308],\n          [-1.7522, -1.7870, -1.7522,  ...,  2.4308,  2.4134,  2.4134],\n          [-1.7870, -1.7870, -1.7522,  ...,  2.3960,  2.3960,  2.3960],\n          ...,\n          [-0.8458, -0.6193, -0.8110,  ..., -0.8110, -0.2358,  0.1999],\n          [-0.7936, -0.7936, -0.8981,  ..., -0.2358,  0.1476, -0.2707],\n          [-0.7413, -0.6715, -0.8981,  ..., -0.2881,  0.1476, -0.6715]]]])\n\n\n\nrandom_sample_preprocessed[\"pixel_values\"][0].shape\n\ntorch.Size([3, 640, 480])\n\n\n\n# What keys are in the labels?\nrandom_sample_preprocessed[\"labels\"][0].keys()\n\ndict_keys(['size', 'image_id', 'class_labels', 'boxes', 'area', 'iscrowd', 'orig_size'])\n\n\nTK - break it down what‚Äôs in the labels\n\nsize = image size in format [height, width]\nimage_id = ID of image passed in\nclass_labels = list of labels assosciated with image e.g.¬†tensor([5, 1, 0, 0, 4]) -&gt; {0: 'bin', 1: 'hand', 2: 'not_bin', 3: 'not_hand', 4: 'not_trash', 5: 'trash'}\nboxes = list of boxes with coordinates for where the box is on the image in format CXCYWH (normalized)\n\n\nrandom_sample_preprocessed[\"labels\"][0]\n\n{'size': tensor([640, 480]), 'image_id': tensor([0]), 'class_labels': tensor([5, 1, 0, 0]), 'boxes': tensor([[0.6097, 0.4473, 0.2764, 0.1361],\n        [0.7454, 0.4520, 0.1877, 0.1860],\n        [0.4638, 0.4845, 0.4666, 0.3878],\n        [0.1650, 0.4283, 0.3140, 0.3441]]), 'area': tensor([11553.8154, 10726.4053, 55584.3906, 33191.6758]), 'iscrowd': tensor([0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}\n\n\n\nrandom_sample_preprocessed[\"pixel_mask\"][0].shape\n\ntorch.Size([640, 480])\n\n\n\nid2label\n\n{0: 'bin',\n 1: 'hand',\n 2: 'not_bin',\n 3: 'not_hand',\n 4: 'not_trash',\n 5: 'trash',\n 6: 'trash_arm'}\n\n\n\n# Setup the model\ndef create_model():\n    model = AutoModelForObjectDetection.from_pretrained(\n        pretrained_model_name_or_path=MODEL_NAME,\n        label2id=label2id,\n        id2label=id2label,\n        ignore_mismatched_sizes=True,\n        backbone=\"resnet50\"\n    )\n    return model\n\nmodel = create_model()\nmodel\n\nSome weights of ConditionalDetrForObjectDetection were not initialized from the model checkpoint at microsoft/conditional-detr-resnet-50 and are newly initialized because the shapes did not match:\n- class_labels_classifier.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([7]) in the model instantiated\n- class_labels_classifier.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nConditionalDetrForObjectDetection(\n  (model): ConditionalDetrModel(\n    (backbone): ConditionalDetrConvModel(\n      (conv_encoder): ConditionalDetrConvEncoder(\n        (model): FeatureListNet(\n          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n          (bn1): ConditionalDetrFrozenBatchNorm2d()\n          (act1): ReLU(inplace=True)\n          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n          (layer1): Sequential(\n            (0): Bottleneck(\n              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n              (downsample): Sequential(\n                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (1): ConditionalDetrFrozenBatchNorm2d()\n              )\n            )\n            (1): Bottleneck(\n              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (2): Bottleneck(\n              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n          )\n          (layer2): Sequential(\n            (0): Bottleneck(\n              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n              (downsample): Sequential(\n                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n                (1): ConditionalDetrFrozenBatchNorm2d()\n              )\n            )\n            (1): Bottleneck(\n              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (2): Bottleneck(\n              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (3): Bottleneck(\n              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n          )\n          (layer3): Sequential(\n            (0): Bottleneck(\n              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n              (downsample): Sequential(\n                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n                (1): ConditionalDetrFrozenBatchNorm2d()\n              )\n            )\n            (1): Bottleneck(\n              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (2): Bottleneck(\n              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (3): Bottleneck(\n              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (4): Bottleneck(\n              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (5): Bottleneck(\n              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n          )\n          (layer4): Sequential(\n            (0): Bottleneck(\n              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n              (downsample): Sequential(\n                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n                (1): ConditionalDetrFrozenBatchNorm2d()\n              )\n            )\n            (1): Bottleneck(\n              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (2): Bottleneck(\n              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n          )\n        )\n      )\n      (position_embedding): ConditionalDetrSinePositionEmbedding()\n    )\n    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n    (query_position_embeddings): Embedding(300, 256)\n    (encoder): ConditionalDetrEncoder(\n      (layers): ModuleList(\n        (0-5): 6 x ConditionalDetrEncoderLayer(\n          (self_attn): DetrAttention(\n            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): ReLU()\n          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (decoder): ConditionalDetrDecoder(\n      (layers): ModuleList(\n        (0): ConditionalDetrDecoderLayer(\n          (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)\n          (self_attn): ConditionalDetrAttention(\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)\n          (encoder_attn): ConditionalDetrAttention(\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        )\n        (1-5): 5 x ConditionalDetrDecoderLayer(\n          (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)\n          (self_attn): ConditionalDetrAttention(\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_qpos_proj): None\n          (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)\n          (encoder_attn): ConditionalDetrAttention(\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (query_scale): MLP(\n        (layers): ModuleList(\n          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n        )\n      )\n      (ref_point_head): MLP(\n        (layers): ModuleList(\n          (0): Linear(in_features=256, out_features=256, bias=True)\n          (1): Linear(in_features=256, out_features=2, bias=True)\n        )\n      )\n    )\n  )\n  (class_labels_classifier): Linear(in_features=256, out_features=7, bias=True)\n  (bbox_predictor): ConditionalDetrMLPPredictionHead(\n    (layers): ModuleList(\n      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n      (2): Linear(in_features=256, out_features=4, bias=True)\n    )\n  )\n)\n\n\n\nNote: This may output some information about the model not being prepared for a custom dataset due to it originally being prepared for a certain number of classes (e.g.¬†the model can only recognize what it was trained on). We‚Äôve initialized it with an output head to have 4\n\n\nmodel.forward?\n\nSignature:\nmodel.forward(\n    pixel_values: torch.FloatTensor,\n    pixel_mask: Optional[torch.LongTensor] = None,\n    decoder_attention_mask: Optional[torch.LongTensor] = None,\n    encoder_outputs: Optional[torch.FloatTensor] = None,\n    inputs_embeds: Optional[torch.FloatTensor] = None,\n    decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n    labels: Optional[List[dict]] = None,\n    output_attentions: Optional[bool] = None,\n    output_hidden_states: Optional[bool] = None,\n    return_dict: Optional[bool] = None,\n) -&gt; Union[Tuple[torch.FloatTensor], transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrObjectDetectionOutput]\nDocstring:\nThe [`ConditionalDetrForObjectDetection`] forward method, overrides the `__call__` special method.\n\n&lt;Tip&gt;\n\nAlthough the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\ninstance afterwards instead of this since the former takes care of running the pre and post processing steps while\nthe latter silently ignores them.\n\n&lt;/Tip&gt;\n\nArgs:\n    pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n        Pixel values. Padding will be ignored by default should you provide it.\n\n        Pixel values can be obtained using [`AutoImageProcessor`]. See [`ConditionalDetrImageProcessor.__call__`]\n        for details.\n\n    pixel_mask (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):\n        Mask to avoid performing attention on padding pixel values. Mask values selected in `[0, 1]`:\n\n        - 1 for pixels that are real (i.e. **not masked**),\n        - 0 for pixels that are padding (i.e. **masked**).\n\n        [What are attention masks?](../glossary#attention-mask)\n\n    decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n        Not used by default. Can be used to mask object queries.\n    encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n        Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n        `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n        hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n    inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n        Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n        can choose to directly pass a flattened representation of an image.\n    decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n        Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n        embedded representation.\n    output_attentions (`bool`, *optional*):\n        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n        tensors for more detail.\n    output_hidden_states (`bool`, *optional*):\n        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n        more detail.\n    return_dict (`bool`, *optional*):\n        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\n    labels (`List[Dict]` of len `(batch_size,)`, *optional*):\n        Labels for computing the bipartite matching loss. List of dicts, each dictionary containing at least the\n        following 2 keys: 'class_labels' and 'boxes' (the class labels and bounding boxes of an image in the batch\n        respectively). The class labels themselves should be a `torch.LongTensor` of len `(number of bounding boxes\n        in the image,)` and the boxes a `torch.FloatTensor` of shape `(number of bounding boxes in the image, 4)`.\n\n\n    Returns:\n        [`transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrObjectDetectionOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrObjectDetectionOutput`] or a tuple of\n        `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n        elements depending on the configuration ([`ConditionalDetrConfig`]) and inputs.\n\n        - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)) -- Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n          bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n          scale-invariant IoU loss.\n        - **loss_dict** (`Dict`, *optional*) -- A dictionary containing the individual losses. Useful for logging.\n        - **logits** (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`) -- Classification logits (including no-object) for all queries.\n        - **pred_boxes** (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`) -- Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n          values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n          possible padding). You can use [`~ConditionalDetrImageProcessor.post_process_object_detection`] to retrieve the\n          unnormalized bounding boxes.\n        - **auxiliary_outputs** (`list[Dict]`, *optional*) -- Optional, only returned when auxilary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n          and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n          `pred_boxes`) for each decoder layer.\n        - **last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) -- Sequence of hidden-states at the output of the last layer of the decoder of the model.\n        - **decoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n          shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the decoder at the output of each\n          layer plus the initial embedding outputs.\n        - **decoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n          sequence_length)`. Attentions weights of the decoder, after the attention softmax, used to compute the\n          weighted average in the self-attention heads.\n        - **cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n          sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n          used to compute the weighted average in the cross-attention heads.\n        - **encoder_last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) -- Sequence of hidden-states at the output of the last layer of the encoder of the model.\n        - **encoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n          shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n          layer plus the initial embedding outputs.\n        - **encoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n          sequence_length)`. Attentions weights of the encoder, after the attention softmax, used to compute the\n          weighted average in the self-attention heads.\n  \n\n    Examples:\n\n    ```python\n    &gt;&gt;&gt; from transformers import AutoImageProcessor, AutoModelForObjectDetection\n    &gt;&gt;&gt; from PIL import Image\n    &gt;&gt;&gt; import requests\n\n    &gt;&gt;&gt; url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n    &gt;&gt;&gt; image = Image.open(requests.get(url, stream=True).raw)\n\n    &gt;&gt;&gt; image_processor = AutoImageProcessor.from_pretrained(\"microsoft/conditional-detr-resnet-50\")\n    &gt;&gt;&gt; model = AutoModelForObjectDetection.from_pretrained(\"microsoft/conditional-detr-resnet-50\")\n\n    &gt;&gt;&gt; inputs = image_processor(images=image, return_tensors=\"pt\")\n\n    &gt;&gt;&gt; outputs = model(**inputs)\n\n    &gt;&gt;&gt; # convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n    &gt;&gt;&gt; target_sizes = torch.tensor([image.size[::-1]])\n    &gt;&gt;&gt; results = image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[\n    ...     0\n    ... ]\n    &gt;&gt;&gt; for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    ...     box = [round(i, 2) for i in box.tolist()]\n    ...     print(\n    ...         f\"Detected {model.config.id2label[label.item()]} with confidence \"\n    ...         f\"{round(score.item(), 3)} at location {box}\"\n    ...     )\n    Detected remote with confidence 0.833 at location [38.31, 72.1, 177.63, 118.45]\n    Detected cat with confidence 0.831 at location [9.2, 51.38, 321.13, 469.0]\n    Detected cat with confidence 0.804 at location [340.3, 16.85, 642.93, 370.95]\n    Detected remote with confidence 0.683 at location [334.48, 73.49, 366.37, 190.01]\n    Detected couch with confidence 0.535 at location [0.52, 1.19, 640.35, 475.1]\n    ```\nFile:      ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/conditional_detr/modeling_conditional_detr.py\nType:      method\n\n\n\nrandom_sample_preprocessed[\"pixel_values\"][0].shape # [color_channels, height, width]\n\ntorch.Size([3, 640, 480])\n\n\n\n# Do a single forward pass with the model\nrandom_sample_outputs = model(pixel_values=random_sample_preprocessed[\"pixel_values\"][0].unsqueeze(0), # model expects input [batch_size, color_channels, height, width]\n                              pixel_mask=None)\nrandom_sample_outputs\n\nConditionalDetrObjectDetectionOutput(loss=None, loss_dict=None, logits=tensor([[[ 0.0454, -0.0711, -0.4182,  ...,  0.2894,  0.0483,  0.0123],\n         [-0.1012, -0.1597, -0.1998,  ..., -0.0486, -0.1782, -0.2652],\n         [ 0.1434,  0.0662, -0.1789,  ...,  0.0542, -0.0454, -0.0935],\n         ...,\n         [-0.3237, -0.4062, -0.1989,  ...,  0.2875, -0.0910,  0.2941],\n         [ 0.1114, -0.0177, -0.3141,  ..., -0.0593, -0.1495, -0.1393],\n         [-0.1669, -0.1889,  0.1891,  ...,  0.1096, -0.2838, -0.0589]]],\n       grad_fn=&lt;ViewBackward0&gt;), pred_boxes=tensor([[[0.8267, 0.6865, 0.3329, 0.6065],\n         [0.6527, 0.1801, 0.0381, 0.0135],\n         [0.8987, 0.5712, 0.2006, 0.2254],\n         ...,\n         [0.3474, 0.3090, 0.6915, 0.1174],\n         [0.8373, 0.5285, 0.3022, 0.1941],\n         [0.0810, 0.2927, 0.1605, 0.0432]]], grad_fn=&lt;SigmoidBackward0&gt;), auxiliary_outputs=None, last_hidden_state=tensor([[[ 0.2234,  0.0444,  0.9698,  ..., -1.0443, -0.1137,  0.3582],\n         [ 0.2838, -0.6804,  0.3960,  ...,  0.7212,  0.3551,  0.3658],\n         [ 0.5051, -0.0147,  0.5885,  ..., -1.2090, -0.0941, -0.0717],\n         ...,\n         [ 0.4280, -1.5612,  0.3054,  ..., -0.8336,  0.0790, -0.3486],\n         [ 0.2858, -0.0132,  0.5693,  ..., -1.1525, -0.1821, -0.1940],\n         [ 0.2017,  0.1479, -0.3311,  ..., -1.1814, -0.0651, -0.0979]]],\n       grad_fn=&lt;NativeLayerNormBackward0&gt;), decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[-0.3918,  0.4741, -0.3829,  ..., -0.5659,  0.4583,  0.3095],\n         [ 0.1083,  0.5762, -0.0826,  ...,  0.2379,  0.1619,  0.3629],\n         [ 0.1359,  0.6453, -0.1079,  ..., -0.1028,  0.1878,  0.3184],\n         ...,\n         [ 0.1694,  0.8391, -0.1381,  ...,  0.1942,  0.0713,  0.2323],\n         [ 0.1709,  0.6931, -0.0919,  ...,  0.2428,  0.0508,  0.1932],\n         [-0.1842,  0.4742, -0.1434,  ..., -0.1434,  0.2518,  0.2516]]],\n       grad_fn=&lt;NativeLayerNormBackward0&gt;), encoder_hidden_states=None, encoder_attentions=None)\n\n\n\n# Check the keys of the output\nrandom_sample_outputs.keys()\n\nodict_keys(['logits', 'pred_boxes', 'last_hidden_state', 'encoder_last_hidden_state'])\n\n\n\n# We get 300 total boxes with shape the same as our number of labels\nrandom_sample_outputs.logits.shape\n\ntorch.Size([1, 300, 7])\n\n\n\nrandom_sample_outputs.pred_boxes.shape\n\ntorch.Size([1, 300, 4])\n\n\nTK - note: see forward() method for output format of boxes -&gt; https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection.forward\nFrom the docs:\n\nReturns ‚Ä¶ pred_boxes (torch.FloatTensor of shape (batch_size, num_queries, 4)) ‚Äî Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding possible padding). You can use post_process_object_detection() to retrieve the unnormalized bounding boxes.\n\n\n# Example pred box output\n# Box output comes in the form CXCYWH normalized (e.g. [center_X, center_Y, width, height]) to be between 0 and 1, this is in the docs\nrandom_sample_outputs.pred_boxes[:, 0]\n\ntensor([[0.8267, 0.6865, 0.3329, 0.6065]], grad_fn=&lt;SelectBackward0&gt;)\n\n\n\n# Model outputs one logit per category value (e.g. 6 categories = 6 logits)\nlen(categories.names)\n\n7\n\n\n\n# For example, one value for each of the following:\nlabel2id\n\n{'bin': 0,\n 'hand': 1,\n 'not_bin': 2,\n 'not_hand': 3,\n 'not_trash': 4,\n 'trash': 5,\n 'trash_arm': 6}\n\n\n\nrandom_sample\n\n{'image': &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,\n 'image_id': 384,\n 'annotations': {'file_name': ['3e85a851-513d-40b8-8b16-240b365132d8.jpeg',\n   '3e85a851-513d-40b8-8b16-240b365132d8.jpeg',\n   '3e85a851-513d-40b8-8b16-240b365132d8.jpeg',\n   '3e85a851-513d-40b8-8b16-240b365132d8.jpeg'],\n  'image_id': [384, 384, 384, 384],\n  'category_id': [5, 1, 0, 0],\n  'bbox': [[452.70001220703125,\n    485.3999938964844,\n    265.29998779296875,\n    174.1999969482422],\n   [625.5, 459.5, 180.1999969482422, 238.10000610351562],\n   [221.3000030517578,\n    371.8999938964844,\n    447.8999938964844,\n    496.3999938964844],\n   [7.699999809265137, 328.0, 301.3999938964844, 440.5]],\n  'iscrowd': [0, 0, 0, 0],\n  'area': [46215.26171875, 42905.62109375, 222337.5625, 132766.703125]},\n 'label_source': 'manual_prodigy_label',\n 'image_source': 'manual_taken_photo'}\n\n\n\n\n6.4 Post process a single output\nAlways a good step to get your model working end-to-end on a single sample and then upgrading it.\nBox formats:\n\nStarting data (the input data) -&gt; [x_top_left, y_top_left, width, height] -&gt; XYWH (absolute)\nOut of image_processor.preprocess() -&gt; [center_x, center_y, width, height] -&gt; CXCYWH (normalized) -&gt; into model\n\nSee docs: https://huggingface.co/docs/transformers.js/en/custom_usage\n\nOut of model -&gt; [center_x, center_y, width, height] -&gt; CXCYWH (normalized)\n\nSee docs for forward() and output pred_boxes: https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection.forward\n\nOut of image_processor.post_process_object_detection() -&gt; [x_top_left, y_top_left, x_bottom_right, y_bottom_right] -&gt; XYXY\n\nThis is PASCL VOC format - (xmin, ymin, xmax, ymax)\nSee docs: https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.post_process_object_detection\n\n\n\n# Check the keys of the labels for the image\nrandom_sample_preprocessed[\"labels\"][0].keys()\n\ndict_keys(['size', 'image_id', 'class_labels', 'boxes', 'area', 'iscrowd', 'orig_size'])\n\n\n\nprint(f\"[INFO] Image original size: {random_sample_preprocessed.labels[0].orig_size} (height, width)\")\nprint(f\"[INFO] Image size after preprocessing: {random_sample_preprocessed.labels[0].size} (height, width)\")\n\n[INFO] Image original size: tensor([1280,  960]) (height, width)\n[INFO] Image size after preprocessing: tensor([640, 480]) (height, width)\n\n\n\n# Output logits will be post-processed to turn into prediction probabilities as well as boxes\n\n# Get pred probs from logits, this will be used for our threshold parameter in post_process_object_detection \ntorch.softmax(random_sample_outputs.logits, dim=-1)\n\ntensor([[[0.1471, 0.1309, 0.0925,  ..., 0.1878, 0.1475, 0.1423],\n         [0.1330, 0.1255, 0.1205,  ..., 0.1402, 0.1232, 0.1129],\n         [0.1611, 0.1492, 0.1167,  ..., 0.1474, 0.1334, 0.1272],\n         ...,\n         [0.0988, 0.0910, 0.1119,  ..., 0.1821, 0.1247, 0.1833],\n         [0.1683, 0.1479, 0.1100,  ..., 0.1419, 0.1297, 0.1310],\n         [0.1238, 0.1211, 0.1767,  ..., 0.1632, 0.1101, 0.1379]]],\n       grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\n\nrandom_sample_outputs_post_processed = image_processor.post_process_object_detection(\n    outputs=random_sample_outputs,\n    threshold=0.3, # prediction probability threshold for boxes (note: boxes from an untrained model will likely be bad)\n    target_sizes=[random_sample_preprocessed[\"labels\"][0][\"orig_size\"]] # original input image size (or whichever target size you'd like), required to be same number of input items in a list\n)\n\nrandom_sample_outputs_post_processed\n\n[{'scores': tensor([0.6839, 0.6737, 0.6616, 0.6614, 0.6574, 0.6541, 0.6478, 0.6476, 0.6475,\n          0.6475, 0.6472, 0.6472, 0.6448, 0.6444, 0.6436, 0.6434, 0.6426, 0.6419,\n          0.6416, 0.6408, 0.6404, 0.6383, 0.6382, 0.6374, 0.6372, 0.6359, 0.6354,\n          0.6352, 0.6346, 0.6338, 0.6310, 0.6308, 0.6302, 0.6280, 0.6277, 0.6273,\n          0.6272, 0.6272, 0.6271, 0.6265, 0.6265, 0.6265, 0.6259, 0.6255, 0.6248,\n          0.6243, 0.6242, 0.6241, 0.6237, 0.6229, 0.6223, 0.6221, 0.6215, 0.6213,\n          0.6207, 0.6207, 0.6203, 0.6199, 0.6196, 0.6195, 0.6185, 0.6184, 0.6183,\n          0.6177, 0.6163, 0.6160, 0.6150, 0.6144, 0.6144, 0.6139, 0.6139, 0.6137,\n          0.6135, 0.6129, 0.6125, 0.6124, 0.6108, 0.6106, 0.6104, 0.6101, 0.6100,\n          0.6099, 0.6097, 0.6092, 0.6089, 0.6089, 0.6085, 0.6085, 0.6079, 0.6076,\n          0.6070, 0.6070, 0.6068, 0.6063, 0.6057, 0.6057, 0.6056, 0.6055, 0.6053,\n          0.6051], grad_fn=&lt;IndexBackward0&gt;),\n  'labels': tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n          3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 4, 3, 3, 3, 3, 4,\n          3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 4, 4, 3, 6, 3, 4, 3, 3, 3, 3,\n          3, 3, 3, 3, 3, 3, 4, 3, 4, 3, 3, 4, 3, 3, 3, 3, 4, 3, 4, 4, 3, 3, 4, 4,\n          3, 3, 6, 3]),\n  'boxes': tensor([[ 1.5021e+02,  2.2275e+02,  9.2612e+02,  3.1100e+02],\n          [ 3.0236e+02,  2.3473e+02,  8.8168e+02,  3.0422e+02],\n          [ 5.0880e+02,  2.1115e+02,  5.5853e+02,  2.3331e+02],\n          [ 4.7810e+02,  1.9805e+02,  5.2191e+02,  2.2018e+02],\n          [ 4.3730e+02,  2.4722e+02,  9.2742e+02,  3.1818e+02],\n          [ 9.0321e+02,  3.0775e+02,  9.6117e+02,  3.3864e+02],\n          [ 7.1403e+02,  2.3229e+02,  7.5484e+02,  2.4963e+02],\n          [ 6.4448e+02,  2.3185e+02,  7.0163e+02,  2.4743e+02],\n          [ 9.7196e+01,  2.4972e+02,  8.0122e+02,  3.4492e+02],\n          [ 6.9588e+02,  2.2955e+02,  7.2308e+02,  2.4626e+02],\n          [ 6.8868e+02,  2.3618e+02,  7.3616e+02,  2.5229e+02],\n          [ 6.1711e+02,  2.2698e+02,  6.7091e+02,  2.4342e+02],\n          [ 1.1697e+02,  2.0081e+02,  5.3524e+02,  2.5107e+02],\n          [ 4.8558e+02,  2.0865e+02,  5.4637e+02,  2.3481e+02],\n          [ 7.4454e+02,  2.2994e+02,  7.8603e+02,  2.5308e+02],\n          [ 6.8199e+02,  2.3242e+02,  7.2622e+02,  2.4876e+02],\n          [ 5.3925e+02,  2.2372e+02,  5.9300e+02,  2.4156e+02],\n          [ 4.4184e+02,  2.0462e+02,  4.7448e+02,  2.2387e+02],\n          [ 6.6260e+02,  2.3085e+02,  7.0588e+02,  2.4724e+02],\n          [ 7.2336e+02,  2.3609e+02,  7.7372e+02,  2.5295e+02],\n          [ 5.9377e+02,  2.3010e+02,  6.5768e+02,  2.4654e+02],\n          [ 6.5659e+02,  2.3656e+02,  7.1577e+02,  2.5289e+02],\n          [ 2.1040e+02,  1.9925e+02,  3.2352e+02,  2.3685e+02],\n          [ 1.8045e+02,  1.5578e+02,  3.1142e+02,  1.9401e+02],\n          [ 4.2432e+02,  2.0136e+02,  4.5325e+02,  2.1855e+02],\n          [ 7.8032e+02,  2.2787e+02,  8.3353e+02,  2.5188e+02],\n          [ 6.4112e+02,  2.1902e+02,  6.7090e+02,  2.3924e+02],\n          [ 3.7899e+02,  2.4951e+02,  7.8740e+02,  3.0072e+02],\n          [ 1.6532e+02,  2.3143e+02,  3.2209e+02,  2.6452e+02],\n          [ 1.0642e+02,  2.3275e+02,  2.2752e+02,  2.5950e+02],\n          [-2.7386e+00,  2.8250e+02,  9.5569e+02,  8.3506e+02],\n          [ 5.6764e+02,  2.2579e+02,  6.0133e+02,  2.4452e+02],\n          [ 7.2771e+02,  2.3997e+02,  7.9102e+02,  2.5931e+02],\n          [ 1.3404e+02,  4.4818e+02,  9.4260e+02,  8.4707e+02],\n          [ 4.5860e+02,  4.7699e+02,  5.8809e+02,  5.3134e+02],\n          [ 1.6226e+02,  2.1322e+02,  3.1575e+02,  2.5415e+02],\n          [ 5.8245e+02,  2.3491e+02,  7.3097e+02,  2.5595e+02],\n          [ 9.5602e+02,  2.3172e+02,  9.5996e+02,  2.8035e+02],\n          [ 7.6245e+02,  2.3498e+02,  8.0948e+02,  2.5390e+02],\n          [ 8.5605e+02,  3.1974e+02,  9.6036e+02,  3.4395e+02],\n          [ 1.4028e+01,  2.9742e+02,  6.6831e+02,  7.8667e+02],\n          [ 2.1273e+00,  8.9889e+02,  9.6114e+02,  1.2628e+03],\n          [ 3.9504e+02,  2.1397e+02,  4.3043e+02,  2.2851e+02],\n          [ 3.4446e-01,  1.8749e+02,  7.8433e+01,  2.2499e+02],\n          [ 6.7294e+02,  2.5800e+02,  7.6941e+02,  2.8344e+02],\n          [ 6.0833e+02,  2.2191e+02,  6.4493e+02,  2.3915e+02],\n          [-4.1992e+00,  3.3792e+02,  9.5432e+02,  1.1725e+03],\n          [ 3.5863e+02,  2.0646e+02,  3.8990e+02,  2.3022e+02],\n          [ 4.6378e+02,  2.2039e+02,  5.2133e+02,  2.3814e+02],\n          [-5.5702e+00,  2.8453e+02,  8.7009e+02,  7.9807e+02],\n          [ 7.9385e+02,  2.3517e+02,  8.4780e+02,  2.5737e+02],\n          [ 8.8628e+02,  2.7585e+02,  9.6165e+02,  3.1819e+02],\n          [-1.4085e+01,  7.4535e+02,  9.4306e+02,  1.2644e+03],\n          [ 7.6982e-01,  4.5878e+02,  9.5951e+02,  8.9642e+02],\n          [ 3.8868e+02,  1.9604e+02,  4.2339e+02,  2.1367e+02],\n          [ 1.0205e+02,  2.3049e+02,  1.9162e+02,  2.5180e+02],\n          [ 1.5999e+02,  1.4399e+02,  2.6803e+02,  1.9518e+02],\n          [ 8.3596e+02,  2.8685e+02,  9.6574e+02,  3.4110e+02],\n          [ 4.0079e+02,  2.0236e+02,  4.3893e+02,  2.2204e+02],\n          [ 2.2829e-02,  1.7629e+02,  7.5778e+00,  2.1321e+02],\n          [ 4.0679e+02,  2.0669e+02,  4.4338e+02,  2.2396e+02],\n          [-1.5052e+00,  3.2000e+02,  6.8636e+02,  8.1026e+02],\n          [-1.0880e+01,  4.2899e+02,  9.4746e+02,  8.7154e+02],\n          [ 8.5131e+00,  5.3675e+02,  6.7816e+02,  8.2705e+02],\n          [ 5.8866e+02,  2.2367e+02,  6.3574e+02,  2.4067e+02],\n          [ 1.4028e+01,  2.9742e+02,  6.6831e+02,  7.8667e+02],\n          [ 7.6864e+02,  3.0461e+02,  8.5804e+02,  3.3697e+02],\n          [ 3.2264e+02,  7.8797e+02,  3.9313e+02,  8.3845e+02],\n          [ 1.2273e+02,  3.3624e+02,  2.9674e+02,  3.8669e+02],\n          [ 4.9169e+02,  6.0451e+02,  5.7784e+02,  6.6474e+02],\n          [ 7.3638e+01,  1.6488e+02,  1.4301e+02,  2.1829e+02],\n          [ 8.8692e+02,  2.2889e+02,  9.5606e+02,  2.5846e+02],\n          [ 2.5381e+02,  1.2226e+03,  9.6300e+02,  1.2849e+03],\n          [-1.0880e+01,  4.2899e+02,  9.4746e+02,  8.7154e+02],\n          [-2.6988e+00,  2.1334e+02,  5.3616e+01,  2.4198e+02],\n          [ 8.3390e+02,  2.2830e+02,  8.8685e+02,  2.5244e+02],\n          [ 9.3645e+00,  2.3032e+02,  1.1699e+02,  2.6169e+02],\n          [ 5.2715e+02,  2.8053e+02,  5.8582e+02,  3.0946e+02],\n          [ 4.9539e+02,  2.8255e+02,  5.5250e+02,  3.0303e+02],\n          [ 8.7392e+02,  3.0168e+02,  9.6059e+02,  3.3615e+02],\n          [ 2.7833e+02,  4.6456e+02,  9.4896e+02,  9.1551e+02],\n          [ 4.8947e+02,  4.1488e+02,  6.1218e+02,  4.9252e+02],\n          [ 7.5197e+02,  2.5210e+02,  8.2422e+02,  2.7998e+02],\n          [ 2.5285e+02,  4.3588e+02,  9.3822e+02,  8.7275e+02],\n          [ 8.7054e+02,  2.2011e+02,  9.3370e+02,  2.4845e+02],\n          [ 9.0750e+02,  3.1742e+02,  9.6316e+02,  3.5016e+02],\n          [-4.3446e-01,  2.1810e+02,  2.3509e+01,  2.4937e+02],\n          [ 1.1742e+02,  2.1277e+02,  3.1611e+02,  2.5940e+02],\n          [ 9.6207e+01,  3.5631e+02,  9.5135e+02,  9.5458e+02],\n          [ 2.7833e+02,  4.6456e+02,  9.4896e+02,  9.1551e+02],\n          [ 4.0679e+02,  2.0669e+02,  4.4338e+02,  2.2396e+02],\n          [-1.1057e+01,  2.5194e+02,  3.7717e+02,  7.1443e+02],\n          [ 3.7007e+02,  1.9513e+02,  4.0063e+02,  2.1357e+02],\n          [ 4.0936e+02,  5.2738e+02,  6.3709e+02,  6.3685e+02],\n          [ 6.5659e+02,  2.3656e+02,  7.1577e+02,  2.5289e+02],\n          [ 7.2771e+02,  2.3997e+02,  7.9102e+02,  2.5931e+02],\n          [ 2.6593e+02,  1.8549e+02,  3.8638e+02,  2.2928e+02],\n          [ 4.9952e+02,  1.2543e+03,  8.9540e+02,  1.2808e+03],\n          [-1.1057e+01,  2.5194e+02,  3.7717e+02,  7.1443e+02],\n          [-4.4207e+00,  2.8687e+02,  9.5472e+02,  1.0455e+03]],\n         grad_fn=&lt;IndexBackward0&gt;)}]\n\n\nTK - let‚Äôs visualize, visualize, visualize!\n\n# Extract scores, labels and boxes\nrandom_sample_pred_scores = random_sample_outputs_post_processed[0][\"scores\"]\nrandom_sample_pred_labels = random_sample_outputs_post_processed[0][\"labels\"]\nrandom_sample_pred_boxes = random_sample_outputs_post_processed[0][\"boxes\"]\n\n# Create a list of labels to plot on the boxes \nrandom_sample_labels_to_plot = [f\"Pred: {id2label[label_pred.item()]} ({round(score_pred.item(), 4)})\" \n                  for label_pred, score_pred in zip(random_sample_pred_labels, random_sample_pred_scores)]\n\nprint(f\"[INFO] Labels with scores: {random_sample_labels_to_plot}\")\n\n# Plot the random sample image with randomly predicted boxes (these will be very poor since the model is not trained on our data yet)\nto_pil_image(\n    pic=draw_bounding_boxes(\n        image=pil_to_tensor(pic=random_sample[\"image\"]),\n        boxes=random_sample_pred_boxes,\n        labels=random_sample_labels_to_plot,\n        width=3\n    )\n)\n\n[INFO] Labels with scores: ['Pred: not_hand (0.6839)', 'Pred: not_hand (0.6737)', 'Pred: not_hand (0.6616)', 'Pred: not_hand (0.6614)', 'Pred: not_hand (0.6574)', 'Pred: not_hand (0.6541)', 'Pred: not_hand (0.6478)', 'Pred: not_hand (0.6476)', 'Pred: not_hand (0.6475)', 'Pred: not_hand (0.6475)', 'Pred: not_hand (0.6472)', 'Pred: not_hand (0.6472)', 'Pred: not_hand (0.6448)', 'Pred: not_hand (0.6444)', 'Pred: not_hand (0.6436)', 'Pred: not_hand (0.6434)', 'Pred: not_hand (0.6426)', 'Pred: not_hand (0.6419)', 'Pred: not_hand (0.6416)', 'Pred: not_hand (0.6408)', 'Pred: not_hand (0.6404)', 'Pred: not_hand (0.6383)', 'Pred: not_hand (0.6382)', 'Pred: not_hand (0.6374)', 'Pred: not_hand (0.6372)', 'Pred: not_hand (0.6359)', 'Pred: not_hand (0.6354)', 'Pred: not_hand (0.6352)', 'Pred: not_hand (0.6346)', 'Pred: not_hand (0.6338)', 'Pred: not_hand (0.631)', 'Pred: not_hand (0.6308)', 'Pred: not_hand (0.6302)', 'Pred: not_hand (0.628)', 'Pred: not_hand (0.6277)', 'Pred: not_hand (0.6273)', 'Pred: not_hand (0.6272)', 'Pred: not_hand (0.6272)', 'Pred: not_hand (0.6271)', 'Pred: not_hand (0.6265)', 'Pred: not_trash (0.6265)', 'Pred: not_hand (0.6265)', 'Pred: not_trash (0.6259)', 'Pred: not_hand (0.6255)', 'Pred: not_hand (0.6248)', 'Pred: not_hand (0.6243)', 'Pred: not_hand (0.6242)', 'Pred: not_trash (0.6241)', 'Pred: not_hand (0.6237)', 'Pred: not_trash (0.6229)', 'Pred: not_hand (0.6223)', 'Pred: not_hand (0.6221)', 'Pred: not_hand (0.6215)', 'Pred: not_hand (0.6213)', 'Pred: not_hand (0.6207)', 'Pred: not_hand (0.6207)', 'Pred: not_hand (0.6203)', 'Pred: not_hand (0.6199)', 'Pred: not_hand (0.6196)', 'Pred: not_hand (0.6195)', 'Pred: not_hand (0.6185)', 'Pred: trash_arm (0.6184)', 'Pred: not_trash (0.6183)', 'Pred: not_trash (0.6177)', 'Pred: not_hand (0.6163)', 'Pred: trash_arm (0.616)', 'Pred: not_hand (0.615)', 'Pred: not_trash (0.6144)', 'Pred: not_hand (0.6144)', 'Pred: not_hand (0.6139)', 'Pred: not_hand (0.6139)', 'Pred: not_hand (0.6137)', 'Pred: not_hand (0.6135)', 'Pred: not_hand (0.6129)', 'Pred: not_hand (0.6125)', 'Pred: not_hand (0.6124)', 'Pred: not_hand (0.6108)', 'Pred: not_hand (0.6106)', 'Pred: not_trash (0.6104)', 'Pred: not_hand (0.6101)', 'Pred: not_trash (0.61)', 'Pred: not_hand (0.6099)', 'Pred: not_hand (0.6097)', 'Pred: not_trash (0.6092)', 'Pred: not_hand (0.6089)', 'Pred: not_hand (0.6089)', 'Pred: not_hand (0.6085)', 'Pred: not_hand (0.6085)', 'Pred: not_trash (0.6079)', 'Pred: not_hand (0.6076)', 'Pred: not_trash (0.607)', 'Pred: not_trash (0.607)', 'Pred: not_hand (0.6068)', 'Pred: not_hand (0.6063)', 'Pred: not_trash (0.6057)', 'Pred: not_trash (0.6057)', 'Pred: not_hand (0.6056)', 'Pred: not_hand (0.6055)', 'Pred: trash_arm (0.6053)', 'Pred: not_hand (0.6051)']\n\n\n\n\n\n\n\n\n\nOur predictions are poor since our model hasn‚Äôt been specifically trained on our data.\nBut we can improve them by fine-tuning the model to our dataset.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#tk---fine-tune-the-model-to-our-dataset",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#tk---fine-tune-the-model-to-our-dataset",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "7 TK - Fine-tune the model to our dataset",
    "text": "7 TK - Fine-tune the model to our dataset\nSteps: - preprocess dataset (no augmentation) - get it ready for a model to train on - train model - inspect the results of the trained model\n\n7.1 TK - Preprocess dataset for model\n\nWe‚Äôve preprocessed and tried one sample, now we can do the same for batches of data.\n\nUPTOHERE\nNext: - TK - write a function to transform batches of images (no augmentation, later can add augmentation) - TK - e.g.¬†call it ‚Äúpreprocess_batch_of_examples‚Äù - TK - preprocess datasets using .with_transform (only need one function to batchify data, can add transforms later) - TK - create a collate function\n\ndef preprocess_batch(examples,\n                    #  transforms, # Note: Could optionally add transforms (e.g. data augmentation) here \n                     image_processor):\n    \"\"\"\n    Function to preprocess batches of data.\n\n    Can optionally apply a transform later on.\n    \"\"\"\n    images = []\n    coco_annotations = [] \n\n    for image, image_id, annotations_dict in zip(examples[\"image\"], examples[\"image_id\"], examples[\"annotations\"]):\n        # Note: may need to open image if it is an image path rather than PIL.Image\n        bbox_list = annotations_dict[\"bbox\"]\n        category_list = annotations_dict[\"category_id\"]\n        area_list = annotations_dict[\"area\"]\n    \n        # Note: Could optionally apply a transform here.\n        ###\n\n        # Format the annotations into COCO format\n        cooc_format_annotations = format_image_annotations_as_coco(image_id=image_id,\n                                                                   categories=category_list,\n                                                                   areas=area_list,\n                                                                   bboxes=bbox_list)\n        \n        # Add images/annotations to their respective lists\n        images.append(image)\n        coco_annotations.append(cooc_format_annotations)\n\n    \n    # Apply the image processor to lists of images and annotations\n    preprocessed_batch = image_processor.preprocess(images=images,\n                                                    annotations=coco_annotations,\n                                                    return_tensors=\"pt\")\n    \n    return preprocessed_batch\n\n\n# Create a partial function for preprocessing\nfrom functools import partial\n\n# Note: Could create separate \npreprocess_batch_partial = partial(preprocess_batch,\n                                   image_processor=image_processor)\n\n\n\n7.2 TK - Split the data\n\n# Split the data\ndataset_split = dataset[\"train\"].train_test_split(test_size=0.3, seed=42) # split the dataset into 70/30 train/test\ndataset_test_val_split = dataset_split[\"test\"].train_test_split(test_size=0.6, seed=42) # split the test set into 40/60 validation/test\n\n# Create splits\ndataset[\"train\"] = dataset_split[\"train\"]\ndataset[\"validation\"] = dataset_test_val_split[\"train\"]\ndataset[\"test\"] = dataset_test_val_split[\"test\"]\n\ndataset\n\nTK - apply processing function to each split\n\n# Apply the preprocessing function to the datasets (the preprocessing will happen on the fly, e.g. when the dataset is called rather than in-place)\nprocessed_dataset = dataset.copy()\nprocessed_dataset[\"train\"] = dataset[\"train\"].with_transform(transform=preprocess_batch_partial)\nprocessed_dataset[\"validation\"] = dataset[\"validation\"].with_transform(transform=preprocess_batch_partial)\nprocessed_dataset[\"test\"] = dataset[\"test\"].with_transform(transform=preprocess_batch_partial)\n\n\nprocessed_dataset[\"validation\"][0]\n\n{'pixel_values': tensor([[[ 0.1254,  0.1254,  0.1597,  ..., -2.0837, -1.9809, -1.9295],\n          [ 0.1426,  0.1254,  0.1597,  ..., -2.0494, -1.9638, -1.9467],\n          [ 0.1426,  0.1426,  0.1597,  ..., -1.9467, -1.9295, -1.9467],\n          ...,\n          [ 1.2899,  1.0502,  1.1358,  ...,  0.7248,  0.7933,  0.7762],\n          [ 1.4098,  1.1872,  1.0331,  ...,  0.7077,  0.7419,  0.7419],\n          [ 1.2728,  0.9646,  0.9303,  ...,  0.7077,  0.7591,  0.7248]],\n \n         [[ 1.2206,  1.1856,  1.1506,  ..., -1.9832, -1.8782, -1.7731],\n          [ 1.2381,  1.1856,  1.1506,  ..., -1.9657, -1.8606, -1.8256],\n          [ 1.2381,  1.2031,  1.1681,  ..., -1.8606, -1.8256, -1.8431],\n          ...,\n          [ 1.2906,  1.0630,  1.1506,  ...,  0.3803,  0.4503,  0.4328],\n          [ 1.4307,  1.2031,  1.0280,  ...,  0.3627,  0.3978,  0.3978],\n          [ 1.2906,  0.9755,  0.9230,  ...,  0.3627,  0.4153,  0.3803]],\n \n         [[ 2.1346,  2.2217,  2.1868,  ..., -1.7173, -1.6127, -1.5604],\n          [ 2.1520,  2.2217,  2.1868,  ..., -1.6999, -1.5953, -1.5779],\n          [ 2.1694,  2.2217,  2.1868,  ..., -1.5953, -1.5430, -1.5604],\n          ...,\n          [ 1.2108,  0.9842,  1.0539,  ...,  0.3568,  0.4265,  0.4091],\n          [ 1.3154,  1.0888,  0.9494,  ...,  0.3393,  0.3742,  0.3742],\n          [ 1.1759,  0.8622,  0.8448,  ...,  0.3393,  0.3916,  0.3568]]]),\n 'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         ...,\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1]]),\n 'labels': {'size': tensor([640, 480]), 'image_id': tensor([719]), 'class_labels': tensor([4, 4, 1, 5, 0, 0]), 'boxes': tensor([[0.1898, 0.1767, 0.2161, 0.1620],\n         [0.5669, 0.1938, 0.0742, 0.0805],\n         [0.7672, 0.7768, 0.4526, 0.4327],\n         [0.4715, 0.6213, 0.2235, 0.1502],\n         [0.3973, 0.5639, 0.7729, 0.6337],\n         [0.6906, 0.4581, 0.5110, 0.4600]]), 'area': tensor([ 10753.6875,   1833.4000,  60167.3867,  10316.8945, 150459.0469,\n          72216.3203]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}}\n\n\n\n# Now when we call one or more of our samples, the preprocessing will take place\nprocessed_dataset[\"train\"][0:10]\n\n{'pixel_values': tensor([[[[-1.5870, -1.5870, -1.6042,  ..., -1.2617, -1.2617, -1.2788],\n          [-1.5870, -1.5870, -1.5870,  ..., -0.9363, -0.9192, -0.9192],\n          [-1.6042, -1.5870, -1.5870,  ..., -0.8164, -0.8335, -0.8164],\n          ...,\n          [-1.2959, -1.4329, -0.5938,  ..., -0.5596, -0.2856, -0.4054],\n          [-1.2103, -0.9192, -0.3541,  ..., -0.5596,  0.1426,  0.1768],\n          [-0.5938, -0.6109, -0.7137,  ..., -0.4226,  0.4337,  0.6906]],\n\n         [[-1.9482, -1.9482, -1.9657,  ..., -1.0903, -1.0903, -1.1078],\n          [-1.9482, -1.9482, -1.9482,  ..., -0.7227, -0.6877, -0.7052],\n          [-1.9657, -1.9482, -1.9482,  ..., -0.5476, -0.5826, -0.5651],\n          ...,\n          [-0.9503, -1.0728, -0.1975,  ..., -0.1625,  0.0826, -0.0924],\n          [-0.8803, -0.5476,  0.0476,  ..., -0.1625,  0.5028,  0.4678],\n          [-0.2150, -0.1975, -0.2850,  ..., -0.0399,  0.7654,  0.9755]],\n\n         [[-1.7347, -1.7347, -1.7522,  ..., -0.8807, -0.8807, -0.8981],\n          [-1.7347, -1.7347, -1.7347,  ..., -0.5321, -0.4973, -0.5147],\n          [-1.7522, -1.7347, -1.7347,  ..., -0.3753, -0.3927, -0.3753],\n          ...,\n          [-1.4210, -1.4907, -0.8110,  ..., -1.0550, -0.7761, -0.9330],\n          [-1.3861, -1.2467, -0.8110,  ..., -1.0898, -0.3927, -0.4275],\n          [-1.0376, -1.1247, -1.3687,  ..., -0.9853, -0.1835,  0.0256]]],\n\n\n        [[[-1.7412, -1.8268, -1.7754,  ..., -1.5870, -1.2788, -1.4329],\n          [-1.6555, -1.6213, -1.7583,  ..., -1.3815, -1.4158, -1.7240],\n          [-1.7583, -1.7583, -1.3987,  ..., -1.6042, -1.8782, -1.9124],\n          ...,\n          [ 0.2624,  1.4440,  1.3584,  ...,  0.3823,  0.8276,  1.0502],\n          [ 0.4851,  1.4783,  0.3823,  ...,  1.2557,  0.9988,  0.7419],\n          [-0.0801, -0.0116, -0.1828,  ...,  0.9988,  0.8276,  0.8447]],\n\n         [[-1.5280, -1.6155, -1.5455,  ..., -1.4930, -1.1604, -1.3179],\n          [-1.4755, -1.3704, -1.5105,  ..., -1.2654, -1.3004, -1.5980],\n          [-1.5980, -1.5455, -1.1078,  ..., -1.4755, -1.7731, -1.8081],\n          ...,\n          [ 0.3978,  1.6057,  1.5182,  ...,  0.4853,  0.9230,  1.1155],\n          [ 0.6254,  1.6408,  0.5203,  ...,  1.3782,  1.1155,  0.8354],\n          [ 0.0476,  0.1176, -0.0749,  ...,  1.1331,  0.9405,  0.9230]],\n\n         [[-1.7173, -1.6824, -1.6127,  ..., -1.3164, -1.0550, -1.2293],\n          [-1.5430, -1.5779, -1.6650,  ..., -1.1073, -1.2119, -1.5256],\n          [-1.5953, -1.6476, -1.4733,  ..., -1.3513, -1.6650, -1.7347],\n          ...,\n          [ 0.4439,  1.6640,  1.5942,  ...,  0.4265,  0.8099,  0.9842],\n          [ 0.6531,  1.6814,  0.6008,  ...,  1.2631,  0.9668,  0.6356],\n          [ 0.0605,  0.1651,  0.0256,  ...,  0.9842,  0.7576,  0.7054]]],\n\n\n        [[[-0.9363, -0.7479, -1.0390,  ..., -2.1008, -2.1008, -2.0665],\n          [-1.3302, -0.9363, -0.7822,  ..., -2.1008, -2.1008, -2.0665],\n          [-1.5014, -1.2617, -0.9705,  ..., -2.1008, -2.1008, -2.1008],\n          ...,\n          [ 1.8550,  1.8379,  1.7523,  ...,  1.2899,  1.2899,  0.8789],\n          [ 1.8208,  1.7523,  1.6838,  ...,  1.1015,  1.3927,  0.9474],\n          [ 1.7009,  1.6153,  1.6324,  ...,  1.1187,  1.4783,  1.1187]],\n\n         [[-0.7577, -0.5651, -0.8627,  ..., -1.9832, -1.9832, -1.9482],\n          [-1.1604, -0.7577, -0.6001,  ..., -1.9832, -1.9657, -1.9307],\n          [-1.3354, -1.0903, -0.7927,  ..., -1.9832, -1.9832, -1.9657],\n          ...,\n          [ 1.1681,  1.1506,  1.0630,  ...,  1.5007,  1.4482,  0.9755],\n          [ 1.1331,  1.0455,  0.9930,  ...,  1.2906,  1.5357,  1.0455],\n          [ 1.0280,  0.9580,  0.9755,  ...,  1.3256,  1.6408,  1.2206]],\n\n         [[-1.0376, -0.8110, -1.0724,  ..., -1.5779, -1.5779, -1.5604],\n          [-1.4036, -0.9678, -0.7761,  ..., -1.5779, -1.5604, -1.5604],\n          [-1.5779, -1.2816, -0.9678,  ..., -1.5779, -1.5604, -1.5953],\n          ...,\n          [ 0.8622,  0.8448,  0.7576,  ...,  1.5768,  1.4897,  0.9842],\n          [ 0.8274,  0.7576,  0.7228,  ...,  1.3851,  1.5768,  1.0539],\n          [ 0.7402,  0.6705,  0.7054,  ...,  1.4025,  1.6814,  1.2282]]],\n\n\n        ...,\n\n\n        [[[-1.2103, -1.1760, -1.1075,  ..., -0.7822, -0.9877, -1.0904],\n          [-0.9192, -0.9705, -1.0219,  ..., -0.7993, -1.1247, -1.0219],\n          [-0.5424, -0.8678, -1.0733,  ..., -1.0219, -1.2103, -0.9192],\n          ...,\n          [ 1.2385,  0.7591,  0.2624,  ...,  1.2214,  0.9132,  0.8618],\n          [ 1.2385,  0.9474,  1.0502,  ...,  0.9646, -0.0801,  0.1083],\n          [ 1.1187,  1.1872,  0.9474,  ...,  0.6906,  0.2967,  0.3652]],\n\n         [[-1.0728, -1.0378, -0.9678,  ..., -0.6001, -0.8102, -0.9153],\n          [-0.7752, -0.8277, -0.8803,  ..., -0.6352, -0.9503, -0.8452],\n          [-0.3901, -0.7227, -0.9328,  ..., -0.8627, -1.0553, -0.7752],\n          ...,\n          [ 1.0980,  0.6429,  0.1527,  ...,  1.3081,  0.9755,  0.9405],\n          [ 1.0980,  0.8704,  0.9755,  ...,  1.0280, -0.0574,  0.1352],\n          [ 0.9930,  1.1155,  0.8704,  ...,  0.7479,  0.3102,  0.3803]],\n\n         [[-1.2641, -1.2293, -1.1247,  ..., -0.9504, -1.1596, -1.2293],\n          [-0.9330, -1.0027, -1.0376,  ..., -0.9678, -1.2641, -1.1421],\n          [-0.5321, -0.8633, -1.0898,  ..., -1.1421, -1.2990, -0.9853],\n          ...,\n          [ 0.9319,  0.5659,  0.1651,  ...,  1.1934,  0.9145,  0.8797],\n          [ 1.0539,  0.7751,  0.8797,  ...,  0.8971, -0.1312,  0.0605],\n          [ 1.0365,  1.0017,  0.6356,  ...,  0.5834,  0.1999,  0.2871]]],\n\n\n        [[[-1.4843, -1.3473, -1.4329,  ..., -0.9020, -0.8678, -0.8507],\n          [-1.6898, -1.6555, -1.4843,  ..., -0.8507, -0.8507, -0.8507],\n          [-1.4500, -1.6898, -1.3987,  ..., -0.8507, -0.8678, -0.8849],\n          ...,\n          [-0.8849, -0.7308, -0.4911,  ...,  1.8208,  1.8722,  1.8722],\n          [-1.2274, -1.0219, -0.6109,  ...,  1.8550,  1.9064,  1.9064],\n          [-1.7069, -1.4843, -1.1418,  ...,  1.8379,  1.9235,  1.9578]],\n\n         [[-1.2829, -1.1779, -1.3004,  ...,  0.2752,  0.2577,  0.2402],\n          [-1.4755, -1.4930, -1.3529,  ...,  0.2577,  0.2752,  0.2752],\n          [-1.2829, -1.5280, -1.2654,  ...,  0.2927,  0.2927,  0.2927],\n          ...,\n          [-0.7752, -0.6176, -0.3550,  ...,  1.1681,  1.2381,  1.2556],\n          [-1.1429, -0.9328, -0.4951,  ...,  1.2031,  1.2906,  1.3081],\n          [-1.6331, -1.4055, -1.0378,  ...,  1.2031,  1.3081,  1.3606]],\n\n         [[-1.3164, -1.1944, -1.2641,  ...,  1.7511,  1.7511,  1.7685],\n          [-1.5256, -1.5256, -1.3513,  ...,  1.7685,  1.7337,  1.6814],\n          [-1.2990, -1.5430, -1.2641,  ...,  1.7337,  1.7511,  1.7511],\n          ...,\n          [-0.6890, -0.5321, -0.2881,  ...,  0.8971,  0.9668,  0.9668],\n          [-1.0027, -0.8110, -0.4101,  ...,  0.9319,  1.0017,  1.0191],\n          [-1.4733, -1.2816, -0.9504,  ...,  0.9145,  1.0191,  1.0714]]],\n\n\n        [[[-1.6042, -1.6213, -1.5870,  ..., -0.1486, -0.1314,  0.0056],\n          [-1.5699, -1.5528, -1.5699,  ..., -0.1314, -0.1143,  0.0569],\n          [-1.5870, -1.5185, -1.4843,  ..., -0.1143, -0.0629,  0.1597],\n          ...,\n          [ 0.9132,  1.1187,  1.3413,  ..., -0.7822, -0.7822, -0.7650],\n          [ 1.4440,  1.0844,  1.3242,  ..., -0.7993, -0.7650, -0.7479],\n          [ 1.3755,  0.8961,  1.3927,  ..., -0.8335, -0.7993, -0.7993]],\n\n         [[-1.5980, -1.6506, -1.6506,  ..., -0.0224, -0.0049,  0.1176],\n          [-1.5980, -1.6155, -1.6331,  ..., -0.0224, -0.0049,  0.1527],\n          [-1.6506, -1.5980, -1.5805,  ..., -0.0399,  0.0126,  0.2402],\n          ...,\n          [ 0.4853,  0.7129,  0.9580,  ..., -0.7577, -0.7402, -0.7227],\n          [ 1.0280,  0.6604,  0.9230,  ..., -0.7752, -0.7402, -0.7052],\n          [ 0.9405,  0.4503,  1.0105,  ..., -0.8102, -0.7752, -0.7577]],\n\n         [[-1.5256, -1.5604, -1.5430,  ...,  0.0779,  0.1128,  0.2522],\n          [-1.4733, -1.4733, -1.4907,  ...,  0.0779,  0.1302,  0.2871],\n          [-1.4733, -1.4210, -1.3861,  ...,  0.0779,  0.1476,  0.3742],\n          ...,\n          [ 0.1476,  0.3393,  0.5834,  ..., -0.6541, -0.6715, -0.6541],\n          [ 0.7054,  0.3393,  0.6008,  ..., -0.6715, -0.6367, -0.6367],\n          [ 0.6705,  0.1651,  0.7228,  ..., -0.6890, -0.6541, -0.6890]]]]), 'pixel_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         ...,\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1]],\n\n        [[1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         ...,\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1]],\n\n        [[1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         ...,\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1]],\n\n        ...,\n\n        [[1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         ...,\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1]],\n\n        [[1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         ...,\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1]],\n\n        [[1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         ...,\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1]]]), 'labels': [{'size': tensor([640, 480]), 'image_id': tensor([69]), 'class_labels': tensor([5, 0, 1, 4, 4, 4, 4, 4]), 'boxes': tensor([[0.4675, 0.5152, 0.1846, 0.2045],\n        [0.5092, 0.5843, 0.3970, 0.3951],\n        [0.2719, 0.5861, 0.3738, 0.2471],\n        [0.1023, 0.6896, 0.2019, 0.1655],\n        [0.3902, 0.0924, 0.1530, 0.0898],\n        [0.5345, 0.0871, 0.0252, 0.0556],\n        [0.6370, 0.0877, 0.1357, 0.0899],\n        [0.9383, 0.0634, 0.0789, 0.0627]]), 'area': tensor([11597.7402, 48180.5664, 28372.1094, 10266.5547,  4223.3750,   430.7600,\n         3749.3826,  1517.7850]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([1027]), 'class_labels': tensor([5, 4, 1, 0, 0]), 'boxes': tensor([[0.4669, 0.5782, 0.1456, 0.1290],\n        [0.5031, 0.6013, 0.0410, 0.0237],\n        [0.5269, 0.6380, 0.1138, 0.1280],\n        [0.3863, 0.5047, 0.4801, 0.3840],\n        [0.1074, 0.4195, 0.2101, 0.3353]]), 'area': tensor([ 5770.2451,   298.4550,  4471.7402, 56633.0859, 21642.4102]), 'iscrowd': tensor([0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([1092]), 'class_labels': tensor([2, 5, 1, 0]), 'boxes': tensor([[0.1943, 0.1126, 0.1849, 0.0794],\n        [0.5387, 0.5818, 0.3646, 0.2689],\n        [0.3515, 0.7725, 0.3171, 0.2903],\n        [0.5404, 0.4307, 0.6236, 0.4566]]), 'area': tensor([ 4508.5000, 30117.5000, 28278.7598, 87485.0391]), 'iscrowd': tensor([0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([228]), 'class_labels': tensor([0]), 'boxes': tensor([[0.5187, 0.5418, 0.4982, 0.5698]]), 'area': tensor([87218.0078]), 'iscrowd': tensor([0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([511]), 'class_labels': tensor([5, 1]), 'boxes': tensor([[0.5284, 0.5886, 0.2903, 0.3347],\n        [0.7784, 0.7873, 0.4400, 0.4222]]), 'area': tensor([29848.7695, 57066.2383]), 'iscrowd': tensor([0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([338]), 'class_labels': tensor([5, 0, 1]), 'boxes': tensor([[0.4990, 0.5424, 0.2227, 0.1716],\n        [0.5455, 0.5335, 0.3754, 0.3595],\n        [0.7111, 0.6979, 0.3313, 0.2838]]), 'area': tensor([11742.9648, 41455.0117, 28882.3496]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([405]), 'class_labels': tensor([0, 1, 5]), 'boxes': tensor([[0.4952, 0.6559, 0.6088, 0.4872],\n        [0.2074, 0.7760, 0.4117, 0.4459],\n        [0.4132, 0.5714, 0.0663, 0.0580]]), 'area': tensor([91107.9609, 56385.1602,  1179.7800]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([3]), 'class_labels': tensor([0, 5, 1, 4, 4, 4]), 'boxes': tensor([[0.5020, 0.4466, 0.6579, 0.5829],\n        [0.5148, 0.5684, 0.2288, 0.1367],\n        [0.7040, 0.7836, 0.4468, 0.4219],\n        [0.3160, 0.8416, 0.3991, 0.2993],\n        [0.4095, 0.0661, 0.0888, 0.0666],\n        [0.7489, 0.1356, 0.3843, 0.2637]]), 'area': tensor([117809.1875,   9607.5000,  57901.5000,  36691.4023,   1814.7600,\n         31125.9375]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([182]), 'class_labels': tensor([0, 1, 5]), 'boxes': tensor([[0.5786, 0.5016, 0.5992, 0.4539],\n        [0.6307, 0.7197, 0.4165, 0.3323],\n        [0.4415, 0.6429, 0.1546, 0.2070]]), 'area': tensor([83547.7969, 42508.7344,  9827.7900]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([640]), 'class_labels': tensor([5, 1, 0]), 'boxes': tensor([[0.5314, 0.6391, 0.2920, 0.4553],\n        [0.7088, 0.7733, 0.5596, 0.4422],\n        [0.5282, 0.5060, 0.5678, 0.4612]]), 'area': tensor([40839.7109, 76013.7969, 80443.1328]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}]}\n\n\n\n# Images are reshaped to be the IMAGE_SIZE value that we set\nprocessed_dataset[\"train\"][0][\"pixel_values\"].shape\n\ntorch.Size([3, 640, 480])\n\n\n\n\n7.3 TK - Create a collation function\nNotes: * The input to the data_collator function will be the output of image_processor, see below for format. * The output of the data_collator will be passed to our model‚Äôs forward() method. * data_collator for transformers.Trainer - https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.data_collator * ‚ÄúThe function to use to form a batch from a list of elements of train_dataset.\nInput to data_collator is the output of image_processor:\n{'pixel_values': tensor([[[ 2.2318,  2.2318,  2.2318,  ...,  0.3309,  0.2282,  0.1254],\n          [ 2.2318,  2.2318,  2.2318,  ...,  0.3138,  0.2111,  0.1426],\n          [ 2.2318,  2.2318,  2.2489,  ...,  0.2967,  0.2111,  0.1426],\n          ...,\n          [-0.8164, -0.8164, -0.7993,  ...,  0.5878,  0.5707,  0.5878],\n          [-0.9363, -0.8849, -0.8164,  ...,  0.5193,  0.5364,  0.5707],\n          [-0.9877, -0.9363, -0.9192,  ...,  0.5707,  0.5707,  0.5878]],\n \n         [[ 2.4286,  2.4286,  2.4286,  ...,  0.4853,  0.4153,  0.3277],\n          [ 2.4286,  2.4286,  2.4286,  ...,  0.4853,  0.3978,  0.3277],\n          [ 2.4286,  2.4286,  2.4286,  ...,  0.4678,  0.3803,  0.3102],\n          ...,\n          [-1.1253, -1.1253, -1.1078,  ...,  0.2052,  0.1877,  0.2052],\n          [-1.2129, -1.1604, -1.1253,  ...,  0.1352,  0.1527,  0.1877],\n          [-1.2479, -1.2129, -1.2304,  ...,  0.1877,  0.1877,  0.2052]],\n \n         [[ 2.6051,  2.6051,  2.6051,  ...,  0.6531,  0.6008,  0.5311],\n          [ 2.6051,  2.6051,  2.6051,  ...,  0.6531,  0.5659,  0.5136],\n          [ 2.6051,  2.6051,  2.6051,  ...,  0.6356,  0.5485,  0.4788],\n          ...,\n          [-1.3861, -1.3687, -1.3339,  ..., -0.2358, -0.2532, -0.2358],\n          [-1.4907, -1.4210, -1.3513,  ..., -0.3055, -0.2881, -0.2532],\n          [-1.5256, -1.4733, -1.4559,  ..., -0.2532, -0.2532, -0.2358]]]),\n 'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n...\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1]]),\n 'labels': {'size': tensor([1066,  800]), 'image_id': tensor([0]), 'class_labels': tensor([1, 0]), 'boxes': tensor([[0.7553, 0.5571, 0.4196, 0.2626],\n         [0.5022, 0.5583, 0.9827, 0.8609]]), 'area': tensor([ 93955.8828, 721446.3750]), 'iscrowd': tensor([0, 0]), 'orig_size': tensor([1280,  960])}}\nThe data_collator function will turn collections of these into batches (e.g.¬†stack together the pixel_values, pixel_mask, labels etc).\n\n# Create data_collate_function to collect samples into batches\n# TK - want to get a dictionary of {\"pixel_mask\": [batch_of_samples], \"labels\": [batch_of_samples], \"pixel_mask\": [batch_of_samples]}\ndef data_collate_function(batch):\n    collated_data = {} \n\n    # Stack together a collection of pixel_values tensors\n    collated_data[\"pixel_values\"] = torch.stack([sample[\"pixel_values\"] for sample in batch])\n\n    # Get the labels (these are dictionaries so no need to use torch.stack)\n    collated_data[\"labels\"] = [sample[\"labels\"] for sample in batch]\n\n    # If there is a pixel_mask key, return the pixel_mask's as well\n    if \"pixel_mask\" in batch[0]:\n        collated_data[\"pixel_mask\"] = torch.stack([sample[\"pixel_mask\"] for sample in batch])\n\n    return collated_data\n\n\n%%time\n\n# Try data_collate_function \nexample_collated_data_batch = data_collate_function(processed_dataset[\"train\"].select(range(32)))\nexample_collated_data_batch[\"pixel_values\"].shape\n\nCPU times: user 2.01 s, sys: 131 ms, total: 2.14 s\nWall time: 1.45 s\n\n\ntorch.Size([32, 3, 640, 480])\n\n\n\nexample_collated_data_batch.keys()\n\ndict_keys(['pixel_values', 'labels', 'pixel_mask'])\n\n\n\n# 32 samples (because that's our batch size)\nlen(example_collated_data_batch[\"pixel_values\"]), len(example_collated_data_batch[\"labels\"]), len(example_collated_data_batch[\"pixel_mask\"])\n\n(32, 32, 32)\n\n\nTK - We get a batch of 32 samples with size 640, 480, these are all preprocessed as well and will be fed to our model.\n\n%%time \n\n# Try pass a batch through our model (note: this will be slow if our model is on the CPU)\nexample_batch_outputs = model(example_collated_data_batch[\"pixel_values\"])\nexample_batch_outputs\n\nCPU times: user 1min 4s, sys: 12.5 s, total: 1min 17s\nWall time: 5.21 s\n\n\nConditionalDetrObjectDetectionOutput(loss=None, loss_dict=None, logits=tensor([[[ 0.1756,  0.0112, -0.1084,  ...,  0.1422,  0.0683,  0.1605],\n         [-0.2120, -0.2104, -0.1722,  ...,  0.3864, -0.1778,  0.2019],\n         [ 0.1066,  0.1096,  0.2123,  ...,  0.1867, -0.0547,  0.2594],\n         ...,\n         [-0.3185,  0.3699, -0.2245,  ...,  0.1371,  0.2279,  0.2639],\n         [ 0.0702,  0.0533,  0.1279,  ...,  0.2358, -0.1269,  0.2406],\n         [-0.1309, -0.3195,  0.1867,  ...,  0.4492, -0.0839,  0.4281]],\n\n        [[ 0.1036,  0.0428, -0.2660,  ...,  0.0152,  0.0188,  0.0505],\n         [-0.1730, -0.3609, -0.0393,  ...,  0.2778, -0.2219,  0.1670],\n         [ 0.0929,  0.2278,  0.2457,  ...,  0.0409, -0.1385,  0.1913],\n         ...,\n         [-0.0265,  0.0631,  0.0627,  ...,  0.0372, -0.1568,  0.0072],\n         [ 0.0708,  0.1320,  0.1984,  ...,  0.1450, -0.0370,  0.1971],\n         [-0.2185, -0.3554,  0.0250,  ...,  0.1523, -0.1766, -0.2412]],\n\n        [[-0.0034, -0.1252, -0.4586,  ...,  0.0920, -0.0194,  0.0565],\n         [-0.1779, -0.3050, -0.0245,  ...,  0.1755, -0.2620,  0.3097],\n         [-0.0193,  0.0550, -0.0951,  ..., -0.0771,  0.0046,  0.0384],\n         ...,\n         [-0.2811, -0.0509, -0.0340,  ...,  0.4088, -0.0885,  0.1977],\n         [-0.1411, -0.2114, -0.0364,  ...,  0.1844, -0.2052, -0.1303],\n         [-0.0397, -0.3287,  0.0959,  ...,  0.3857, -0.2455,  0.3551]],\n\n        ...,\n\n        [[-0.2905, -0.1199, -0.5113,  ...,  0.0797,  0.0761, -0.1454],\n         [-0.3391, -0.4398,  0.1613,  ...,  0.3521, -0.2897,  0.4688],\n         [ 0.0515,  0.1871,  0.2654,  ...,  0.0055,  0.0177, -0.2444],\n         ...,\n         [-0.5897,  0.2452, -0.1715,  ...,  0.1403,  0.2739,  0.2423],\n         [ 0.0082,  0.3222,  0.1669,  ...,  0.0938,  0.1326, -0.1318],\n         [-0.1835, -0.0591,  0.1662,  ...,  0.1506, -0.1369, -0.0960]],\n\n        [[ 0.0088,  0.0562, -0.1568,  ...,  0.0956,  0.1420, -0.0164],\n         [-0.1252, -0.3315, -0.0670,  ...,  0.3029, -0.3670,  0.2253],\n         [ 0.1418,  0.0832,  0.1878,  ...,  0.2082, -0.2881,  0.0064],\n         ...,\n         [-0.3357,  0.0241, -0.2351,  ...,  0.1009,  0.2384,  0.1972],\n         [ 0.1632,  0.0212,  0.1528,  ...,  0.2441, -0.2813, -0.1012],\n         [-0.2424, -0.3850,  0.1242,  ...,  0.2214, -0.4294, -0.2708]],\n\n        [[ 0.0734, -0.0391, -0.4524,  ...,  0.0742, -0.0376, -0.1117],\n         [ 0.0506, -0.0210,  0.0115,  ...,  0.0043, -0.1665, -0.0796],\n         [ 0.0133, -0.2106, -0.0142,  ...,  0.5130, -0.2083,  0.1878],\n         ...,\n         [-0.2460, -0.1284, -0.1073,  ...,  0.2888, -0.2080,  0.0897],\n         [-0.1026, -0.2328, -0.1268,  ...,  0.4177, -0.3034,  0.1005],\n         [-0.2828, -0.4220,  0.1543,  ...,  0.3707, -0.5253, -0.1016]]],\n       grad_fn=&lt;ViewBackward0&gt;), pred_boxes=tensor([[[0.9500, 0.6381, 0.1323, 0.6838],\n         [0.6333, 0.0871, 0.1233, 0.0674],\n         [0.9906, 0.3960, 0.0203, 0.1109],\n         ...,\n         [0.3539, 0.4133, 0.7001, 0.7790],\n         [0.9606, 0.3789, 0.0489, 0.0365],\n         [0.0161, 0.1030, 0.0344, 0.0579]],\n\n        [[0.7669, 0.9339, 0.5311, 0.1394],\n         [0.6464, 0.0556, 0.1167, 0.1030],\n         [0.9931, 0.5555, 0.0139, 0.1452],\n         ...,\n         [0.3557, 0.4036, 0.2504, 0.1227],\n         [0.9974, 0.1280, 0.0060, 0.2545],\n         [0.0663, 0.3102, 0.1351, 0.1014]],\n\n        [[0.7914, 0.7499, 0.3953, 0.4964],\n         [0.6263, 0.0586, 0.2221, 0.0999],\n         [0.8788, 0.5819, 0.2370, 0.4087],\n         ...,\n         [0.5177, 0.3094, 0.5866, 0.2528],\n         [0.8649, 0.4862, 0.2535, 0.2284],\n         [0.0075, 0.1096, 0.0162, 0.0404]],\n\n        ...,\n\n        [[0.6732, 0.8154, 0.6100, 0.3546],\n         [0.6279, 0.0335, 0.0599, 0.0650],\n         [0.9707, 0.6607, 0.0617, 0.3039],\n         ...,\n         [0.4202, 0.3927, 0.8342, 0.4869],\n         [0.9947, 0.7069, 0.0112, 0.4913],\n         [0.0305, 0.3756, 0.0607, 0.2126]],\n\n        [[0.8158, 0.7939, 0.3493, 0.3947],\n         [0.6333, 0.0566, 0.1713, 0.1139],\n         [0.9268, 0.5027, 0.1466, 0.1161],\n         ...,\n         [0.3697, 0.3443, 0.7456, 0.7034],\n         [0.8904, 0.4677, 0.2055, 0.1397],\n         [0.0306, 0.3043, 0.0608, 0.0623]],\n\n        [[0.7667, 0.7876, 0.4440, 0.4146],\n         [0.6509, 0.2361, 0.0934, 0.0602],\n         [0.9449, 0.3689, 0.0946, 0.0297],\n         ...,\n         [0.4404, 0.3215, 0.4841, 0.1150],\n         [0.8951, 0.3655, 0.1890, 0.0374],\n         [0.0411, 0.2678, 0.0848, 0.0570]]], grad_fn=&lt;SigmoidBackward0&gt;), auxiliary_outputs=None, last_hidden_state=tensor([[[-1.4020e-01, -1.5893e-01,  4.4403e-01,  ...,  1.5252e-01,\n           2.8576e-01,  2.6249e-01],\n         [ 6.8369e-02, -2.7463e-01, -4.5402e-01,  ..., -9.0982e-01,\n          -4.7036e-01,  7.2642e-01],\n         [-1.7512e-01,  3.1511e-01,  2.2512e-01,  ..., -1.4200e-01,\n           2.5577e-01,  4.1778e-01],\n         ...,\n         [ 1.5953e-01, -4.0302e-01,  2.2796e-01,  ..., -8.1157e-01,\n          -3.6345e-01, -8.9928e-02],\n         [ 4.0930e-02,  6.6010e-04,  1.2503e-01,  ..., -5.6554e-02,\n           3.2782e-01,  3.9761e-01],\n         [-1.5904e-02,  5.8626e-01, -1.3788e-01,  ..., -7.4208e-01,\n          -1.3682e-01,  1.0417e-01]],\n\n        [[ 1.8487e-01, -2.6388e-01,  7.6519e-01,  ..., -4.4617e-01,\n           1.6003e-01,  5.6029e-01],\n         [ 5.1641e-01, -5.4275e-02,  1.0399e+00,  ..., -8.5620e-01,\n          -2.2614e-01, -2.9099e-01],\n         [-2.0582e-01,  2.9136e-01,  2.8441e-01,  ..., -4.6227e-02,\n           2.9668e-01,  7.5241e-01],\n         ...,\n         [ 3.8438e-01,  6.9957e-01, -5.8716e-01,  ..., -9.2270e-01,\n          -4.5221e-02, -1.3225e-01],\n         [-6.4926e-02,  1.9942e-01,  4.3592e-01,  ...,  3.0664e-02,\n           5.1831e-01,  3.6161e-01],\n         [ 4.8070e-01, -5.2024e-01,  2.0143e-01,  ..., -1.5431e+00,\n          -3.6578e-01, -2.4390e-01]],\n\n        [[ 2.7832e-01,  7.0842e-02,  1.2050e+00,  ..., -7.3184e-01,\n           1.7189e-01,  3.8562e-02],\n         [ 7.5524e-01,  1.0498e-01,  5.4896e-01,  ..., -4.7316e-01,\n           7.8752e-03,  2.6307e-01],\n         [-3.7225e-01,  5.2872e-02,  5.6387e-01,  ..., -1.3147e+00,\n           2.3460e-01,  4.7530e-01],\n         ...,\n         [ 3.1748e-01, -1.0066e+00,  3.5116e-01,  ..., -8.5966e-01,\n          -1.8258e-01,  2.6463e-01],\n         [-3.0530e-02, -1.0162e+00,  4.3357e-01,  ..., -1.1250e+00,\n          -1.9363e-01, -7.9971e-02],\n         [ 3.0213e-01,  1.3661e-01, -6.4669e-01,  ..., -5.1888e-01,\n          -6.2747e-02,  6.2570e-01]],\n\n        ...,\n\n        [[ 3.9975e-01, -9.5206e-01,  8.8087e-01,  ..., -8.3797e-01,\n          -3.4231e-02,  1.5127e-02],\n         [ 3.7874e-01,  4.6002e-01,  5.5632e-01,  ..., -8.4079e-01,\n           3.5074e-01, -1.0479e-01],\n         [-2.1702e-01, -6.3238e-01,  3.0843e-01,  ..., -4.9595e-01,\n           3.9976e-01,  7.5963e-01],\n         ...,\n         [-9.0952e-02, -1.8212e+00, -7.9186e-02,  ..., -1.0548e+00,\n          -7.6392e-02,  3.0424e-01],\n         [-5.6228e-02, -5.4257e-01,  3.7607e-01,  ..., -1.8365e-01,\n           7.9351e-01,  1.0800e+00],\n         [ 8.0718e-02, -3.2467e-01,  3.0199e-02,  ..., -1.0819e+00,\n           1.6267e-01,  4.1212e-01]],\n\n        [[ 4.9446e-01, -3.8678e-01,  9.7415e-01,  ..., -9.0278e-01,\n           9.9647e-03,  4.2870e-02],\n         [ 7.2289e-01,  2.6472e-01,  6.9674e-01,  ..., -8.4964e-01,\n          -3.5554e-01, -4.0242e-01],\n         [ 2.0905e-01,  1.7493e-01,  7.1425e-01,  ..., -6.0879e-01,\n          -2.6598e-01,  5.8427e-01],\n         ...,\n         [ 3.1929e-01, -1.3318e+00,  1.0949e+00,  ..., -1.0937e+00,\n          -4.9580e-01, -4.8511e-01],\n         [ 2.8816e-01,  1.6738e-04,  1.1606e+00,  ..., -7.3686e-01,\n          -2.4679e-01,  1.9954e-01],\n         [ 1.8261e-01, -1.2720e-02, -3.0613e-01,  ..., -6.9232e-01,\n          -2.6717e-01,  1.7242e-01]],\n\n        [[ 1.1384e-01,  1.4387e-01,  3.6687e-02,  ..., -7.7477e-01,\n           1.0376e-01, -2.5709e-01],\n         [ 3.4558e-01, -4.4018e-01,  3.6415e-01,  ...,  1.7454e-01,\n           2.4093e-01, -4.9051e-02],\n         [ 1.7516e-01, -2.2057e-01, -1.2419e-01,  ..., -1.5287e-01,\n           6.2450e-02,  4.9240e-02],\n         ...,\n         [ 6.6910e-01, -3.4297e-01, -2.0511e-01,  ..., -1.0155e+00,\n           7.9812e-03,  3.0636e-01],\n         [ 4.0032e-01, -3.4343e-01,  1.5294e-01,  ..., -3.3256e-01,\n          -2.5672e-01, -1.9711e-01],\n         [-1.1014e-01, -4.8125e-01,  1.0338e-01,  ..., -7.0084e-01,\n           4.9208e-03,  2.7278e-01]]], grad_fn=&lt;NativeLayerNormBackward0&gt;), decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[-0.3462,  0.1944, -0.1375,  ..., -0.4447,  0.4016,  0.4290],\n         [ 0.0648,  0.2144,  0.0340,  ...,  0.2365,  0.1294,  0.3575],\n         [ 0.1515,  0.5005, -0.0685,  ..., -0.0157,  0.1598,  0.3866],\n         ...,\n         [ 0.1488,  0.8020, -0.2199,  ...,  0.2656,  0.0879,  0.2309],\n         [ 0.1548,  0.6870, -0.1847,  ...,  0.3029,  0.0465,  0.2264],\n         [-0.1375,  0.4506, -0.2336,  ..., -0.0616,  0.1774,  0.2659]],\n\n        [[-0.3369,  0.3608, -0.2942,  ..., -0.4818,  0.4762,  0.3779],\n         [ 0.0714,  0.3084,  0.0148,  ...,  0.0797,  0.2380,  0.3244],\n         [ 0.0873,  0.4330, -0.0352,  ..., -0.2179,  0.2011,  0.2788],\n         ...,\n         [-0.0706,  0.0146,  0.1921,  ..., -0.1177, -0.1456,  0.0187],\n         [ 0.1120,  0.2591,  0.0263,  ...,  0.1479, -0.0880,  0.0873],\n         [-0.1774,  0.3163, -0.0410,  ...,  0.0425,  0.1321,  0.2753]],\n\n        [[-0.2815,  0.3443, -0.2270,  ..., -0.5475,  0.2527,  0.3086],\n         [ 0.1719,  0.4588, -0.0811,  ...,  0.0694,  0.0811,  0.3715],\n         [ 0.2389,  0.2392, -0.1076,  ..., -0.1341, -0.2286,  0.2902],\n         ...,\n         [ 0.2274,  0.4766,  0.0128,  ...,  0.2001,  0.2571,  0.2773],\n         [ 0.2339,  0.5257,  0.0034,  ...,  0.2795,  0.2356,  0.2127],\n         [-0.0985,  0.3517, -0.0659,  ..., -0.0961,  0.3029,  0.1836]],\n\n        ...,\n\n        [[-0.3820,  0.4122, -0.4279,  ..., -0.4390,  0.4537,  0.3619],\n         [ 0.0776,  0.4093, -0.1319,  ...,  0.3167,  0.1865,  0.4449],\n         [ 0.0644,  0.5139, -0.1786,  ...,  0.1034,  0.1915,  0.3504],\n         ...,\n         [-0.0715,  0.1232,  0.0057,  ...,  0.2714,  0.0190,  0.1771],\n         [ 0.1267,  0.3740,  0.0213,  ..., -0.0367,  0.0245,  0.2749],\n         [-0.1652,  0.1528,  0.1033,  ..., -0.1985,  0.0891,  0.3079]],\n\n        [[-0.2655,  0.2723, -0.2191,  ..., -0.3646,  0.3872,  0.2680],\n         [ 0.1672,  0.2333, -0.0337,  ...,  0.2537,  0.2663,  0.3487],\n         [ 0.1631,  0.3007, -0.1148,  ...,  0.1061,  0.1698,  0.2983],\n         ...,\n         [ 0.1221,  0.1708,  0.0071,  ...,  0.4499, -0.0821,  0.0854],\n         [ 0.1202,  0.0732, -0.0148,  ...,  0.6552, -0.2320,  0.0461],\n         [-0.0094,  0.2407,  0.1013,  ..., -0.1772, -0.1296,  0.0011]],\n\n        [[-0.3589,  0.4908, -0.3906,  ..., -0.5620,  0.4539,  0.2588],\n         [ 0.1310,  0.5131, -0.0584,  ...,  0.1296,  0.1215,  0.2423],\n         [ 0.1021,  0.6150, -0.0859,  ..., -0.0818,  0.1724,  0.2820],\n         ...,\n         [ 0.2026,  0.4986,  0.1082,  ...,  0.1570,  0.1229,  0.1716],\n         [ 0.1716,  0.3375,  0.1374,  ...,  0.4551,  0.0419,  0.0987],\n         [-0.0736,  0.2892,  0.0910,  ..., -0.2655,  0.1247,  0.0657]]],\n       grad_fn=&lt;NativeLayerNormBackward0&gt;), encoder_hidden_states=None, encoder_attentions=None)\n\n\n\nexample_batch_outputs.keys()\n\nodict_keys(['logits', 'pred_boxes', 'last_hidden_state', 'encoder_last_hidden_state'])\n\n\n\n# We get 300 predictions per image in our batch, each with a logit value for each of the classes in our dataset \nexample_batch_outputs.logits.shape\n\ntorch.Size([32, 300, 7])\n\n\nThis is what will happen during training, our model will continually go over batches over data and try to match its own predictions with the ground truth labels.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#tk---setup-trainingarguments-trainer",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#tk---setup-trainingarguments-trainer",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "8 TK - Setup TrainingArguments + Trainer",
    "text": "8 TK - Setup TrainingArguments + Trainer\nUPTOHERE - creating TrainingArguments + Trainer + Training a model\n\nTK - for hyperparameters, see example in RT-DETR paper: https://arxiv.org/pdf/2304.08069\nAs well as DETR - https://arxiv.org/pdf/2005.12872 (see Appendix A.4)\nTry training for 25 epochs and see what happens\n\n\nprocessed_dataset[\"validation\"][0]\n\n{'pixel_values': tensor([[[ 0.1254,  0.1254,  0.1597,  ..., -2.0837, -1.9809, -1.9295],\n          [ 0.1426,  0.1254,  0.1597,  ..., -2.0494, -1.9638, -1.9467],\n          [ 0.1426,  0.1426,  0.1597,  ..., -1.9467, -1.9295, -1.9467],\n          ...,\n          [ 1.2899,  1.0502,  1.1358,  ...,  0.7248,  0.7933,  0.7762],\n          [ 1.4098,  1.1872,  1.0331,  ...,  0.7077,  0.7419,  0.7419],\n          [ 1.2728,  0.9646,  0.9303,  ...,  0.7077,  0.7591,  0.7248]],\n \n         [[ 1.2206,  1.1856,  1.1506,  ..., -1.9832, -1.8782, -1.7731],\n          [ 1.2381,  1.1856,  1.1506,  ..., -1.9657, -1.8606, -1.8256],\n          [ 1.2381,  1.2031,  1.1681,  ..., -1.8606, -1.8256, -1.8431],\n          ...,\n          [ 1.2906,  1.0630,  1.1506,  ...,  0.3803,  0.4503,  0.4328],\n          [ 1.4307,  1.2031,  1.0280,  ...,  0.3627,  0.3978,  0.3978],\n          [ 1.2906,  0.9755,  0.9230,  ...,  0.3627,  0.4153,  0.3803]],\n \n         [[ 2.1346,  2.2217,  2.1868,  ..., -1.7173, -1.6127, -1.5604],\n          [ 2.1520,  2.2217,  2.1868,  ..., -1.6999, -1.5953, -1.5779],\n          [ 2.1694,  2.2217,  2.1868,  ..., -1.5953, -1.5430, -1.5604],\n          ...,\n          [ 1.2108,  0.9842,  1.0539,  ...,  0.3568,  0.4265,  0.4091],\n          [ 1.3154,  1.0888,  0.9494,  ...,  0.3393,  0.3742,  0.3742],\n          [ 1.1759,  0.8622,  0.8448,  ...,  0.3393,  0.3916,  0.3568]]]),\n 'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         ...,\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1]]),\n 'labels': {'size': tensor([640, 480]), 'image_id': tensor([719]), 'class_labels': tensor([4, 4, 1, 5, 0, 0]), 'boxes': tensor([[0.1898, 0.1767, 0.2161, 0.1620],\n         [0.5669, 0.1938, 0.0742, 0.0805],\n         [0.7672, 0.7768, 0.4526, 0.4327],\n         [0.4715, 0.6213, 0.2235, 0.1502],\n         [0.3973, 0.5639, 0.7729, 0.6337],\n         [0.6906, 0.4581, 0.5110, 0.4600]]), 'area': tensor([ 10753.6875,   1833.4000,  60167.3867,  10316.8945, 150459.0469,\n          72216.3203]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}}\n\n\n\n# Note: Depending on the size/speed of your GPU, this may take a while\nfrom transformers import TrainingArguments, Trainer\n\n# Set the batch size according to the memory you have available on your GPU\n# e.g. on my NVIDIA RTX 4090 with 24GB of VRAM, I can use a batch size of 32 without running out of memory\nBATCH_SIZE = 16\n\n# Note: AdamW Optimizer is used by default\ntraining_args = TrainingArguments(\n    output_dir=\"detr_finetuned_trashify_box_detector\",\n    num_train_epochs=25,\n    fp16=True,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    learning_rate=1e-4,\n    lr_scheduler_type=\"linear\",\n    weight_decay=1e-4,\n    max_grad_norm=0.01,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    report_to=\"none\", # don't save experiments to a third party service\n    dataloader_num_workers=4, # note: if you're on Google Colab, you may have to lower this to os.cpu_count() or to 0 \n    warmup_ratio=0.05, # learning rate warmup\n    push_to_hub=False,\n    eval_do_concat_batches=False\n)\n\nmodel_v1_trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=processed_dataset[\"train\"],\n    eval_dataset=processed_dataset[\"validation\"],\n    tokenizer=image_processor,\n    data_collator=data_collate_function,\n    # compute_metrics=None # TODO: TK - can add a metrics function, just see if model trains first, see here for an example: https://github.com/huggingface/transformers/blob/336dc69d63d56f232a183a3e7f52790429b871ef/examples/pytorch/object-detection/run_object_detection.py#L160 \n)\n\nmodel_v1_results = model_v1_trainer.train()\n\n/home/daniel/miniconda3/envs/ai/lib/python3.11/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n\n\n\n    \n      \n      \n      [1250/1250 04:25, Epoch 25/25]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\n\n\n\n\n1\n101.878300\n7.513162\n\n\n2\n4.145500\n3.055572\n\n\n3\n2.596400\n2.273679\n\n\n4\n2.277300\n2.069138\n\n\n5\n2.081800\n1.849403\n\n\n6\n1.925300\n1.687234\n\n\n7\n1.780200\n1.603322\n\n\n8\n1.675000\n1.451112\n\n\n9\n1.526300\n1.409718\n\n\n10\n1.432200\n1.339651\n\n\n11\n1.386000\n1.289711\n\n\n12\n1.309800\n1.281332\n\n\n13\n1.248000\n1.209565\n\n\n14\n1.209000\n1.220024\n\n\n15\n1.175700\n1.198685\n\n\n16\n1.144000\n1.175700\n\n\n17\n1.073200\n1.193522\n\n\n18\n1.050100\n1.153087\n\n\n19\n0.986400\n1.157631\n\n\n20\n0.994100\n1.151300\n\n\n21\n0.958900\n1.144987\n\n\n22\n0.927900\n1.135496\n\n\n23\n0.907100\n1.123257\n\n\n24\n0.885100\n1.133819\n\n\n25\n0.870900\n1.130216\n\n\n\n\n\n\nTK - Note: May get an error at the beginning where a box is predicted a negative output. This will break training as boxes are expected to be positive floats.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#tk---make-predictions-on-the-test-dataset",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#tk---make-predictions-on-the-test-dataset",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "9 TK - Make predictions on the test dataset",
    "text": "9 TK - Make predictions on the test dataset\n\nprocessed_dataset[\"test\"][0]\n\n{'pixel_values': tensor([[[-0.9705, -0.7308, -0.9705,  ..., -1.8953, -1.8268, -1.3130],\n          [-1.2959, -0.9363, -0.3883,  ..., -1.8953, -1.7240, -0.5596],\n          [-1.4843, -1.1418, -0.1999,  ..., -1.8782, -1.2788, -0.5424],\n          ...,\n          [ 1.3242,  1.3242,  1.3413,  ..., -0.6452, -0.2856, -0.9877],\n          [ 1.3070,  1.3584,  1.4098,  ..., -0.8678,  0.0398, -0.4911],\n          [ 1.2728,  1.3413,  1.4098,  ..., -0.9705,  0.1768, -0.1657]],\n \n         [[-0.5476, -0.3550, -0.6527,  ..., -1.7031, -1.6155, -1.0903],\n          [-0.8803, -0.5476, -0.0399,  ..., -1.6856, -1.5280, -0.3200],\n          [-1.0728, -0.7402,  0.1527,  ..., -1.6506, -1.0553, -0.3025],\n          ...,\n          [-1.7031, -1.7031, -1.6856,  ..., -0.3901,  0.0301, -0.7227],\n          [-1.7206, -1.6681, -1.6155,  ..., -0.6176,  0.3803, -0.1800],\n          [-1.7556, -1.6856, -1.6155,  ..., -0.7052,  0.5203,  0.1527]],\n \n         [[-1.0550, -0.7064, -0.8284,  ..., -1.6824, -1.5953, -1.0201],\n          [-1.3861, -0.9504, -0.2881,  ..., -1.6999, -1.4559, -0.2532],\n          [-1.6476, -1.1944, -0.1661,  ..., -1.6650, -1.0376, -0.2881],\n          ...,\n          [-1.2641, -1.2641, -1.2467,  ..., -0.9504, -0.8284, -1.2293],\n          [-1.2816, -1.2293, -1.1770,  ..., -1.1596, -0.5321, -1.0027],\n          [-1.3164, -1.2467, -1.1770,  ..., -1.3513, -0.4973, -0.8981]]]),\n 'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         ...,\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1]]),\n 'labels': {'size': tensor([640, 480]), 'image_id': tensor([61]), 'class_labels': tensor([4, 5, 1, 0]), 'boxes': tensor([[0.2104, 0.8563, 0.2855, 0.2720],\n         [0.4194, 0.4927, 0.2398, 0.1785],\n         [0.3610, 0.6227, 0.2706, 0.2330],\n         [0.4974, 0.4785, 0.3829, 0.3820]]), 'area': tensor([23860.4043, 13150.1748, 19368.0898, 44929.9102]), 'iscrowd': tensor([0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}}\n\n\n\n# Make predictions with trainer containing trained model\ntest_dataset_preds = model_v1_trainer.predict(test_dataset=processed_dataset[\"test\"])\n# test_dataset_preds\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[39], line 2\n      1 # Make predictions with trainer containing trained model\n----&gt; 2 test_dataset_preds = model_v1_trainer.predict(test_dataset=processed_dataset[\"test\"])\n      3 # test_dataset_preds\n\nNameError: name 'model_v1_trainer' is not defined\n\n\n\n\n# Get the logits\ntest_pred_logits = test_dataset_preds.predictions[0][1]\n\n# Get the boxes\ntest_pred_boxes = test_dataset_preds.predictions[0][2]\n\n# Get the label IDs\ntest_pred_label_ids = test_dataset_preds.label_ids\n\n# Check shapes\ntest_pred_logits.shape, test_pred_boxes.shape, len(test_pred_label_ids)\n\n((16, 300, 7), (16, 300, 4), 13)\n\n\n\n%%time\n\n# Get a random sample from the test preds\nrandom_test_pred_index = random.randint(0, len(processed_dataset[\"test\"]))\nprint(f\"[INFO] Making predictions on test item with index: {random_test_pred_index}\")\n\nrandom_test_sample = processed_dataset[\"test\"][random_test_pred_index]\n\n# Do a single forward pass with the model\nrandom_test_sample_outputs = model(pixel_values=random_test_sample[\"pixel_values\"].unsqueeze(0).to(\"cuda\"), # model expects input [batch_size, color_channels, height, width]\n                                   pixel_mask=None)\n# random_test_sample_outputs\n\n[INFO] Making predictions on test item with index: 163\nCPU times: user 51.5 ms, sys: 10.3 ms, total: 61.8 ms\nWall time: 63.1 ms\n\n\n\n# image_processor.preprocess?\n\n\n# Get a random sample from the test preds\nrandom_test_pred_index = random.randint(0, len(processed_dataset[\"test\"]))\nprint(f\"[INFO] Making predictions on test item with index: {random_test_pred_index}\")\n\nrandom_test_sample = processed_dataset[\"test\"][random_test_pred_index]\n\n# # Do a single forward pass with the model\nrandom_test_sample_outputs = model(pixel_values=random_test_sample[\"pixel_values\"].unsqueeze(0).to(\"cuda\"), # model expects input [batch_size, color_channels, height, width]\n                                   pixel_mask=None)\n\n# Post process a random item from test preds\nrandom_test_sample_outputs_post_processed = image_processor.post_process_object_detection(\n    outputs=random_test_sample_outputs,\n    threshold=0.25, # prediction probability threshold for boxes (note: boxes from an untrained model will likely be bad)\n    target_sizes=[random_test_sample[\"labels\"][\"orig_size\"]] # original input image size (or whichever target size you'd like), required to be same number of input items in a list\n)\n\n# Plot the random sample test preds\n# Extract scores, labels and boxes\nrandom_test_sample_pred_scores = random_test_sample_outputs_post_processed[0][\"scores\"]\nrandom_test_sample_pred_labels = random_test_sample_outputs_post_processed[0][\"labels\"]\nrandom_test_sample_pred_boxes = random_test_sample_outputs_post_processed[0][\"boxes\"]\n\n# Create a list of labels to plot on the boxes \nrandom_test_sample_labels_to_plot = [f\"Pred: {id2label[label_pred.item()]} ({round(score_pred.item(), 4)})\" \n                  for label_pred, score_pred in zip(random_test_sample_pred_labels, random_test_sample_pred_scores)]\n\nprint(f\"[INFO] Labels with scores: {random_test_sample_labels_to_plot}\")\n\n# Plot the predicted boxes on the random test image \nto_pil_image(\n    pic=draw_bounding_boxes(\n        image=pil_to_tensor(pic=dataset[\"test\"][random_test_pred_index][\"image\"]),\n        boxes=random_test_sample_pred_boxes,\n        labels=random_test_sample_labels_to_plot,\n        width=3\n    )\n)\n\n[INFO] Making predictions on test item with index: 28\n[INFO] Labels with scores: ['Pred: hand (0.4208)', 'Pred: trash (0.3352)']\n\n\n\n\n\n\n\n\n\nTK - nice!!! these boxes look far better than our randomly predicted boxes with an untrained model‚Ä¶\n\n9.1 TK - Predict on image from filepath\n\n# Pred on image from pathname\nfrom pathlib import Path\nfrom PIL import Image\npath_to_test_image_folder = Path(\"data/trashify_test_images\")\ntest_image_filepaths = list(path_to_test_image_folder.rglob(\"*.jp*g\"))\ntest_image_targ_filepath = random.choice(test_image_filepaths)\n# test_image_targ_filepath = \"data/trashify_test_images/IMG_6692.jpeg\"\ntest_image_pil = Image.open(test_image_targ_filepath)\ntest_image_preprocessed = image_processor.preprocess(images=test_image_pil,\n                                                     return_tensors=\"pt\")\n\ndef get_image_dimensions_from_pil(image: Image.Image) -&gt; torch.tensor:\n    \"\"\"\n    Convert the dimensions of a PIL image to a PyTorch tensor in the order (height, width).\n\n    Args:\n        image (Image.Image): The input PIL image.\n\n    Returns:\n        torch.Tensor: A tensor containing the height and width of the image.\n    \"\"\"\n    # Get (width, height) of image (PIL.Image.size returns width, height)\n    width, height = image.size\n\n    # Convert to a tensor in the order (height, width)\n    image_dimensions_tensor = torch.tensor([height, width])\n\n    return image_dimensions_tensor\n\n# Get image original size\ntest_image_size = get_image_dimensions_from_pil(image=test_image_pil)\n\n# Make predictions on the preprocessed image\nrandom_test_sample_outputs = model(pixel_values=test_image_preprocessed[\"pixel_values\"].to(\"cuda\"), # model expects input [batch_size, color_channels, height, width]\n                                   pixel_mask=None)\n\nTHRESHOLD = 0.2\n\n# Post process the predictions\nrandom_test_sample_outputs_post_processed = image_processor.post_process_object_detection(\n    outputs=random_test_sample_outputs,\n    threshold=THRESHOLD,\n    target_sizes=[test_image_size] # needs to be same length as batch dimension of the logits (e.g. [[height, width]])\n)\n\n# Extract scores, labels and boxes\nrandom_test_sample_pred_scores = random_test_sample_outputs_post_processed[0][\"scores\"]\nrandom_test_sample_pred_labels = random_test_sample_outputs_post_processed[0][\"labels\"]\nrandom_test_sample_pred_boxes = random_test_sample_outputs_post_processed[0][\"boxes\"]\n\n# Create a lsit of labels to plot on the boxes \nrandom_test_sample_labels_to_plot = [f\"Pred: {id2label[label_pred.item()]} ({round(score_pred.item(), 4)})\"\n                                     for label_pred, score_pred in zip(random_test_sample_pred_labels, random_test_sample_pred_scores)]\n\nprint(\"[INFO] Labels with scores:\")\nfor item in random_test_sample_labels_to_plot:\n    print(item)\n\n# Plot the predicted boxes on the random test image \nto_pil_image(\n    pic=draw_bounding_boxes(\n        image=pil_to_tensor(pic=test_image_pil),                    \n        boxes=random_test_sample_pred_boxes,\n        labels=random_test_sample_labels_to_plot,\n        width=3\n    )\n)\n\n\n# # Plot the random sample image with randomly predicted boxes (these will be very poor since the model is not trained on our data yet)\n# to_pil_image(\n#     pic=draw_bounding_boxes(\n#         image=pil_to_tensor(pic=dataset[\"test\"][random_test_pred_index][\"image\"]),\n#         boxes=random_test_sample_pred_boxes,\n#         labels=random_test_sample_labels_to_plot,\n#         width=3\n#     )\n# )\n\n[INFO] Labels with scores:\nPred: trash (0.7138)\nPred: bin (0.699)\nPred: hand (0.6244)\nPred: bin (0.6231)\nPred: not_trash (0.4189)\nPred: bin (0.2655)\nPred: hand (0.2617)\nPred: not_trash (0.2392)\nPred: not_trash (0.2335)",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#tk---upload-our-trained-model-to-hugging-face-hub",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#tk---upload-our-trained-model-to-hugging-face-hub",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "10 TK - Upload our trained model to Hugging Face Hub",
    "text": "10 TK - Upload our trained model to Hugging Face Hub\nTK - Let‚Äôs make our model available for others to use.\n\n# UPTOHERE\n# Make extensions to make the model better... (e.g. data augmentation = harder training set = better overall validation loss)\n# Model with data augmentation\n# Model with longer training (e.g. 100 epochs) \n# Research eval_do_concat_batches=False/True & see what the results do...\n\n\n# Save the model\nfrom datetime import datetime\n\n# TODO: update this save path so we know when the model was saved and what its parameters were\ntraining_epochs_ = training_args.num_train_epochs\nlearning_rate_ = \"{:.0e}\".format(training_args.learning_rate)\n\nmodel_save_path = f\"models/learn_hf_microsoft_detr_finetuned_trashify_box_dataset_only_manual_data_no_aug_{training_epochs_}_epochs_lr_{learning_rate_}\"\nprint(f\"[INFO] Saving model to: {model_save_path}\")\nmodel_v1_trainer.save_model(model_save_path)\n\n[INFO] Saving model to: models/learn_hf_microsoft_detr_finetuned_trashify_box_dataset_only_manual_data_no_aug_25_epochs_lr_1e-04\n\n\n\n# Push the model to the hub\n# Note: this will require you to have your Hugging Face account setup \nmodel_v1_trainer.push_to_hub(commit_message=\"upload trashify object detection model\",\n                    # token=None # Optional to add a token manually\n                    )\n\n\n\n\n\n\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/mrdbourke/detr_finetuned_trashify_box_detector/commit/ab273cec67e5124ac047dc1e068c379c718e6c37', commit_message='upload trashify object detection model', commit_description='', oid='ab273cec67e5124ac047dc1e068c379c718e6c37', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mrdbourke/detr_finetuned_trashify_box_detector', endpoint='https://huggingface.co', repo_type='model', repo_id='mrdbourke/detr_finetuned_trashify_box_detector'), pr_revision=None, pr_num=None)",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#creating-a-demo-of-our-model-with-gradio",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#creating-a-demo-of-our-model-with-gradio",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "11 Creating a demo of our model with Gradio",
    "text": "11 Creating a demo of our model with Gradio\n\n%%writefile demos/trashify_object_detector/README.md\n---\ntitle: Trashify Demo V1 üöÆ\nemoji: üóëÔ∏è\ncolorFrom: purple\ncolorTo: blue\nsdk: gradio\nsdk_version: 4.40.0\napp_file: app.py\npinned: false\nlicense: apache-2.0\n---\n\n# üöÆ Trashify Object Detector V1 \n\nObject detection demo to detect `trash`, `bin`, `hand`, `trash_arm`, `not_trash`, `not_bin`, `not_hand`. \n\nUsed as example for encouraging people to cleanup their local area.\n\nIf `trash`, `hand`, `bin` all detected = +1 point.\n\n## Dataset\n\nAll Trashify models are trained on a custom hand-labelled dataset of people picking up trash and placing it in a bin.\n\nThe dataset can be found on Hugging Face as [`mrdbourke/trashify_manual_labelled_images`](https://huggingface.co/datasets/mrdbourke/trashify_manual_labelled_images).\n\n## Demos\n\n* [V1](https://huggingface.co/spaces/mrdbourke/trashify_demo_v1) = Fine-tuned DETR model trained *without* data augmentation.\n* [V2](https://huggingface.co/spaces/mrdbourke/trashify_demo_v2) = Fine-tuned DETR model trained *with* data augmentation.\n* [V3](https://huggingface.co/spaces/mrdbourke/trashify_demo_v3) = Fine-tuned DETR model trained *with* data augmentation (same as V2) with an NMS (Non Maximum Suppression) post-processing step.\n\nTK - add links to resources to learn more\n\nOverwriting demos/trashify_object_detector/README.md\n\n\n\n%%writefile demos/trashify_object_detector/requirements.txt\ntimm\ngradio\ntorch\ntransformers\n\nOverwriting demos/trashify_object_detector/requirements.txt\n\n\n\n%%writefile demos/trashify_object_detector/app.py\nimport gradio as gr\nimport torch\nfrom PIL import Image, ImageDraw, ImageFont\n\nfrom transformers import AutoImageProcessor\nfrom transformers import AutoModelForObjectDetection\n\n# Note: Can load from Hugging Face or can load from local \nmodel_save_path = \"mrdbourke/detr_finetuned_trashify_box_detector\"\n\n# Load the model and preprocessor\nimage_processor = AutoImageProcessor.from_pretrained(model_save_path)\nmodel = AutoModelForObjectDetection.from_pretrained(model_save_path)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(device)\n\n# Get the id2label dictionary from the model\nid2label = model.config.id2label\n\n# Set up a colour dictionary for plotting boxes with different colours\ncolor_dict = {   \n    \"bin\": \"green\",\n    \"trash\": \"blue\",\n    \"hand\": \"purple\",\n    \"trash_arm\": \"yellow\",\n    \"not_trash\": \"red\",\n    \"not_bin\": \"red\",\n    \"not_hand\": \"red\",\n}\n\n# Create helper functions for seeing if items from one list are in another \ndef any_in_list(list_a, list_b):\n    \"Returns True if any item from list_a is in list_b, otherwise False.\"\n    return any(item in list_b for item in list_a)\n\ndef all_in_list(list_a, list_b):\n    \"Returns True if all items from list_a are in list_b, otherwise False.\"\n    return all(item in list_b for item in list_a)\n\ndef predict_on_image(image, conf_threshold):\n    with torch.no_grad():\n        inputs = image_processor(images=[image], return_tensors=\"pt\")\n        outputs = model(**inputs.to(device))\n\n        target_sizes = torch.tensor([[image.size[1], image.size[0]]]) # height, width \n\n        results = image_processor.post_process_object_detection(outputs,\n                                                                threshold=conf_threshold,\n                                                                target_sizes=target_sizes)[0]\n    # Return all items in results to CPU\n    for key, value in results.items():\n        try:\n            results[key] = value.item().cpu() # can't get scalar as .item() so add try/except block\n        except:\n            results[key] = value.cpu()\n\n    # Can return results as plotted on a PIL image (then display the image)\n    draw = ImageDraw.Draw(image)\n\n    # Get a font from ImageFont\n    font = ImageFont.load_default(size=20)\n\n    # Get class names as text for print out\n    class_name_text_labels = []\n\n    for box, score, label in zip(results[\"boxes\"], results[\"scores\"], results[\"labels\"]):\n        # Create coordinates\n        x, y, x2, y2 = tuple(box.tolist())\n\n        # Get label_name\n        label_name = id2label[label.item()]\n        targ_color = color_dict[label_name]\n        class_name_text_labels.append(label_name)\n\n        # Draw the rectangle\n        draw.rectangle(xy=(x, y, x2, y2), \n                       outline=targ_color,\n                       width=3)\n        \n        # Create a text string to display\n        text_string_to_show = f\"{label_name} ({round(score.item(), 3)})\"\n\n        # Draw the text on the image\n        draw.text(xy=(x, y),\n                  text=text_string_to_show,\n                  fill=\"white\",\n                  font=font)\n    \n    # Remove the draw each time\n    del draw\n\n    # Setup blank string to print out\n    return_string = \"\"\n\n    # Setup list of target items to discover\n    target_items = [\"trash\", \"bin\", \"hand\"]\n\n    # If no items detected or trash, bin, hand not in list, return notification \n    if (len(class_name_text_labels) == 0) or not (any_in_list(list_a=target_items, list_b=class_name_text_labels)):\n        return_string = f\"No trash, bin or hand detected at confidence threshold {conf_threshold}. Try another image or lowering the confidence threshold.\"\n        return image, return_string\n\n    # If there are some missing, print the ones which are missing\n    elif not all_in_list(list_a=target_items, list_b=class_name_text_labels):\n        missing_items = []\n        for item in target_items:\n            if item not in class_name_text_labels:\n                missing_items.append(item)\n        return_string = f\"Detected the following items: {class_name_text_labels}. But missing the following in order to get +1: {missing_items}. If this is an error, try another image or altering the confidence threshold. Otherwise, the model may need to be updated with better data.\"\n        \n    # If all 3 trash, bin, hand occur = + 1\n    if all_in_list(list_a=target_items, list_b=class_name_text_labels):\n        return_string = f\"+1! Found the following items: {class_name_text_labels}, thank you for cleaning up the area!\"\n\n    print(return_string)\n    \n    return image, return_string\n\n# Create the interface\ndemo = gr.Interface(\n    fn=predict_on_image,\n    inputs=[\n        gr.Image(type=\"pil\", label=\"Target Image\"),\n        gr.Slider(minimum=0, maximum=1, value=0.25, label=\"Confidence Threshold\")\n    ],\n    outputs=[\n        gr.Image(type=\"pil\", label=\"Image Output\"),\n        gr.Text(label=\"Text Output\")\n    ],\n    title=\"üöÆ Trashify Object Detection Demo V1\",\n    description=\"Help clean up your local area! Upload an image and get +1 if there is all of the following items detected: trash, bin, hand.\",\n    # Examples come in the form of a list of lists, where each inner list contains elements to prefill the `inputs` parameter with\n    examples=[\n        [\"examples/trashify_example_1.jpeg\", 0.25],\n        [\"examples/trashify_example_2.jpeg\", 0.25],\n        [\"examples/trashify_example_3.jpeg\", 0.25],\n    ],\n    cache_examples=True\n)\n\n# Launch the demo\ndemo.launch()\n\nOverwriting demos/trashify_object_detector/app.py\n\n\n\n11.1 TK - Upload demo to Hugging Face Spaces to get it live\n\n# 1. Import the required methods for uploading to the Hugging Face Hub\nfrom huggingface_hub import (\n    create_repo,\n    get_full_repo_name,\n    upload_file, # for uploading a single file (if necessary)\n    upload_folder # for uploading multiple files (in a folder)\n)\n\n# 2. Define the parameters we'd like to use for the upload\nLOCAL_DEMO_FOLDER_PATH_TO_UPLOAD = \"demos/trashify_object_detector\" # TK - update this path \nHF_TARGET_SPACE_NAME = \"trashify_demo_v1\"\nHF_REPO_TYPE = \"space\" # we're creating a Hugging Face Space\nHF_SPACE_SDK = \"gradio\"\nHF_TOKEN = \"\" # optional: set to your Hugging Face token (but I'd advise storing this as an environment variable as previously discussed)\n\n# 3. Create a Space repository on Hugging Face Hub \nprint(f\"[INFO] Creating repo on Hugging Face Hub with name: {HF_TARGET_SPACE_NAME}\")\ncreate_repo(\n    repo_id=HF_TARGET_SPACE_NAME,\n    # token=HF_TOKEN, # optional: set token manually (though it will be automatically recognized if it's available as an environment variable)\n    repo_type=HF_REPO_TYPE,\n    private=False, # set to True if you don't want your Space to be accessible to others\n    space_sdk=HF_SPACE_SDK,\n    exist_ok=True, # set to False if you want an error to raise if the repo_id already exists \n)\n\n# 4. Get the full repository name (e.g. {username}/{model_id} or {username}/{space_name})\nfull_hf_repo_name = get_full_repo_name(model_id=HF_TARGET_SPACE_NAME)\nprint(f\"[INFO] Full Hugging Face Hub repo name: {full_hf_repo_name}\")\n\n# 5. Upload our demo folder\nprint(f\"[INFO] Uploading {LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD} to repo: {full_hf_repo_name}\")\nfolder_upload_url = upload_folder(\n    repo_id=full_hf_repo_name,\n    folder_path=LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD,\n    path_in_repo=\".\", # upload our folder to the root directory (\".\" means \"base\" or \"root\", this is the default)\n    # token=HF_TOKEN, # optional: set token manually\n    repo_type=HF_REPO_TYPE,\n    commit_message=\"Uploading Trashify box detection model app.py\"\n)\nprint(f\"[INFO] Demo folder successfully uploaded with commit URL: {folder_upload_url}\")\n\n[INFO] Creating repo on Hugging Face Hub with name: trashify_demo_v1\n[INFO] Full Hugging Face Hub repo name: mrdbourke/trashify_demo_v1\n[INFO] Uploading demos/trashify_object_detector to repo: mrdbourke/trashify_demo_v1\n[INFO] Demo folder successfully uploaded with commit URL: https://huggingface.co/spaces/mrdbourke/trashify_demo_v1/tree/main/.\n\n\nTK - see the demo here: https://huggingface.co/spaces/mrdbourke/trashify_demo_v1\n\n\n11.2 TK - Testing the hosted demo\n\nfrom IPython.display import HTML\n\n\n# You can get embeddable HTML code for your demo by clicking the \"Embed\" button on the demo page\nHTML(data='''\n&lt;iframe\n    src=\"https://mrdbourke-trashify-demo-v1.hf.space\"\n    frameborder=\"0\"\n    width=\"850\"\n    height=\"1000\"\n&gt;&lt;/iframe&gt;     \n''')",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#tk---improve-our-model-with-data-augmentation",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#tk---improve-our-model-with-data-augmentation",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "12 TK - Improve our model with data augmentation",
    "text": "12 TK - Improve our model with data augmentation\nUPTOHERE - Read for object detection augmentation (keep it simple) - Check out the papers for detection augmentation - Train a model with data augmentation - Compare the model‚Äôs metrics between data augmentation and no data augmentation\n\n12.1 Load dataset\n\nfrom datasets import load_dataset\n\n# load_dataset?\ndataset = load_dataset(path=\"mrdbourke/trashify_manual_labelled_images\")\n\nprint(f\"[INFO] Length of original dataset: {len(dataset['train'])}\")\n\n# Split the data\ndataset_split = dataset[\"train\"].train_test_split(test_size=0.3, seed=42) # split the dataset into 70/30 train/test\ndataset_test_val_split = dataset_split[\"test\"].train_test_split(test_size=0.6, seed=42) # split the test set into 40/60 validation/test\n\n# Create splits\ndataset[\"train\"] = dataset_split[\"train\"]\ndataset[\"validation\"] = dataset_test_val_split[\"train\"]\ndataset[\"test\"] = dataset_test_val_split[\"test\"]\n\ndataset\n\n[INFO] Length of original dataset: 1128\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],\n        num_rows: 789\n    })\n    validation: Dataset({\n        features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],\n        num_rows: 135\n    })\n    test: Dataset({\n        features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],\n        num_rows: 204\n    })\n})\n\n\n\n# Get the categories from the dataset\n# Note: this requires the dataset to have been uploaded with this feature setup\ncategories = dataset[\"train\"].features[\"annotations\"].feature[\"category_id\"]\n\n# Get the names attribute\ncategories.names\n\n['bin', 'hand', 'not_bin', 'not_hand', 'not_trash', 'trash', 'trash_arm']\n\n\n\nid2label = {i: class_name for i, class_name in enumerate(categories.names)}\nlabel2id = {value: key for key, value in id2label.items()}\n\nid2label, label2id\n\n({0: 'bin',\n  1: 'hand',\n  2: 'not_bin',\n  3: 'not_hand',\n  4: 'not_trash',\n  5: 'trash',\n  6: 'trash_arm'},\n {'bin': 0,\n  'hand': 1,\n  'not_bin': 2,\n  'not_hand': 3,\n  'not_trash': 4,\n  'trash': 5,\n  'trash_arm': 6})\n\n\n\n# View a random sample\nimport random\nrandom_idx = random.randint(0, len(dataset[\"train\"]))\nrandom_sample = dataset[\"train\"][random_idx]\nrandom_sample\n\n{'image': &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,\n 'image_id': 955,\n 'annotations': {'file_name': ['ed8cb1ab-2882-4ab7-a839-c53fa2908a72.jpeg',\n   'ed8cb1ab-2882-4ab7-a839-c53fa2908a72.jpeg',\n   'ed8cb1ab-2882-4ab7-a839-c53fa2908a72.jpeg',\n   'ed8cb1ab-2882-4ab7-a839-c53fa2908a72.jpeg'],\n  'image_id': [955, 955, 955, 955],\n  'category_id': [5, 1, 0, 4],\n  'bbox': [[464.79998779296875, 625.5999755859375, 68.30000305175781, 92.5],\n   [483.0, 686.2000122070312, 173.0, 247.3000030517578],\n   [102.80000305175781, 361.70001220703125, 813.5, 734.0],\n   [325.29998779296875,\n    716.5999755859375,\n    189.60000610351562,\n    215.3000030517578]],\n  'iscrowd': [0, 0, 0, 0],\n  'area': [6317.75, 42782.8984375, 597109.0, 40820.87890625]},\n 'label_source': 'manual_prodigy_label',\n 'image_source': 'manual_taken_photo'}\n\n\n\n\n12.2 Setup model\n\nfrom transformers import AutoModelForObjectDetection, AutoImageProcessor\n\n# Model config - https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrConfig \n# Model docs - https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrModel \nMODEL_NAME = \"microsoft/conditional-detr-resnet-50\"\n\n# Set image size\nIMAGE_SIZE = 640 # other common image sizes include: 300x300, 480x480, 512x512, 640x640, 800x800 (best to experiment and see which works best)\n\n# Get the image processor (this is required for prepraring images)\n# See docs: https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.preprocess\nimage_processor = AutoImageProcessor.from_pretrained(\n    pretrained_model_name_or_path=MODEL_NAME,\n    format=\"coco_detection\", # this is the default\n    do_convert_annotations=True, # defaults to True, converts boxes to (center_x, center_y, width, height)\n    size={\"shortest_edge\": IMAGE_SIZE, \"longest_edge\": IMAGE_SIZE},\n    max_size=None # Note: this parameter is deprecated and will produce a warning if used during processing.\n)\n\n# Check out the image processor\nimage_processor\n\nConditionalDetrImageProcessor {\n  \"do_convert_annotations\": true,\n  \"do_normalize\": true,\n  \"do_pad\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"format\": \"coco_detection\",\n  \"image_mean\": [\n    0.485,\n    0.456,\n    0.406\n  ],\n  \"image_processor_type\": \"ConditionalDetrImageProcessor\",\n  \"image_std\": [\n    0.229,\n    0.224,\n    0.225\n  ],\n  \"pad_size\": null,\n  \"resample\": 2,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"longest_edge\": 640,\n    \"shortest_edge\": 640\n  }\n}\n\n\n\n# First create a couple of dataclasses to store our data format\nfrom dataclasses import dataclass, asdict\nfrom typing import List, Tuple\n\n@dataclass\nclass SingleCOCOAnnotation:\n    \"An instance of a single COCO annotation. See COCO format: https://cocodataset.org/#format-data\"\n    image_id: int\n    category_id: int\n    bbox: List[float] # bboxes in format [x_top_left, y_top_left, width, height]\n    area: float = 0.0\n    iscrowd: int = 0\n\n@dataclass\nclass ImageCOCOAnnotations:\n    \"A collection of COCO annotations for a given image_id.\"\n    image_id: int\n    annotations: List[SingleCOCOAnnotation]\n\ndef format_image_annotations_as_coco(\n        image_id: int,\n        categories: List[int],\n        areas: List[float],\n        bboxes: List[Tuple[float, float, float, float]] # bboxes in format \n) -&gt; dict:\n    # Turn input lists into a list of dicts\n    coco_format_annotations = [\n        asdict(SingleCOCOAnnotation(\n            image_id=image_id,\n            category_id=category,\n            bbox=list(bbox),\n            area=area,\n        ))\n        for category, area, bbox in zip(categories, areas, bboxes)\n    ]\n\n    # Return dictionary of annotations with format {\"image_id\": ..., \"annotations\": ...}\n    return asdict(ImageCOCOAnnotations(image_id=image_id,\n                                       annotations=coco_format_annotations))\n\n# Let's try it out\nimage_id = 0\nrandom_idx = random.randint(0, len(dataset[\"train\"]))\nrandom_sample = dataset[\"train\"][random_idx]\nrandom_sample_categories = random_sample[\"annotations\"][\"category_id\"]\nrandom_sample_areas = random_sample[\"annotations\"][\"area\"]\nrandom_sample_bboxes = random_sample[\"annotations\"][\"bbox\"]\n\nrandom_sample_coco_annotations = format_image_annotations_as_coco(image_id=image_id,\n                                                                  categories=random_sample_categories,\n                                                                  areas=random_sample_areas,\n                                                                  bboxes=random_sample_bboxes)\nrandom_sample_coco_annotations\n\n{'image_id': 0,\n 'annotations': [{'image_id': 0,\n   'category_id': 0,\n   'bbox': [452.79998779296875,\n    446.6000061035156,\n    272.70001220703125,\n    388.20001220703125],\n   'area': 105862.140625,\n   'iscrowd': 0},\n  {'image_id': 0,\n   'category_id': 0,\n   'bbox': [146.5, 487.5, 348.3999938964844, 424.79998779296875],\n   'area': 148000.3125,\n   'iscrowd': 0},\n  {'image_id': 0,\n   'category_id': 0,\n   'bbox': [8.300000190734863, 522.5, 241.3000030517578, 505.0],\n   'area': 121856.5,\n   'iscrowd': 0}]}\n\n\n\n# Setup the model\n# TODO: Can functionize this to create a base model (e.g. a model with all the base settings/untrained weights) \ndef create_model():\n    model = AutoModelForObjectDetection.from_pretrained(\n                pretrained_model_name_or_path=MODEL_NAME,\n                label2id=label2id,\n                id2label=id2label,\n                ignore_mismatched_sizes=True,\n                backbone=\"resnet50\")\n    return model\n\nmodel_aug = create_model()\nmodel_aug\n\nSome weights of ConditionalDetrForObjectDetection were not initialized from the model checkpoint at microsoft/conditional-detr-resnet-50 and are newly initialized because the shapes did not match:\n- class_labels_classifier.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([7]) in the model instantiated\n- class_labels_classifier.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nConditionalDetrForObjectDetection(\n  (model): ConditionalDetrModel(\n    (backbone): ConditionalDetrConvModel(\n      (conv_encoder): ConditionalDetrConvEncoder(\n        (model): FeatureListNet(\n          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n          (bn1): ConditionalDetrFrozenBatchNorm2d()\n          (act1): ReLU(inplace=True)\n          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n          (layer1): Sequential(\n            (0): Bottleneck(\n              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n              (downsample): Sequential(\n                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (1): ConditionalDetrFrozenBatchNorm2d()\n              )\n            )\n            (1): Bottleneck(\n              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (2): Bottleneck(\n              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n          )\n          (layer2): Sequential(\n            (0): Bottleneck(\n              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n              (downsample): Sequential(\n                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n                (1): ConditionalDetrFrozenBatchNorm2d()\n              )\n            )\n            (1): Bottleneck(\n              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (2): Bottleneck(\n              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (3): Bottleneck(\n              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n          )\n          (layer3): Sequential(\n            (0): Bottleneck(\n              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n              (downsample): Sequential(\n                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n                (1): ConditionalDetrFrozenBatchNorm2d()\n              )\n            )\n            (1): Bottleneck(\n              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (2): Bottleneck(\n              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (3): Bottleneck(\n              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (4): Bottleneck(\n              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (5): Bottleneck(\n              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n          )\n          (layer4): Sequential(\n            (0): Bottleneck(\n              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n              (downsample): Sequential(\n                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n                (1): ConditionalDetrFrozenBatchNorm2d()\n              )\n            )\n            (1): Bottleneck(\n              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (2): Bottleneck(\n              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n          )\n        )\n      )\n      (position_embedding): ConditionalDetrSinePositionEmbedding()\n    )\n    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n    (query_position_embeddings): Embedding(300, 256)\n    (encoder): ConditionalDetrEncoder(\n      (layers): ModuleList(\n        (0-5): 6 x ConditionalDetrEncoderLayer(\n          (self_attn): DetrAttention(\n            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): ReLU()\n          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (decoder): ConditionalDetrDecoder(\n      (layers): ModuleList(\n        (0): ConditionalDetrDecoderLayer(\n          (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)\n          (self_attn): ConditionalDetrAttention(\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)\n          (encoder_attn): ConditionalDetrAttention(\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        )\n        (1-5): 5 x ConditionalDetrDecoderLayer(\n          (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)\n          (self_attn): ConditionalDetrAttention(\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_qpos_proj): None\n          (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)\n          (encoder_attn): ConditionalDetrAttention(\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (query_scale): MLP(\n        (layers): ModuleList(\n          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n        )\n      )\n      (ref_point_head): MLP(\n        (layers): ModuleList(\n          (0): Linear(in_features=256, out_features=256, bias=True)\n          (1): Linear(in_features=256, out_features=2, bias=True)\n        )\n      )\n    )\n  )\n  (class_labels_classifier): Linear(in_features=256, out_features=7, bias=True)\n  (bbox_predictor): ConditionalDetrMLPPredictionHead(\n    (layers): ModuleList(\n      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n      (2): Linear(in_features=256, out_features=4, bias=True)\n    )\n  )\n)\n\n\n\n\n12.3 tk - Setup and visualize transforms (augmentations)\n\nTK - explain simple augmentations:\n\nRandomHorizontalFlip\nColorJitter\n\nThat‚Äôs it‚Ä¶\nTailor the data augmentations to your own dataset/problem\n\n\n\n\nimport torch\nimport torchvision\n\nfrom torchvision.transforms import v2 \nfrom torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor, pad\nfrom torchvision.utils import draw_bounding_boxes\n\n# Optional transform from here: https://arxiv.org/pdf/2012.07177\n# Scale jitter -&gt; pad -&gt; resize \n\ntrain_transforms = v2.Compose([\n    v2.ToImage(),\n    # v2.RandomResizedCrop(size=(640, 640), antialias=True),\n    # v2.Resize(size=(640, 640)),\n    # v2.RandomShortestSize(min_size=480, max_size=640),\n    # v2.ScaleJitter(target_size=(640, 640)),\n    # PadToSize(target_height=640, target_width=640),\n    v2.RandomHorizontalFlip(p=0.5),\n    # v2.RandomPhotometricDistort(p=0.75),\n    # v2.RandomShortestSize(min_size=480, max_size=640),\n    # v2.Resize(size=(640, 640)),\n    v2.ColorJitter(brightness=0.75, # randomly adjust the brightness \n                   contrast=0.75), # randomly alter the contrast\n    # v2.RandomPerspective(distortion_scale=0.3, \n    #                      p=0.3,\n    #                      fill=(123, 117, 104)), # fill with average colour\n    # v2.RandomZoomOut(side_range=(1.0, 1.5),\n    #                  fill=(123, 117, 104)),\n    v2.ToDtype(dtype=torch.float32, scale=True),\n\n    # v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    # sanitize boxes, recommended to be called at least once at the end of the transform pipeline\n    # https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.SanitizeBoundingBoxes.html#torchvision.transforms.v2.SanitizeBoundingBoxes\n    v2.SanitizeBoundingBoxes(labels_getter=None) \n])\n\n\n\n12.4 TK - Visualize transforms\n\nimport random\nrandom_idx = random.randint(0, len(dataset[\"train\"]))\nrandom_sample = dataset[\"train\"][random_idx]\n\n# Perform transform on image\nrandom_sample_image = random_sample[\"image\"]\nrandom_sample_image_width, random_sample_image_height = random_sample[\"image\"].size\nrandom_sample_boxes_xywh = random_sample[\"annotations\"][\"bbox\"] # these are in XYWH format\nrandom_sample_boxes_xyxy = torchvision.ops.box_convert(boxes=torch.tensor(random_sample_boxes_xywh),\n                                                       in_fmt=\"xywh\",\n                                                       out_fmt=\"xyxy\")\n\n# Format boxes to be xyxy for transforms\nrandom_sample_boxes_xyxy = torchvision.tv_tensors.BoundingBoxes(\n    data=random_sample_boxes_xyxy,\n    format=\"XYXY\",\n    canvas_size=(random_sample_image_height, random_sample_image_width) # comes in the form height, width\n)\n\nrandom_sample_image_transformed, random_sample_boxes_transformed = train_transforms(random_sample_image,\n                                                                                    random_sample_boxes_xyxy)\n\n\nrandom_sample_original_image_with_boxes = to_pil_image(pic=draw_bounding_boxes(\n                                                       image=pil_to_tensor(pic=random_sample_image),                    \n                                                       boxes=random_sample_boxes_xyxy,\n                                                       labels=None,\n                                                       width=3))\nrandom_sample_original_image_with_boxes_size = (random_sample_original_image_with_boxes.size[1], random_sample_original_image_with_boxes.size[0])\n\n# Plot the predicted boxes on the random test image \nrandom_sample_transformed_image_with_boxes = to_pil_image(pic=draw_bounding_boxes(\n                                                          image=random_sample_image_transformed,                    \n                                                          boxes=random_sample_boxes_transformed,\n                                                          labels=None,\n                                                          width=3))\nrandom_sample_transformed_image_with_boxes_size = (random_sample_transformed_image_with_boxes.size[1], random_sample_transformed_image_with_boxes.size[0])\n\n# Visualize the transformed image \nimport matplotlib.pyplot as plt\n\n# Create a figure with two subplots\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n# Display image 1\naxes[0].imshow(random_sample_original_image_with_boxes)\naxes[0].axis(\"off\")  # Hide axes\naxes[0].set_title(f\"Original Image | Size: {random_sample_original_image_with_boxes_size} (hxw)\")\n\n# Display image 2\naxes[1].imshow(random_sample_transformed_image_with_boxes)\naxes[1].axis(\"off\")  # Hide axes\naxes[1].set_title(f\"Transformed Image | Size: {random_sample_transformed_image_with_boxes_size} (hxw)\")\n\n# Show the plot\nplt.tight_layout()\nplt.show();\n\n\n\n\n\n\n\n\n\n\n12.5 TK - Create function to preprocess and transform batch of examples\n\nfrom torchvision import tv_tensors\n\ndef preprocess_and_transform_batch(examples,\n                                   image_processor,\n                                   transforms=None # Note: Could optionally add transforms (e.g. data augmentation) here \n                                   ):\n    \"\"\"\n    Function to preprocess batches of data.\n\n    Can optionally apply a transform later on.\n    \"\"\"\n    images = []\n    \n    coco_annotations = [] \n\n    for image, image_id, annotations_dict in zip(examples[\"image\"], examples[\"image_id\"], examples[\"annotations\"]):\n        # Note: may need to open image if it is an image path rather than PIL.Image\n        bbox_list = annotations_dict[\"bbox\"]\n        category_list = annotations_dict[\"category_id\"]\n        area_list = annotations_dict[\"area\"]\n    \n        # Note: Could optionally apply a transform here.\n        if transforms:\n            width, height = image.size[0], image.size[1]\n            bbox_list = tv_tensors.BoundingBoxes(data=torch.tensor(bbox_list),\n                                                 format=\"XYWH\",\n                                                 canvas_size=(height, width)) # canvas_size = height, width\n            image, bbox_list = transforms(image, \n                                          bbox_list)\n\n        # Format the annotations into COCO format\n        cooc_format_annotations = format_image_annotations_as_coco(image_id=image_id,\n                                                                   categories=category_list,\n                                                                   areas=area_list,\n                                                                   bboxes=bbox_list)\n        \n        # Add images/annotations to their respective lists\n        images.append(image)\n        coco_annotations.append(cooc_format_annotations)\n\n    \n    # Apply the image processor to lists of images and annotations\n    preprocessed_batch = image_processor.preprocess(images=images,\n                                                    annotations=coco_annotations,\n                                                    return_tensors=\"pt\",\n                                                    do_rescale=False if transforms else True,\n                                                    do_resize=True,\n                                                    do_pad=True)\n    \n    return preprocessed_batch\n\n\nfrom functools import partial\n\n# Make a transform for different splits\ntrain_transform_batch = partial(\n    preprocess_and_transform_batch,\n    transforms=train_transforms,\n    image_processor=image_processor\n)\n\nvalidation_transform_batch = partial(\n    preprocess_and_transform_batch,\n    transforms=None,\n    image_processor=image_processor\n)\n\n\nprocessed_dataset = dataset.copy()\nprocessed_dataset[\"train\"] = dataset[\"train\"].with_transform(train_transform_batch)\nprocessed_dataset[\"validation\"] = dataset[\"validation\"].with_transform(validation_transform_batch)\nprocessed_dataset[\"test\"] = dataset[\"test\"].with_transform(validation_transform_batch)\n\n\n# Create data_collate_function to collect samples into batches\n# TK - want to get a dictionary of {\"pixel_mask\": [batch_of_samples], \"labels\": [batch_of_samples], \"pixel_mask\": [batch_of_samples]}\ndef data_collate_function(batch):\n    collated_data = {} \n\n    # Stack together a collection of pixel_values tensors\n    collated_data[\"pixel_values\"] = torch.stack([sample[\"pixel_values\"] for sample in batch])\n\n    # Get the labels (these are dictionaries so no need to use torch.stack)\n    collated_data[\"labels\"] = [sample[\"labels\"] for sample in batch]\n\n    # If there is a pixel_mask key, return the pixel_mask's as well\n    if \"pixel_mask\" in batch[0]:\n        collated_data[\"pixel_mask\"] = torch.stack([sample[\"pixel_mask\"] for sample in batch])\n\n    return collated_data\n\n\nmodel_aug = create_model()\nmodel_aug\n\nSome weights of ConditionalDetrForObjectDetection were not initialized from the model checkpoint at microsoft/conditional-detr-resnet-50 and are newly initialized because the shapes did not match:\n- class_labels_classifier.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([7]) in the model instantiated\n- class_labels_classifier.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nConditionalDetrForObjectDetection(\n  (model): ConditionalDetrModel(\n    (backbone): ConditionalDetrConvModel(\n      (conv_encoder): ConditionalDetrConvEncoder(\n        (model): FeatureListNet(\n          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n          (bn1): ConditionalDetrFrozenBatchNorm2d()\n          (act1): ReLU(inplace=True)\n          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n          (layer1): Sequential(\n            (0): Bottleneck(\n              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n              (downsample): Sequential(\n                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (1): ConditionalDetrFrozenBatchNorm2d()\n              )\n            )\n            (1): Bottleneck(\n              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (2): Bottleneck(\n              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n          )\n          (layer2): Sequential(\n            (0): Bottleneck(\n              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n              (downsample): Sequential(\n                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n                (1): ConditionalDetrFrozenBatchNorm2d()\n              )\n            )\n            (1): Bottleneck(\n              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (2): Bottleneck(\n              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (3): Bottleneck(\n              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n          )\n          (layer3): Sequential(\n            (0): Bottleneck(\n              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n              (downsample): Sequential(\n                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n                (1): ConditionalDetrFrozenBatchNorm2d()\n              )\n            )\n            (1): Bottleneck(\n              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (2): Bottleneck(\n              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (3): Bottleneck(\n              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (4): Bottleneck(\n              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (5): Bottleneck(\n              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n          )\n          (layer4): Sequential(\n            (0): Bottleneck(\n              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n              (downsample): Sequential(\n                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n                (1): ConditionalDetrFrozenBatchNorm2d()\n              )\n            )\n            (1): Bottleneck(\n              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (2): Bottleneck(\n              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n          )\n        )\n      )\n      (position_embedding): ConditionalDetrSinePositionEmbedding()\n    )\n    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n    (query_position_embeddings): Embedding(300, 256)\n    (encoder): ConditionalDetrEncoder(\n      (layers): ModuleList(\n        (0-5): 6 x ConditionalDetrEncoderLayer(\n          (self_attn): DetrAttention(\n            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): ReLU()\n          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (decoder): ConditionalDetrDecoder(\n      (layers): ModuleList(\n        (0): ConditionalDetrDecoderLayer(\n          (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)\n          (self_attn): ConditionalDetrAttention(\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)\n          (encoder_attn): ConditionalDetrAttention(\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        )\n        (1-5): 5 x ConditionalDetrDecoderLayer(\n          (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)\n          (self_attn): ConditionalDetrAttention(\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_qpos_proj): None\n          (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)\n          (encoder_attn): ConditionalDetrAttention(\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (query_scale): MLP(\n        (layers): ModuleList(\n          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n        )\n      )\n      (ref_point_head): MLP(\n        (layers): ModuleList(\n          (0): Linear(in_features=256, out_features=256, bias=True)\n          (1): Linear(in_features=256, out_features=2, bias=True)\n        )\n      )\n    )\n  )\n  (class_labels_classifier): Linear(in_features=256, out_features=7, bias=True)\n  (bbox_predictor): ConditionalDetrMLPPredictionHead(\n    (layers): ModuleList(\n      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n      (2): Linear(in_features=256, out_features=4, bias=True)\n    )\n  )\n)\n\n\n\n# Note: Depending on the size/speed of your GPU, this may take a while\n\nfrom transformers import TrainingArguments, Trainer\n\n# Set the batch size according to the memory you have available on your GPU\n# e.g. on my NVIDIA RTX 4090 with 24GB of VRAM, I can use a batch size of 32 without running out of memory\nBATCH_SIZE = 16\n\n# Disable warnings about `max_size` parameter being deprecated (this is okay)\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\"The `max_size` parameter is deprecated*\")\n\n# Note: AdamW Optimizer is used by default\ntraining_args = TrainingArguments(\n    output_dir=\"detr_finetuned_trashify_box_detector_with_data_aug\", # Tk - make sure this is suitable for data aug model\n    num_train_epochs=25,\n    fp16=True,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    learning_rate=1e-4,\n    lr_scheduler_type=\"linear\", # default = \"linear\", can try others such as \"cosine\", \"constant\" etc\n    weight_decay=1e-4,\n    max_grad_norm=0.01,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    save_total_limit=2,\n    remove_unused_columns=False,\n    report_to=\"none\", # don't save experiments to a third party service\n    dataloader_num_workers=4,\n    warmup_ratio=0.05,\n    push_to_hub=False,\n    eval_do_concat_batches=False\n)\n\nmodel_v2_trainer = Trainer(\n    model=model_aug,\n    args=training_args,\n    train_dataset=processed_dataset[\"train\"],\n    eval_dataset=processed_dataset[\"validation\"],\n    tokenizer=image_processor,\n    data_collator=data_collate_function,\n    # compute_metrics=None # TODO: add a metrics function, just see if model trains first\n)\n\nmodel_v2_results = model_v2_trainer.train()\n\n/home/daniel/miniconda3/envs/ai/lib/python3.11/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n\n\n\n    \n      \n      \n      [1250/1250 08:19, Epoch 25/25]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\n\n\n\n\n1\n100.473500\n8.029722\n\n\n2\n4.369000\n2.737582\n\n\n3\n2.551800\n2.183892\n\n\n4\n2.222600\n1.922801\n\n\n5\n1.990600\n1.740759\n\n\n6\n1.821900\n1.557272\n\n\n7\n1.697400\n1.477890\n\n\n8\n1.602700\n1.451024\n\n\n9\n1.551700\n1.371128\n\n\n10\n1.449100\n1.317680\n\n\n11\n1.433500\n1.281066\n\n\n12\n1.364500\n1.247493\n\n\n13\n1.331400\n1.206003\n\n\n14\n1.297300\n1.187397\n\n\n15\n1.250600\n1.179421\n\n\n16\n1.231900\n1.165661\n\n\n17\n1.147900\n1.129974\n\n\n18\n1.146600\n1.117911\n\n\n19\n1.113800\n1.109535\n\n\n20\n1.115300\n1.096120\n\n\n21\n1.089400\n1.078995\n\n\n22\n1.069100\n1.087004\n\n\n23\n1.061900\n1.080366\n\n\n24\n1.045900\n1.071728\n\n\n25\n1.036300\n1.070385\n\n\n\n\n\n\nTK - Note: You might get the following issue (negative bounding box coordinate predictions), can try again for more stable predictions (predictions are inherently random to begin with) or use a learning rate warmup to help stabilize predictions:\n\nValueError: boxes1 must be in [x0, y0, x1, y1] (corner) format, but got tensor([[ 0.5796, 0.5566, 0.9956, 0.9492], [ 0.5718, 0.0610, 0.7202, 0.1738], [ 0.8218, 0.5107, 0.9878, 0.6289], ‚Ä¶, [ 0.1379, 0.1403, 0.6709, 0.6138], [ 0.7471, 0.4319, 1.0088, 0.5864], [-0.0660, 0.2052, 0.2067, 0.5107]], device=‚Äòcuda:0‚Äô, dtype=torch.float16)\n\n\n\n12.6 TK - Save the trained model\n\n# Save the model\nfrom datetime import datetime\n\n# TODO: update this save path so we know when the model was saved and what its parameters were\ntraining_epochs_ = training_args.num_train_epochs\nlearning_rate_ = \"{:.0e}\".format(training_args.learning_rate)\n\nmodel_v2_save_path = f\"models/learn_hf_microsoft_detr_finetuned_trashify_box_dataset_only_manual_data_with_aug_{training_epochs_}_epochs_lr_{learning_rate_}\"\nprint(f\"[INFO] Saving model to: {model_v2_save_path}\")\nmodel_v2_trainer.save_model(model_v2_save_path)\n\n[INFO] Saving model to: models/learn_hf_microsoft_detr_finetuned_trashify_box_dataset_only_manual_data_with_aug_25_epochs_lr_1e-04",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#tk---upload-augmentation-model-to-hugging-face-hub",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#tk---upload-augmentation-model-to-hugging-face-hub",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "13 TK - Upload Augmentation Model to Hugging Face Hub",
    "text": "13 TK - Upload Augmentation Model to Hugging Face Hub\n\n# Push the model to the Hugging Face Hub\n# TK Note: This will require you to have your Hugging Face account setup (e.g. see the setup guide, tk - link to setup guide)\n# TK - this will push to the parameter `output_dir=\"detr_finetuned_trashify_box_detector_with_data_aug\"`\nmodel_v2_trainer.push_to_hub(commit_message=\"upload trashify object detection model with data augmentation\"\n                             # token=None, # Optional to add token manually\n                            )\n\n\n\n\n\n\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/mrdbourke/detr_finetuned_trashify_box_detector_with_data_aug/commit/2f5f3ed0a205b13ddf2a0e3b76120412e33b0861', commit_message='upload trashify object detection model with data augmentation', commit_description='', oid='2f5f3ed0a205b13ddf2a0e3b76120412e33b0861', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mrdbourke/detr_finetuned_trashify_box_detector_with_data_aug', endpoint='https://huggingface.co', repo_type='model', repo_id='mrdbourke/detr_finetuned_trashify_box_detector_with_data_aug'), pr_revision=None, pr_num=None)",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#tk---compare-results-of-different-models",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#tk---compare-results-of-different-models",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "14 TK - Compare results of different models",
    "text": "14 TK - Compare results of different models\nUPTOHERE - Showcase model 2 doing better because of augmentation (harder to learn)\n\nTK - Compare v1 model to v2\n\nTK - Get model_v1 results into a variable and save it for later\nCompare both of these as plots against each other, e.g.¬†have the training curves for aug/no_aug on one plot and the curves for validation data for aug/no_aug on another plot\n\nTK - offer extensions to improve the model\n\nTK - training model for longer, potentially using synthetic data‚Ä¶?\n\nTK - could I use 1000 high quality synthetic data samples to improve our model?\n\nTK - try use a different learning rate\n\n\n\n# TK - Turn this workflow into a function e.g. def get_history_from_trainer() -&gt; df/dict of history\ndef get_history_metrics_from_trainer(trainer):\n    trainer_history = trainer.state.log_history \n    trainer_history_metrics = trainer_history[:-1] # get everything except the training time metrics (we've seen these already)\n    trainer_history_training_time = trainer_history[-1]\n\n    model_train_loss = [item[\"loss\"] for item in trainer_history_metrics if \"loss\" in item.keys()]\n    model_eval_loss = [item[\"eval_loss\"] for item in trainer_history_metrics if \"eval_loss\" in item.keys()]\n    model_learning_rate = [item[\"learning_rate\"] for item in trainer_history_metrics if \"learning_rate\" in item.keys()] \n\n    return model_train_loss, model_eval_loss, model_learning_rate, trainer_history_training_time\n\nmodel_v1_train_loss, model_v1_eval_loss, model_v1_learning_rate, _ = get_history_metrics_from_trainer(trainer=model_v1_trainer)\nmodel_v2_train_loss, model_v2_eval_loss, model_v2_learning_rate, _ = get_history_metrics_from_trainer(trainer=model_v2_trainer)\n\n\nimport matplotlib.pyplot as plt\n\n# Plot model loss curves against each other for same model\n# Note: Start from index 1 onwards to remove large loss spike at beginning of training \nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\nax[0].plot(model_v1_train_loss[1:], label=\"Model V1 Train Loss\")\nax[0].plot(model_v1_eval_loss[1:], label=\"Model V1 Eval Loss\")\nax[0].set_title(\"Model V1 Loss Curves\")\nax[0].set_ylabel(\"Loss\")\nax[0].set_xlabel(\"Epoch\")\nax[0].legend()\n\nax[1].plot(model_v2_train_loss[1:], label=\"Model V2 Train Loss\")\nax[1].plot(model_v2_eval_loss[1:], label=\"Model V2 Eval Loss\")\nax[1].set_title(\"Model V2 Loss Curves\")\nax[1].set_ylabel(\"Loss\")\nax[1].set_xlabel(\"Epoch\")\nax[1].legend();\n\n\n\n\n\n\n\n\ntk - notice the overfitting begin to happen with model v1 (no data augmentation) but model v2 has less overfitting and achieves a lower validation loss\n\nimport matplotlib.pyplot as plt\n\nplt.plot(model_v1_learning_rate, label=\"Model V1\")\nplt.plot(model_v2_learning_rate, label=\"Model V2\")\nplt.title(\"Model Learning Rate vs. Epoch\")\nplt.ylabel(\"Learning Rate\")\nplt.xlabel(\"Epoch\")\nplt.legend();\n\n\n\n\n\n\n\n\n\n# Plot loss values against each other\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\nnum_epochs = range(0, len(model_v1_train_loss))\nax[0].plot(model_v1_train_loss[1:], label=\"Model 1 Training Loss\")\nax[0].plot(model_v2_train_loss[1:], label=\"Model 2 Training Loss\")\nax[0].set_title(\"Model Training Loss Curves\")\nax[0].set_ylabel(\"Training Loss\")\nax[0].set_xlabel(\"Epochs\")\nax[0].legend()\n\nax[1].plot(model_v1_eval_loss[1:], label=\"Model 1 Eval Loss\")\nax[1].plot(model_v2_eval_loss[1:], label=\"Model 2 Eval Loss\")\nax[1].set_title(\"Model Eval Loss Curves\")\nax[1].set_ylabel(\"Eval Loss\")\nax[1].set_xlabel(\"Epochs\")\nax[1].legend();\n\n\n\n\n\n\n\n\ntk - describe the loss curves here, model 2 curves may be higher for training loss but they really start to accelerate on the evaluation set towards the end",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#tk---create-demo-with-augmentation-model",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#tk---create-demo-with-augmentation-model",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "15 TK - Create demo with Augmentation Model",
    "text": "15 TK - Create demo with Augmentation Model\n\n# Make directory for demo\nfrom pathlib import Path\n\ntrashify_data_aug_model_dir = Path(\"demos/trashify_object_detector_data_aug_model/\")\ntrashify_data_aug_model_dir.mkdir(exist_ok=True)\n\n\n%%writefile demos/trashify_object_detector_data_aug_model/README.md\n---\ntitle: Trashify Demo V2 üöÆ\nemoji: üóëÔ∏è\ncolorFrom: purple\ncolorTo: blue\nsdk: gradio\nsdk_version: 4.40.0\napp_file: app.py\npinned: false\nlicense: apache-2.0\n---\n\n# üöÆ Trashify Object Detector Demo V2\n\nObject detection demo to detect `trash`, `bin`, `hand`, `trash_arm`, `not_trash`, `not_bin`, `not_hand`. \n\nUsed as example for encouraging people to cleanup their local area.\n\nIf `trash`, `hand`, `bin` all detected = +1 point.\n\n## Dataset\n\nAll Trashify models are trained on a custom hand-labelled dataset of people picking up trash and placing it in a bin.\n\nThe dataset can be found on Hugging Face as [`mrdbourke/trashify_manual_labelled_images`](https://huggingface.co/datasets/mrdbourke/trashify_manual_labelled_images).\n\n## Demos\n\n* [V1](https://huggingface.co/spaces/mrdbourke/trashify_demo_v1) = Fine-tuned DETR model trained *without* data augmentation.\n* [V2](https://huggingface.co/spaces/mrdbourke/trashify_demo_v2) = Fine-tuned DETR model trained *with* data augmentation.\n* [V3](https://huggingface.co/spaces/mrdbourke/trashify_demo_v3) = Fine-tuned DETR model trained *with* data augmentation (same as V2) with an NMS (Non Maximum Suppression) post-processing step.\n\nTK - finish the README.md + update with links to materials\n\nOverwriting demos/trashify_object_detector_data_aug_model/README.md\n\n\n\n%%writefile demos/trashify_object_detector_data_aug_model/requirements.txt\ntimm\ngradio\ntorch\ntransformers\n\nOverwriting demos/trashify_object_detector_data_aug_model/requirements.txt\n\n\n\n%%writefile demos/trashify_object_detector_data_aug_model/app.py\nimport gradio as gr\nimport torch\nfrom PIL import Image, ImageDraw, ImageFont\n\nfrom transformers import AutoImageProcessor\nfrom transformers import AutoModelForObjectDetection\n\n# Note: Can load from Hugging Face or can load from local.\n# You will have to replace {mrdbourke} for your own username if the model is on your Hugging Face account.\nmodel_save_path = \"mrdbourke/detr_finetuned_trashify_box_detector_with_data_aug\" \n\n# Load the model and preprocessor\nimage_processor = AutoImageProcessor.from_pretrained(model_save_path)\nmodel = AutoModelForObjectDetection.from_pretrained(model_save_path)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(device)\n\n# Get the id2label dictionary from the model\nid2label = model.config.id2label\n\n# Set up a colour dictionary for plotting boxes with different colours\ncolor_dict = {   \n    \"bin\": \"green\",\n    \"trash\": \"blue\",\n    \"hand\": \"purple\",\n    \"trash_arm\": \"yellow\",\n    \"not_trash\": \"red\",\n    \"not_bin\": \"red\",\n    \"not_hand\": \"red\",\n}\n\n# Create helper functions for seeing if items from one list are in another \ndef any_in_list(list_a, list_b):\n    \"Returns True if any item from list_a is in list_b, otherwise False.\"\n    return any(item in list_b for item in list_a)\n\ndef all_in_list(list_a, list_b):\n    \"Returns True if all items from list_a are in list_b, otherwise False.\"\n    return all(item in list_b for item in list_a)\n\ndef predict_on_image(image, conf_threshold):\n    with torch.no_grad():\n        inputs = image_processor(images=[image], return_tensors=\"pt\")\n        outputs = model(**inputs.to(device))\n\n        target_sizes = torch.tensor([[image.size[1], image.size[0]]]) # height, width \n\n        results = image_processor.post_process_object_detection(outputs,\n                                                                threshold=conf_threshold,\n                                                                target_sizes=target_sizes)[0]\n    # Return all items in results to CPU\n    for key, value in results.items():\n        try:\n            results[key] = value.item().cpu() # can't get scalar as .item() so add try/except block\n        except:\n            results[key] = value.cpu()\n\n    # Can return results as plotted on a PIL image (then display the image)\n    draw = ImageDraw.Draw(image)\n\n    # Get a font from ImageFont\n    font = ImageFont.load_default(size=20)\n\n    # Get class names as text for print out\n    class_name_text_labels = []\n\n    for box, score, label in zip(results[\"boxes\"], results[\"scores\"], results[\"labels\"]):\n        # Create coordinates\n        x, y, x2, y2 = tuple(box.tolist())\n\n        # Get label_name\n        label_name = id2label[label.item()]\n        targ_color = color_dict[label_name]\n        class_name_text_labels.append(label_name)\n\n        # Draw the rectangle\n        draw.rectangle(xy=(x, y, x2, y2), \n                       outline=targ_color,\n                       width=3)\n        \n        # Create a text string to display\n        text_string_to_show = f\"{label_name} ({round(score.item(), 3)})\"\n\n        # Draw the text on the image\n        draw.text(xy=(x, y),\n                  text=text_string_to_show,\n                  fill=\"white\",\n                  font=font)\n    \n    # Remove the draw each time\n    del draw\n\n    # Setup blank string to print out\n    return_string = \"\"\n\n    # Setup list of target items to discover\n    target_items = [\"trash\", \"bin\", \"hand\"]\n\n    # If no items detected or trash, bin, hand not in list, return notification \n    if (len(class_name_text_labels) == 0) or not (any_in_list(list_a=target_items, list_b=class_name_text_labels)):\n        return_string = f\"No trash, bin or hand detected at confidence threshold {conf_threshold}. Try another image or lowering the confidence threshold.\"\n        return image, return_string\n\n    # If there are some missing, print the ones which are missing\n    elif not all_in_list(list_a=target_items, list_b=class_name_text_labels):\n        missing_items = []\n        for item in target_items:\n            if item not in class_name_text_labels:\n                missing_items.append(item)\n        return_string = f\"Detected the following items: {class_name_text_labels}. But missing the following in order to get +1: {missing_items}. If this is an error, try another image or altering the confidence threshold. Otherwise, the model may need to be updated with better data.\"\n        \n    # If all 3 trash, bin, hand occur = + 1\n    if all_in_list(list_a=target_items, list_b=class_name_text_labels):\n        return_string = f\"+1! Found the following items: {class_name_text_labels}, thank you for cleaning up the area!\"\n\n    print(return_string)\n    \n    return image, return_string\n\n# Create the interface\ndemo = gr.Interface(\n    fn=predict_on_image,\n    inputs=[\n        gr.Image(type=\"pil\", label=\"Target Image\"),\n        gr.Slider(minimum=0, maximum=1, value=0.25, label=\"Confidence Threshold\")\n    ],\n    outputs=[\n        gr.Image(type=\"pil\", label=\"Image Output\"),\n        gr.Text(label=\"Text Output\")\n    ],\n    title=\"üöÆ Trashify Object Detection Demo V2\",\n    description=\"\"\"Help clean up your local area! Upload an image and get +1 if there is all of the following items detected: trash, bin, hand.\n\n    The [model](https://huggingface.co/mrdbourke/detr_finetuned_trashify_box_detector_with_data_aug) in V2 has been trained with data augmentation preprocessing (color jitter, horizontal flipping) to improve robustness. \n    \"\"\",\n    # Examples come in the form of a list of lists, where each inner list contains elements to prefill the `inputs` parameter with\n    examples=[\n        [\"examples/trashify_example_1.jpeg\", 0.25],\n        [\"examples/trashify_example_2.jpeg\", 0.25],\n        [\"examples/trashify_example_3.jpeg\", 0.25]\n    ],\n    cache_examples=True\n)\n\n# Launch the demo\ndemo.launch()\n\nOverwriting demos/trashify_object_detector_data_aug_model/app.py\n\n\n\n# 1. Import the required methods for uploading to the Hugging Face Hub\nfrom huggingface_hub import (\n    create_repo,\n    get_full_repo_name,\n    upload_file, # for uploading a single file (if necessary)\n    upload_folder # for uploading multiple files (in a folder)\n)\n\n# 2. Define the parameters we'd like to use for the upload\nLOCAL_DEMO_FOLDER_PATH_TO_UPLOAD = \"demos/trashify_object_detector_data_aug_model\" # TK - update this path \nHF_TARGET_SPACE_NAME = \"trashify_demo_v2\"\nHF_REPO_TYPE = \"space\" # we're creating a Hugging Face Space\nHF_SPACE_SDK = \"gradio\"\nHF_TOKEN = \"\" # optional: set to your Hugging Face token (but I'd advise storing this as an environment variable as previously discussed)\n\n# 3. Create a Space repository on Hugging Face Hub \nprint(f\"[INFO] Creating repo on Hugging Face Hub with name: {HF_TARGET_SPACE_NAME}\")\ncreate_repo(\n    repo_id=HF_TARGET_SPACE_NAME,\n    # token=HF_TOKEN, # optional: set token manually (though it will be automatically recognized if it's available as an environment variable)\n    repo_type=HF_REPO_TYPE,\n    private=False, # set to True if you don't want your Space to be accessible to others\n    space_sdk=HF_SPACE_SDK,\n    exist_ok=True, # set to False if you want an error to raise if the repo_id already exists \n)\n\n# 4. Get the full repository name (e.g. {username}/{model_id} or {username}/{space_name})\nfull_hf_repo_name = get_full_repo_name(model_id=HF_TARGET_SPACE_NAME)\nprint(f\"[INFO] Full Hugging Face Hub repo name: {full_hf_repo_name}\")\n\n# 5. Upload our demo folder\nprint(f\"[INFO] Uploading {LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD} to repo: {full_hf_repo_name}\")\nfolder_upload_url = upload_folder(\n    repo_id=full_hf_repo_name,\n    folder_path=LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD,\n    path_in_repo=\".\", # upload our folder to the root directory (\".\" means \"base\" or \"root\", this is the default)\n    # token=HF_TOKEN, # optional: set token manually\n    repo_type=HF_REPO_TYPE,\n    commit_message=\"Uploading Trashify V2 box detection model (with data augmentation) app.py\"\n)\nprint(f\"[INFO] Demo folder successfully uploaded with commit URL: {folder_upload_url}\")\n\n[INFO] Creating repo on Hugging Face Hub with name: trashify_demo_v2\n[INFO] Full Hugging Face Hub repo name: mrdbourke/trashify_demo_v2\n[INFO] Uploading demos/trashify_object_detector_data_aug_model to repo: mrdbourke/trashify_demo_v2\n[INFO] Demo folder successfully uploaded with commit URL: https://huggingface.co/spaces/mrdbourke/trashify_demo_v2/tree/main/.\n\n\n\n# Next:\n# Upload augmentation model to Hugging Face Hub ‚úÖ\n# Create demo for augmentation model ‚úÖ\n# Compare results from augmentation model to non-augmentation model ‚úÖ\n\n\n15.1 TK - Make a prediction on a random test sample with model using data aug model\n\n# Get a random sample from the test preds\nrandom_test_pred_index = random.randint(0, len(processed_dataset[\"test\"]))\nprint(f\"[INFO] Making predictions on test item with index: {random_test_pred_index}\")\n\nrandom_test_sample = processed_dataset[\"test\"][random_test_pred_index]\n\n# # Do a single forward pass with the model\nrandom_test_sample_outputs = model_aug(pixel_values=random_test_sample[\"pixel_values\"].unsqueeze(0).to(\"cuda\"), # model expects input [batch_size, color_channels, height, width]\n                                       pixel_mask=None)\n\n# Post process a random item from test preds\nrandom_test_sample_outputs_post_processed = image_processor.post_process_object_detection(\n    outputs=random_test_sample_outputs,\n    threshold=0.25, # prediction probability threshold for boxes (note: boxes from an untrained model will likely be bad)\n    target_sizes=[random_test_sample[\"labels\"][\"orig_size\"]] # original input image size (or whichever target size you'd like), required to be same number of input items in a list\n)\n\n# Plot the random sample test preds\n# Extract scores, labels and boxes\nrandom_test_sample_pred_scores = random_test_sample_outputs_post_processed[0][\"scores\"]\nrandom_test_sample_pred_labels = random_test_sample_outputs_post_processed[0][\"labels\"]\nrandom_test_sample_pred_boxes = random_test_sample_outputs_post_processed[0][\"boxes\"]\n\n# Create a list of labels to plot on the boxes \nrandom_test_sample_labels_to_plot = [f\"Pred: {id2label[label_pred.item()]} ({round(score_pred.item(), 4)})\" \n                  for label_pred, score_pred in zip(random_test_sample_pred_labels, random_test_sample_pred_scores)]\n\nprint(f\"[INFO] Labels with scores: {random_test_sample_labels_to_plot}\")\nprint(f\"[INFO] Boxes:\")\nfor item in random_test_sample_pred_boxes:\n    print(item.detach().cpu())\nprint(f\"[INFO] Total preds: {len(random_test_sample_labels_to_plot)}\")\n\n# Plot the predicted boxes on the random test image \nto_pil_image(\n    pic=draw_bounding_boxes(\n        image=pil_to_tensor(pic=dataset[\"test\"][random_test_pred_index][\"image\"]),\n        boxes=random_test_sample_pred_boxes,\n        labels=random_test_sample_labels_to_plot,\n        width=3\n    )\n)\n\n[INFO] Making predictions on test item with index: 163\n[INFO] Labels with scores: ['Pred: bin (0.6625)', 'Pred: hand (0.5412)', 'Pred: trash (0.5007)', 'Pred: trash (0.4147)', 'Pred: trash (0.396)', 'Pred: not_trash (0.3237)', 'Pred: hand (0.2799)']\n[INFO] Boxes:\ntensor([  10.7812,  393.1250,  950.1562, 1160.6250])\ntensor([ 149.8828,  667.9688,  471.6797, 1018.2812])\ntensor([405.0000, 679.1406, 668.4375, 972.1094])\ntensor([248.2031, 472.6562, 675.7031, 994.8438])\ntensor([ 140.6250,  467.3438,  675.9375, 1002.6562])\ntensor([ 373.2422,  896.4844,  648.6328, 1063.5156])\ntensor([  10.3125,  667.9688,  472.0312, 1264.5312])\n[INFO] Total preds: 7",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#tk---model-v3---cleaning-up-predictions-with-nms-non-max-suppression",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#tk---model-v3---cleaning-up-predictions-with-nms-non-max-suppression",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "16 TK - Model V3 - Cleaning up predictions with NMS (Non-max Suppression)",
    "text": "16 TK - Model V3 - Cleaning up predictions with NMS (Non-max Suppression)\nUPTOHERE * Take preds from model v2 and perform NMS on them to see what happens * Need to calculate: * IoU (intersection over union) * Can write about these in a blog post as extension material * Test image index good to practice on: * 163, 108 * Create a demo which compares NMS-free boxes to boxes with NMS\n\n16.1 TK - NMS filtering logic to do\nTK - create a table of different items here\n\nSimplest filtering: keep only 1x class label with the highest score per image (e.g.¬†if there are two ‚Äúhand‚Äù predictions, keep only the highest scoring one) ‚úÖ\n\nTK - problem with simple filtering is that it might take out a box that would‚Äôve been helpful, it also assumes that there‚Äôs little false positives (e.g.¬†each box is predicting the class that it should predict)\n\nGreedy IoU filtering: Filter boxes which have IoU &gt; 0.9 (big overlap) and keep the box with the higher score ‚úÖ\n\nTK - problem here is that it may filter heavily overlapping classes (e.g.¬†if there are many boxes of different classes clustered together because your objects overlap, such as on a plate of food, items may overlap)\n\nClass-aware IoU filtering: Filter boxes which have the same label and have IoU &gt; 0.5 and keep the box with the higher score\n\nOther potential NMS options: * Greedy NMS (good for distinct boxes, just take the highest scoring box per class) * Soft-NMS with linear penalty (good for boxes which may have overlap, e.g.¬†smaller boxes in clusters) * Class-aware NMS (only perform NMS on same class of boxes)\n\nSee this video here: https://youtu.be/VAo84c1hQX8?si=dYftsYADb9Kq-bul\nTK - show prediction with more boxes than ideal, then introduce NMS as a technique to fix the predictions (e.g.¬†on the same sample)\n\nTK - NMS doesn‚Äôt need an extra model, just a way to\n\nTK - test index 163 is a good example with many boxes that could be shortened to a few\n\n\n\n16.2 TK - Simple NMS - Keep only highest scoring class per prediction\nTK - This is the simplest method and simply iterates through the boxes and keep the highest scoring box per class (e.g.¬†if there are two ‚Äúhand‚Äù prediction boxes, only keep the higher scoring one).\n\ndef filter_highest_scoring_box_per_class(boxes, labels, scores):\n    \"\"\"\n    Perform NMS (Non-max Supression) to only keep the top scoring box per class.\n\n    Args:\n        boxes: tensor of shape (N, 4)\n        labels: tensor of shape (N,)\n        scores: tensor of shape (N,)\n    Returns:\n        boxes: tensor of shape (N, 4) filtered for max scoring item per class\n        labels: tensor of shape (N,) filtered for max scoring item per class\n        scores: tensor of shape (N,) filtered for max scoring item per class\n    \"\"\"\n    # Start with a blank keep mask (e.g. all False and then update the boxes to keep with True)\n    keep_mask = torch.zeros(len(boxes), dtype=torch.bool)\n\n    # For each unique class\n    for class_id in labels.unique():\n        # Get the indicies for the target class\n        class_mask = labels == class_id\n\n        # If any of the labels match the current class_id\n        if class_mask.any():\n            # Find the index of highest scoring box for this specific class\n            class_scores = scores[class_mask]\n            highest_score_idx = class_scores.argmax()\n\n            # Convert back to the original index\n            original_idx = torch.where(class_mask)[0][highest_score_idx]\n\n            # Update the index in the keep mask to keep the highest scoring box \n            keep_mask[original_idx] = True\n        \n    return boxes[keep_mask], labels[keep_mask], scores[keep_mask]\n\n\n# Mask with simple NMS keep mask\nkeep_boxes, keep_labels, keep_scores = filter_highest_scoring_box_per_class(boxes=random_test_sample_pred_boxes,\n                                                                            labels=random_test_sample_pred_labels,\n                                                                            scores=random_test_sample_pred_scores)\n\nprint(len(random_test_sample_pred_boxes), len(random_test_sample_pred_labels), len(random_test_sample_pred_scores))\nprint(len(keep_scores), len(keep_labels), len(keep_boxes))\n\n7 7 7\n4 4 4\n\n\n\nkeep_boxes, keep_labels, keep_scores\n\n(tensor([[  10.7812,  393.1250,  950.1562, 1160.6250],\n         [ 149.8828,  667.9688,  471.6797, 1018.2812],\n         [ 405.0000,  679.1406,  668.4375,  972.1094],\n         [ 373.2422,  896.4844,  648.6328, 1063.5156]], device='cuda:0',\n        grad_fn=&lt;IndexBackward0&gt;),\n tensor([0, 1, 5, 4], device='cuda:0'),\n tensor([0.6625, 0.5412, 0.5007, 0.3237], device='cuda:0',\n        grad_fn=&lt;IndexBackward0&gt;))\n\n\n\n# Create a list of labels to plot on the boxes \nrandom_test_sample_labels_to_plot = [f\"Pred: {id2label[label_pred.item()]} ({round(score_pred.item(), 4)})\" \n                  for label_pred, score_pred in zip(random_test_sample_pred_labels, random_test_sample_pred_scores)]\n\nprint(f\"[INFO] Labels with scores: {random_test_sample_labels_to_plot}\")\n\n# Plot the predicted boxes on the random test image \ntest_image_with_preds_original = to_pil_image(\n    pic=draw_bounding_boxes(\n        image=pil_to_tensor(pic=dataset[\"test\"][random_test_pred_index][\"image\"]),\n        boxes=random_test_sample_pred_boxes,\n        labels=random_test_sample_labels_to_plot,\n        width=3\n    )\n)\n\n### Create image with filtered boxes\n\n# Create a list of labels to plot on the boxes \nrandom_test_sample_labels_to_plot_filtered = [f\"Pred: {id2label[label_pred.item()]} ({round(score_pred.item(), 4)})\" \n                  for label_pred, score_pred in zip(keep_labels, keep_scores)]\n\nprint(f\"[INFO] Labels with scores: {random_test_sample_labels_to_plot_filtered}\")\n\n# Plot the predicted boxes on the random test image \ntest_image_with_preds_filtered = to_pil_image(\n    pic=draw_bounding_boxes(\n        image=pil_to_tensor(pic=dataset[\"test\"][random_test_pred_index][\"image\"]),\n        boxes=keep_boxes,\n        labels=random_test_sample_labels_to_plot_filtered,\n        width=3\n    )\n)\n\n# Visualize the transformed image \nimport matplotlib.pyplot as plt\n\n# Create a figure with two subplots\nfig, axes = plt.subplots(1, 2, figsize=(20, 10))\n\n# Display image 1\naxes[0].imshow(test_image_with_preds_original)\naxes[0].axis(\"off\")  # Hide axes\naxes[0].set_title(f\"Original Image Preds (total: {len(random_test_sample_pred_boxes)})\")\n\n# Display image 2\naxes[1].imshow(test_image_with_preds_filtered)\naxes[1].axis(\"off\")  # Hide axes\naxes[1].set_title(f\"Filtered Image Preds (total: {len(keep_boxes)})\")\n\n# Show the plot\nplt.suptitle(\"Simple NMS - Only keep the highest scoring box per prediction\")\nplt.tight_layout()\nplt.show();\n\n[INFO] Labels with scores: ['Pred: bin (0.6625)', 'Pred: hand (0.5412)', 'Pred: trash (0.5007)', 'Pred: trash (0.4147)', 'Pred: trash (0.396)', 'Pred: not_trash (0.3237)', 'Pred: hand (0.2799)']\n[INFO] Labels with scores: ['Pred: bin (0.6625)', 'Pred: hand (0.5412)', 'Pred: trash (0.5007)', 'Pred: not_trash (0.3237)']\n\n\n\n\n\n\n\n\n\nTK - problem with simple filtering is that it might take out a box that would‚Äôve been helpful, it also assumes that there‚Äôs little false positives (e.g.¬†each box is predicting the class that it should predict)\n\n\n16.3 TK - Greedy IoU Filtering - Intersection over Union - If a pair of boxes have an IoU over a certain threshold, keep the box with the higher score\n\nIoU in torchmetrics - https://lightning.ai/docs/torchmetrics/stable/detection/intersection_over_union.html\n\nTo calculate the Intersection over Union (IoU) between two bounding boxes:\n\nCoordinates of the intersection rectangle: \\[\nx_{\\text{left}} = \\max(x_{1A}, x_{1B})\n\\] \\[\ny_{\\text{top}} = \\max(y_{1A}, y_{1B})\n\\] \\[\nx_{\\text{right}} = \\min(x_{2A}, x_{2B})\n\\] \\[\ny_{\\text{bottom}} = \\min(y_{2A}, y_{2B})\n\\]\n\nWhere:\n\\[\n   \\text{A} = \\text{Box 1}\n\\] \\[\n   \\text{B} = \\text{Box 2}\n\\]\n\nWidth and height of the intersection: \\[\n\\text{intersection\\_width} = \\max(0, x_{\\text{right}} - x_{\\text{left}})\n\\] \\[\n\\text{intersection\\_height} = \\max(0, y_{\\text{bottom}} - y_{\\text{top}})\n\\]\nArea of Overlap: \\[\n\\text{Area of Overlap} = \\text{intersection\\_width} \\times \\text{intersection\\_height}\n\\]\nArea of Union: \\[\n\\text{Area of Union} = \\text{Area of Box 1} + \\text{Area of Box 2} - \\text{Area of Overlap}\n\\]\nIntersection over Union (IoU): $$ = / \n\n\n# IoU = Intersection / Union\n# Inserction =\n    # x_left = max(x1_A, x1_B)\n    # y_top = max(y1_A, y1_B)\n    # x_right = min(x2_A, x2_B)\n    # y_bottom = min(y2_A, x2_B)\n    # \n    # Where: \n        # A = Box 1\n        # B = Box 2\n    # intersection_width = max(0, x_right - x_left)\n    # interesection_height = max(0, y_bottom - y_top)\n    # area_intersection = intersection_width * intersection_height\n# Union = area_box_1 + area_box_2 - intersection\n\ndef intersection_over_union_score(box_1, box_2):\n    \"\"\"Calculates Intersection over Union (IoU) score for two given boxes in XYXY format.\"\"\"\n    assert len(box_1) == 4, f\"Box 1 should have four elements in the format [x_1, y_1, x_2, y_2] but has: {len(box_1)}, see: {box_1}\"\n    assert len(box_2) == 4, f\"Box 2 should have four elements in the format [x_1, y_1, x_2, y_2] but has: {len(box_2)}, see: {box_2}\"\n\n    x1_box_1, y1_box_1, x2_box_1, y2_box_1 = box_1[0], box_1[1], box_1[2], box_1[3]\n    x1_box_2, y1_box_2, x2_box_2, y2_box_2 = box_2[0], box_2[1], box_2[2], box_2[3]\n\n    # Get coordinates of overlapping box (note: there may not be any overlapping box)\n    x_left = torch.max(x1_box_1, x1_box_2)\n    y_top = torch.max(y1_box_1, y1_box_2)\n    x_right = torch.min(x2_box_1, x2_box_2)\n    y_bottom = torch.min(y2_box_1, y2_box_2)\n\n    # Calculate the intersection width and height (we take the max of 0 and the value to find non-overlapping boxes)\n    intersection_width = max(0, x_right - x_left)\n    intersection_height = max(0, y_bottom - y_top)\n\n    # Calculate the area of intersection (note: this will 0 if either width or height are 0)\n    area_of_intersection = intersection_height * intersection_width\n\n    # Calculate individual box areas\n    box_1_area = (x2_box_1 - x1_box_1) * (y2_box_1 - y1_box_1) # width * height \n    box_2_area = (x2_box_2 - x1_box_2) * (y2_box_2 - y1_box_2)\n\n    # Calcuate area of union (sum of box areas minus the intersection area)\n    area_of_union = box_1_area + box_2_area - area_of_intersection\n\n    # Calculate the IoU score\n    iou_score = area_of_intersection / area_of_union\n\n    return iou_score\n\n\niou_score_test_pred_boxes = intersection_over_union_score(box_1=random_test_sample_pred_boxes[4],\n                                                          box_2=random_test_sample_pred_boxes[3])\n\nprint(f\"[INFO] IoU Score: {iou_score_test_pred_boxes}\")\n\n\nrandom_test_sample_pred_boxes[0], random_test_sample_pred_boxes[1]\n\n[INFO] IoU Score: 0.7790185809135437\n\n\n(tensor([  10.7812,  393.1250,  950.1562, 1160.6250], device='cuda:0',\n        grad_fn=&lt;SelectBackward0&gt;),\n tensor([ 149.8828,  667.9688,  471.6797, 1018.2812], device='cuda:0',\n        grad_fn=&lt;SelectBackward0&gt;))\n\n\n\n# TK - for visualization purposes, write code to highlight the intersecting points on a box and print the IoU score in the middle of the box\n\n# IoU logic\n    # 1. General IoU threshold (removing boxes at a global level, regardless of label)\n        # -&gt; for box pairs with IoU &gt; 0.9, keep the higher scoring box \n    # 2. Label specific IoU threshold (only concern is comparing boxes with the same label)\n        # -&gt; for box pairs with same label and IoU &gt; 0.5, keep the higher scoring box\n\n\nkeep_boxes = []\nkeep_scores = []\nkeep_labels = []\n\nrandom_test_sample_pred_scores = random_test_sample_outputs_post_processed[0][\"scores\"]\nrandom_test_sample_pred_labels = random_test_sample_outputs_post_processed[0][\"labels\"]\nrandom_test_sample_pred_boxes = random_test_sample_outputs_post_processed[0][\"boxes\"]\n\nkeep_indexes = torch.ones(len(random_test_sample_pred_boxes), dtype=torch.bool)\n\niou_general_threshold = 0.9 # general threshold = remove the lower scoring box in box pairs with over iou_general_threshold regardless of the label\niou_class_level_threshold = 0.5 # remove overlapping similar classes\n\n# TODO: Add a clause here to include if class labels are the same, then filter based on the class-specifc IoU threshold\nfilter_global = True\nfilter_same_label = True\n\n# Count the total loops\ntotal_loops = 0\n\nfor i, box_A in enumerate(random_test_sample_pred_boxes):\n    if not keep_indexes[i]: # insert clause to prevent calculating on already filtered labels\n        continue \n\n    for j, box_B in enumerate(random_test_sample_pred_boxes):\n        if not keep_indexes[i]:\n            continue\n\n        # Only calculate IoU score if indexes aren't the same (saves comparing the same index boxes for unwanted calculations)\n        if (i != j): \n            iou_score = intersection_over_union_score(box_1=box_A, box_2=box_B)\n            print(f\"[INFO] IoU Score for box {(i, j)}: {iou_score}\")\n\n            if filter_global:\n                if iou_score &gt; iou_general_threshold:\n                    score_A, score_B = random_test_sample_pred_scores[i], random_test_sample_pred_scores[j]\n                    if score_A &gt; score_B:\n                        print(f\"[INFO] Box to keep index: {i} -&gt; {box_A}\")\n                        keep_indexes[j] = False\n                    else:\n                        print(f\"[INFO] Box to keep index: {j} -&gt; {box_B}\")\n                        keep_indexes[i] = False\n                \n            if filter_same_label:\n                if iou_score &gt; iou_class_level_threshold:\n                    i_label = random_test_sample_pred_labels[i]\n                    j_label = random_test_sample_pred_labels[j]\n                    if i_label == j_label:\n                        print(f\"Labels are equal: {i_label, j_label}\")\n                        score_A, score_B = random_test_sample_pred_scores[i], random_test_sample_pred_scores[j]\n                        if score_A &gt; score_B:\n                            print(f\"[INFO] Box to keep index: {i} -&gt; {box_A}\")\n                            keep_indexes[j] = False\n                        else:\n                            print(f\"[INFO] Box to keep index: {j} -&gt; {box_B}\")\n                            keep_indexes[i] = False\n\n        total_loops += 1\n                \nprint(keep_indexes)\n\nkeep_scores = random_test_sample_pred_scores[keep_indexes]\nkeep_labels = random_test_sample_pred_labels[keep_indexes]\nkeep_boxes = random_test_sample_pred_boxes[keep_indexes]\n\nprint(len(random_test_sample_pred_boxes), len(random_test_sample_pred_labels), len(random_test_sample_pred_boxes))\nprint(len(keep_scores), len(keep_labels), len(keep_boxes), sum(keep_indexes))\n\nprint(f\"[INFO] Number of total loops: {total_loops}, max possible loops: {len(random_test_sample_pred_boxes)**2}\")\n\n[INFO] IoU Score for box (0, 1): 0.156358003616333\n[INFO] IoU Score for box (0, 2): 0.10704872757196426\n[INFO] IoU Score for box (0, 3): 0.3096315264701843\n[INFO] IoU Score for box (0, 4): 0.3974636495113373\n[INFO] IoU Score for box (0, 5): 0.06380129605531693\n[INFO] IoU Score for box (0, 6): 0.2954297661781311\n[INFO] IoU Score for box (1, 0): 0.156358003616333\n[INFO] IoU Score for box (1, 2): 0.11466032266616821\n[INFO] IoU Score for box (1, 3): 0.2778415083885193\n[INFO] IoU Score for box (1, 4): 0.36936208605766296\n[INFO] IoU Score for box (1, 5): 0.08170551061630249\n[INFO] IoU Score for box (1, 6): 0.4092644155025482\n[INFO] IoU Score for box (2, 0): 0.10704872757196426\n[INFO] IoU Score for box (2, 1): 0.11466032266616821\n[INFO] IoU Score for box (2, 3): 0.34572935104370117\n[INFO] IoU Score for box (2, 4): 0.26932957768440247\n[INFO] IoU Score for box (2, 5): 0.17588727176189423\n[INFO] IoU Score for box (2, 6): 0.058975815773010254\n[INFO] IoU Score for box (3, 0): 0.3096315264701843\n[INFO] IoU Score for box (3, 1): 0.2778415083885193\n[INFO] IoU Score for box (3, 2): 0.34572935104370117\n[INFO] IoU Score for box (3, 4): 0.7790185809135437\nLabels are equal: (tensor(5, device='cuda:0'), tensor(5, device='cuda:0'))\n[INFO] Box to keep index: 3 -&gt; tensor([248.2031, 472.6562, 675.7031, 994.8438], device='cuda:0',\n       grad_fn=&lt;UnbindBackward0&gt;)\n[INFO] IoU Score for box (3, 5): 0.11186295002698898\n[INFO] IoU Score for box (3, 6): 0.1719416379928589\n[INFO] IoU Score for box (5, 0): 0.06380129605531693\n[INFO] IoU Score for box (5, 1): 0.08170551061630249\n[INFO] IoU Score for box (5, 2): 0.17588727176189423\n[INFO] IoU Score for box (5, 3): 0.11186295002698898\n[INFO] IoU Score for box (5, 4): 0.0963958203792572\n[INFO] IoU Score for box (5, 6): 0.05411146208643913\n[INFO] IoU Score for box (6, 0): 0.2954297661781311\n[INFO] IoU Score for box (6, 1): 0.4092644155025482\n[INFO] IoU Score for box (6, 2): 0.058975815773010254\n[INFO] IoU Score for box (6, 3): 0.1719416379928589\n[INFO] IoU Score for box (6, 4): 0.24588997662067413\n[INFO] IoU Score for box (6, 5): 0.05411146208643913\ntensor([ True,  True,  True,  True, False,  True,  True])\n7 7 7\n6 6 6 tensor(6)\n[INFO] Number of total loops: 42, max possible loops: 49\n\n\n\n# tensor([ True,  True,  True,  True,  True, False,  True, False])\n# tensor([ True,  True,  True,  True,  True, False,  True, False])\n\n\n# Create a list of labels to plot on the boxes \nrandom_test_sample_labels_to_plot = [f\"Pred: {id2label[label_pred.item()]} ({round(score_pred.item(), 4)})\" \n                  for label_pred, score_pred in zip(random_test_sample_pred_labels, random_test_sample_pred_scores)]\n\nprint(f\"[INFO] Labels with scores: {random_test_sample_labels_to_plot}\")\n\n# Plot the predicted boxes on the random test image \ntest_image_with_preds_original = to_pil_image(\n    pic=draw_bounding_boxes(\n        image=pil_to_tensor(pic=dataset[\"test\"][random_test_pred_index][\"image\"]),\n        boxes=random_test_sample_pred_boxes,\n        labels=random_test_sample_labels_to_plot,\n        width=3\n    )\n)\n\n### Create image with filtered boxes\n\n# Create a list of labels to plot on the boxes \nrandom_test_sample_labels_to_plot_filtered = [f\"Pred: {id2label[label_pred.item()]} ({round(score_pred.item(), 4)})\" \n                  for label_pred, score_pred in zip(keep_labels, keep_scores)]\n\nprint(f\"[INFO] Labels with scores: {random_test_sample_labels_to_plot_filtered}\")\n\n# Plot the predicted boxes on the random test image \ntest_image_with_preds_filtered = to_pil_image(\n    pic=draw_bounding_boxes(\n        image=pil_to_tensor(pic=dataset[\"test\"][random_test_pred_index][\"image\"]),\n        boxes=keep_boxes,\n        labels=random_test_sample_labels_to_plot_filtered,\n        width=3\n    )\n)\n\n# Visualize the transformed image \nimport matplotlib.pyplot as plt\n\n# Create a figure with two subplots\nfig, axes = plt.subplots(1, 2, figsize=(20, 10))\n\n# Display image 1\naxes[0].imshow(test_image_with_preds_original)\naxes[0].axis(\"off\")  # Hide axes\naxes[0].set_title(f\"Original Image Preds (total: {len(random_test_sample_pred_boxes)})\")\n\n# Display image 2\naxes[1].imshow(test_image_with_preds_filtered)\naxes[1].axis(\"off\")  # Hide axes\naxes[1].set_title(f\"Filtered Image Preds (total: {len(keep_boxes)})\")\n\n# Show the plot\nplt.suptitle(f\"Greedy IoU Filtering (General) - For boxes with IoU &gt; {iou_general_threshold}, keep the higher scoring box\")\nplt.tight_layout()\nplt.show();\n\n[INFO] Labels with scores: ['Pred: bin (0.6625)', 'Pred: hand (0.5412)', 'Pred: trash (0.5007)', 'Pred: trash (0.4147)', 'Pred: trash (0.396)', 'Pred: not_trash (0.3237)', 'Pred: hand (0.2799)']\n[INFO] Labels with scores: ['Pred: bin (0.6625)', 'Pred: hand (0.5412)', 'Pred: trash (0.5007)', 'Pred: trash (0.4147)', 'Pred: not_trash (0.3237)', 'Pred: hand (0.2799)']\n\n\n\n\n\n\n\n\n\n\n# TK - more NMS logic:\n# If there are more than two hands, keep the one with the higher score...",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#tk---create-a-demo-with-simple-nms-filtering-only-keep-the-highest-scoring-boxes-per-image",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#tk---create-a-demo-with-simple-nms-filtering-only-keep-the-highest-scoring-boxes-per-image",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "17 TK - Create a Demo with Simple NMS Filtering (only keep the highest scoring boxes per image)",
    "text": "17 TK - Create a Demo with Simple NMS Filtering (only keep the highest scoring boxes per image)\nUPTOHERE:\n\nupload the demo to Hugging Face Spaces as Trashify V3\nMake sure the demo works\nGo back through the code and start tidying up/explaining things\n\nCreate a blog post to discuss different box formats in object detection\nCreate a blog post for NMS + IoU filtering (can create an IoU function that colours in the intersection parts)\nCreate an extension for longer training + synthetic data + evaluation metrics + deploying on transformers.js\n\n\n\n# Make directory for demo\nfrom pathlib import Path\n\ntrashify_data_aug_model_dir = Path(\"demos/trashify_object_detector_data_aug_model_with_nms/\")\ntrashify_data_aug_model_dir.mkdir(exist_ok=True)\n\n\n%%writefile demos/trashify_object_detector_data_aug_model_with_nms/requirements.txt\ntimm\ngradio\ntorch\ntransformers\n\nOverwriting demos/trashify_object_detector_data_aug_model_with_nms/requirements.txt\n\n\n\n%%writefile demos/trashify_object_detector_data_aug_model_with_nms/README.md\n---\ntitle: Trashify Demo V3 üöÆ\nemoji: üóëÔ∏è\ncolorFrom: purple\ncolorTo: blue\nsdk: gradio\nsdk_version: 4.40.0\napp_file: app.py\npinned: false\nlicense: apache-2.0\n---\n\n# üöÆ Trashify Object Detector Demo V3\n\nObject detection demo to detect `trash`, `bin`, `hand`, `trash_arm`, `not_trash`, `not_bin`, `not_hand`. \n\nUsed as example for encouraging people to cleanup their local area.\n\nIf `trash`, `hand`, `bin` all detected = +1 point.\n\n## Dataset\n\nAll Trashify models are trained on a custom hand-labelled dataset of people picking up trash and placing it in a bin.\n\nThe dataset can be found on Hugging Face as [`mrdbourke/trashify_manual_labelled_images`](https://huggingface.co/datasets/mrdbourke/trashify_manual_labelled_images).\n\n## Demos\n\n* [V1](https://huggingface.co/spaces/mrdbourke/trashify_demo_v1) = Fine-tuned DETR model trained *without* data augmentation.\n* [V2](https://huggingface.co/spaces/mrdbourke/trashify_demo_v2) = Fine-tuned DETR model trained *with* data augmentation.\n* [V3](https://huggingface.co/spaces/mrdbourke/trashify_demo_v3) = Fine-tuned DETR model trained *with* data augmentation (same as V2) with an NMS (Non Maximum Suppression) post-processing step.\n\nTK - finish the README.md + update with links to materials\n\nOverwriting demos/trashify_object_detector_data_aug_model_with_nms/README.md\n\n\n\n%%writefile demos/trashify_object_detector_data_aug_model_with_nms/app.py\nimport gradio as gr\nimport torch\nfrom PIL import Image, ImageDraw, ImageFont\n\nfrom transformers import AutoImageProcessor\nfrom transformers import AutoModelForObjectDetection\n\n# Note: Can load from Hugging Face or can load from local.\n# You will have to replace {mrdbourke} for your own username if the model is on your Hugging Face account.\nmodel_save_path = \"mrdbourke/detr_finetuned_trashify_box_detector_with_data_aug\" \n\n# Load the model and preprocessor\nimage_processor = AutoImageProcessor.from_pretrained(model_save_path)\nmodel = AutoModelForObjectDetection.from_pretrained(model_save_path)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(device)\n\n# Get the id2label dictionary from the model\nid2label = model.config.id2label\n\n# Set up a colour dictionary for plotting boxes with different colours\ncolor_dict = {   \n    \"bin\": \"green\",\n    \"trash\": \"blue\",\n    \"hand\": \"purple\",\n    \"trash_arm\": \"yellow\",\n    \"not_trash\": \"red\",\n    \"not_bin\": \"red\",\n    \"not_hand\": \"red\",\n}\n\n# Create helper functions for seeing if items from one list are in another \ndef any_in_list(list_a, list_b):\n    \"Returns True if any item from list_a is in list_b, otherwise False.\"\n    return any(item in list_b for item in list_a)\n\ndef all_in_list(list_a, list_b):\n    \"Returns True if all items from list_a are in list_b, otherwise False.\"\n    return all(item in list_b for item in list_a)\n\ndef filter_highest_scoring_box_per_class(boxes, labels, scores):\n    \"\"\"\n    Perform NMS (Non-max Supression) to only keep the top scoring box per class.\n\n    Args:\n        boxes: tensor of shape (N, 4)\n        labels: tensor of shape (N,)\n        scores: tensor of shape (N,)\n    Returns:\n        boxes: tensor of shape (N, 4) filtered for max scoring item per class\n        labels: tensor of shape (N,) filtered for max scoring item per class\n        scores: tensor of shape (N,) filtered for max scoring item per class\n    \"\"\"\n    # Start with a blank keep mask (e.g. all False and then update the boxes to keep with True)\n    keep_mask = torch.zeros(len(boxes), dtype=torch.bool)\n\n    # For each unique class\n    for class_id in labels.unique():\n        # Get the indicies for the target class\n        class_mask = labels == class_id\n\n        # If any of the labels match the current class_id\n        if class_mask.any():\n            # Find the index of highest scoring box for this specific class\n            class_scores = scores[class_mask]\n            highest_score_idx = class_scores.argmax()\n\n            # Convert back to the original index\n            original_idx = torch.where(class_mask)[0][highest_score_idx]\n\n            # Update the index in the keep mask to keep the highest scoring box \n            keep_mask[original_idx] = True\n        \n    return boxes[keep_mask], labels[keep_mask], scores[keep_mask]\n\ndef create_return_string(list_of_predicted_labels, target_items=[\"trash\", \"bin\", \"hand\"]):\n     # Setup blank string to print out\n    return_string = \"\"\n\n    # If no items detected or trash, bin, hand not in list, return notification \n    if (len(list_of_predicted_labels) == 0) or not (any_in_list(list_a=target_items, list_b=list_of_predicted_labels)):\n        return_string = f\"No trash, bin or hand detected at confidence threshold {conf_threshold}. Try another image or lowering the confidence threshold.\"\n        return return_string\n\n    # If there are some missing, print the ones which are missing\n    elif not all_in_list(list_a=target_items, list_b=list_of_predicted_labels):\n        missing_items = []\n        for item in target_items:\n            if item not in list_of_predicted_labels:\n                missing_items.append(item)\n        return_string = f\"Detected the following items: {list_of_predicted_labels} (total: {len(list_of_predicted_labels)}). But missing the following in order to get +1: {missing_items}. If this is an error, try another image or altering the confidence threshold. Otherwise, the model may need to be updated with better data.\"\n        \n    # If all 3 trash, bin, hand occur = + 1\n    if all_in_list(list_a=target_items, list_b=list_of_predicted_labels):\n        return_string = f\"+1! Found the following items: {list_of_predicted_labels} (total: {len(list_of_predicted_labels)}), thank you for cleaning up the area!\"\n\n    print(return_string)\n\n    return return_string\n\ndef predict_on_image(image, conf_threshold):\n    with torch.no_grad():\n        inputs = image_processor(images=[image], return_tensors=\"pt\")\n        outputs = model(**inputs.to(device))\n\n        target_sizes = torch.tensor([[image.size[1], image.size[0]]]) # height, width \n\n        results = image_processor.post_process_object_detection(outputs,\n                                                                threshold=conf_threshold,\n                                                                target_sizes=target_sizes)[0]\n    # Return all items in results to CPU\n    for key, value in results.items():\n        try:\n            results[key] = value.item().cpu() # can't get scalar as .item() so add try/except block\n        except:\n            results[key] = value.cpu()\n\n    # Can return results as plotted on a PIL image (then display the image)\n    draw = ImageDraw.Draw(image)\n\n    # Create a copy of the image to draw on it for NMS\n    image_nms = image.copy()\n    draw_nms = ImageDraw.Draw(image_nms)\n\n    # Get a font from ImageFont\n    font = ImageFont.load_default(size=20)\n\n    # Get class names as text for print out\n    class_name_text_labels = []\n\n    # TK - update this for NMS\n    class_name_text_labels_nms = []\n\n    # Get original boxes, scores, labels\n    original_boxes = results[\"boxes\"]\n    original_labels = results[\"labels\"]\n    original_scores = results[\"scores\"]\n\n    # Filter boxes and only keep 1x of each label with highest score\n    filtered_boxes, filtered_labels, filtered_scores = filter_highest_scoring_box_per_class(boxes=original_boxes,\n                                                                                            labels=original_labels,\n                                                                                            scores=original_scores)\n    # TODO: turn this into a function so it's cleaner?\n    for box, label, score in zip(original_boxes, original_labels, original_scores):\n        # Create coordinates\n        x, y, x2, y2 = tuple(box.tolist())\n\n        # Get label_name\n        label_name = id2label[label.item()]\n        targ_color = color_dict[label_name]\n        class_name_text_labels.append(label_name)\n\n        # Draw the rectangle\n        draw.rectangle(xy=(x, y, x2, y2), \n                       outline=targ_color,\n                       width=3)\n        \n        # Create a text string to display\n        text_string_to_show = f\"{label_name} ({round(score.item(), 3)})\"\n\n        # Draw the text on the image\n        draw.text(xy=(x, y),\n                  text=text_string_to_show,\n                  fill=\"white\",\n                  font=font)\n    \n    # TODO: turn this into a function so it's cleaner?\n    for box, label, score in zip(filtered_boxes, filtered_labels, filtered_scores):\n        # Create coordinates\n        x, y, x2, y2 = tuple(box.tolist())\n\n        # Get label_name\n        label_name = id2label[label.item()]\n        targ_color = color_dict[label_name]\n        class_name_text_labels_nms.append(label_name)\n\n        # Draw the rectangle\n        draw_nms.rectangle(xy=(x, y, x2, y2), \n                       outline=targ_color,\n                       width=3)\n        \n        # Create a text string to display\n        text_string_to_show = f\"{label_name} ({round(score.item(), 3)})\"\n\n        # Draw the text on the image\n        draw_nms.text(xy=(x, y),\n                  text=text_string_to_show,\n                  fill=\"white\",\n                  font=font)\n    \n    \n    # Remove the draw each time\n    del draw\n    del draw_nms\n\n    # Create the return string\n    return_string = create_return_string(list_of_predicted_labels=class_name_text_labels)\n    return_string_nms = create_return_string(list_of_predicted_labels=class_name_text_labels_nms)\n    \n    return image, return_string, image_nms, return_string_nms\n\n# Create the interface\ndemo = gr.Interface(\n    fn=predict_on_image,\n    inputs=[\n        gr.Image(type=\"pil\", label=\"Target Image\"),\n        gr.Slider(minimum=0, maximum=1, value=0.25, label=\"Confidence Threshold\")\n    ],\n    outputs=[\n        gr.Image(type=\"pil\", label=\"Image Output (no filtering)\"),\n        gr.Text(label=\"Text Output (no filtering)\"),\n        gr.Image(type=\"pil\", label=\"Image Output (with max score per class box filtering)\"),\n        gr.Text(label=\"Text Output (with max score per class box filtering)\")\n        \n    ],\n    title=\"üöÆ Trashify Object Detection Demo V3\",\n    description=\"\"\"Help clean up your local area! Upload an image and get +1 if there is all of the following items detected: trash, bin, hand.\n\n    The model in V3 is [same model](https://huggingface.co/mrdbourke/detr_finetuned_trashify_box_detector_with_data_aug) as in [V2](https://huggingface.co/spaces/mrdbourke/trashify_demo_v2) (trained with data augmentation) but has an additional post-processing step (NMS or [Non Maximum Suppression](https://paperswithcode.com/method/non-maximum-suppression)) to filter classes for only the highest scoring box of each class. \n    \"\"\",\n    # Examples come in the form of a list of lists, where each inner list contains elements to prefill the `inputs` parameter with\n    examples=[\n        [\"examples/trashify_example_1.jpeg\", 0.25],\n        [\"examples/trashify_example_2.jpeg\", 0.25],\n        [\"examples/trashify_example_3.jpeg\", 0.25]\n    ],\n    cache_examples=True\n)\n\n# Launch the demo\ndemo.launch()\n\nOverwriting demos/trashify_object_detector_data_aug_model_with_nms/app.py\n\n\n\n17.1 TK - Upload our demo to the Hugging Face Hub\n\n# 1. Import the required methods for uploading to the Hugging Face Hub\nfrom huggingface_hub import (\n    create_repo,\n    get_full_repo_name,\n    upload_file, # for uploading a single file (if necessary)\n    upload_folder # for uploading multiple files (in a folder)\n)\n\n# 2. Define the parameters we'd like to use for the upload\nLOCAL_DEMO_FOLDER_PATH_TO_UPLOAD = \"demos/trashify_object_detector_data_aug_model_with_nms\" # TK - update this path \nHF_TARGET_SPACE_NAME = \"trashify_demo_v3\"\nHF_REPO_TYPE = \"space\" # we're creating a Hugging Face Space\nHF_SPACE_SDK = \"gradio\"\nHF_TOKEN = \"\" # optional: set to your Hugging Face token (but I'd advise storing this as an environment variable as previously discussed)\n\n# 3. Create a Space repository on Hugging Face Hub \nprint(f\"[INFO] Creating repo on Hugging Face Hub with name: {HF_TARGET_SPACE_NAME}\")\ncreate_repo(\n    repo_id=HF_TARGET_SPACE_NAME,\n    # token=HF_TOKEN, # optional: set token manually (though it will be automatically recognized if it's available as an environment variable)\n    repo_type=HF_REPO_TYPE,\n    private=False, # set to True if you don't want your Space to be accessible to others\n    space_sdk=HF_SPACE_SDK,\n    exist_ok=True, # set to False if you want an error to raise if the repo_id already exists \n)\n\n# 4. Get the full repository name (e.g. {username}/{model_id} or {username}/{space_name})\nfull_hf_repo_name = get_full_repo_name(model_id=HF_TARGET_SPACE_NAME)\nprint(f\"[INFO] Full Hugging Face Hub repo name: {full_hf_repo_name}\")\n\n# 5. Upload our demo folder\nprint(f\"[INFO] Uploading {LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD} to repo: {full_hf_repo_name}\")\nfolder_upload_url = upload_folder(\n    repo_id=full_hf_repo_name,\n    folder_path=LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD,\n    path_in_repo=\".\", # upload our folder to the root directory (\".\" means \"base\" or \"root\", this is the default)\n    # token=HF_TOKEN, # optional: set token manually\n    repo_type=HF_REPO_TYPE,\n    commit_message=\"Uploading Trashify box detection model v3 app.py with NMS post processing\"\n)\nprint(f\"[INFO] Demo folder successfully uploaded with commit URL: {folder_upload_url}\")\n\n[INFO] Creating repo on Hugging Face Hub with name: trashify_demo_v3\n[INFO] Full Hugging Face Hub repo name: mrdbourke/trashify_demo_v3\n[INFO] Uploading demos/trashify_object_detector_data_aug_model_with_nms to repo: mrdbourke/trashify_demo_v3\n[INFO] Demo folder successfully uploaded with commit URL: https://huggingface.co/spaces/mrdbourke/trashify_demo_v3/tree/main/.\n\n\n\n\n17.2 tK - Embed the Space to Test the Model\n\nfrom IPython.display import HTML\n\n\n# You can get embeddable HTML code for your demo by clicking the \"Embed\" button on the demo page\nHTML(data='''\n&lt;iframe\n    src=\"https://mrdbourke-trashify-demo-v3.hf.space\"\n    frameborder=\"0\"\n    width=\"1000\"\n    height=\"1600\"\n&gt;&lt;/iframe&gt;     \n''')\n\n\n     \n\n\n\n# UPTOHERE\n# Next, focus on a single input -&gt; output ‚úÖ\n# Show case what an output from the model looks like untrained (e.g. plot the next boxes on it) ‚úÖ\n# After showcasing 1x prediction, move onto training a model and seeing if we can get it to improve ‚úÖ\n# Continually focus on 1 input, 1 output until we can scale up ‚úÖ\n# Create a demo of our model and upload it to Hugging Face ‚úÖ\n    # Add examples to test the demo ‚úÖ\n    # Write code to upload the demo to Hugging Face ‚úÖ\n# Create visualization of input and output of data augmentation ‚úÖ\n# Create demo of model with data augmentation ‚úÖ\n# Model 2: Try improve our model with data augmentation ‚úÖ\n    # Visualize data augmentation examples in and out of the model \n    # Note: looks like augmentation may hurt our results... ü§î, this is because our data is so similar, potentially could help with more diverse data, e.g. synthetic data \n    # Try in a demo and see how it works -&gt; Trashify Demo V2 ‚úÖ \n    # Extension: Also try a model training for longer \n# Model 3 (just improve with NMS): Create NMS option so only highest quality boxes are kept for each class ‚úÖ\n\n# Next:\n\n# Go through notebook and clean it up for \n# Once we've got a better performing model, introduce evaluation metrics\n# End: three models, three demos, one without data augmentation, one with it, one with NMS (post-processing) + can have as an extension to train the model for longer and see what happens\n\n# Extensions:\n# Train a model for longer and see if it improves (e.g. 72 epochs) \n\n# Workflow:\n# Untrained model -&gt; input/output -&gt; poor results (always visualize, visualize, visualize!)\n# Trained model -&gt; input/output -&gt; better results (always visualize, visualize, visualize!)\n\n# Outline:\n# Single input/output with untrained model (bad output)\n# Train model to improve on single input/output\n# Introduce evaluation metric\n# Introduce data augmentation, see D-FINE paper for data augmentation options (we can keep it simple)\n    # See: https://arxiv.org/pdf/2410.13842 \n    # \"The total batch size is 32 across all variants. Training schedules include 72 epochs with advanced augmentation (RandomPhotometricDistort, RandomZoomOut, RandomIoUCrop, and RMultiScaleInput)\n    # followed by 2 epochs without advanced augmentation for D-FINE-X and D-FINE-L, and 120 epochs with advanced augmentation followed by 4\n    # epochs without advanced augmentation for D-FINE-M and D-FINE-S (RT-DETRv2 Training Strategy (Lv et al., 2024) in Table 3)\"\n    # TODO: Read RT-DETRv2 training strategy from paper mentioned above\n    # TODO: Read PP-YOLO data augmentation paper (keep it simple to begin with, can increase when needed)\n# Create demo with Gradio\n# Create demo with Transformers.js, see: https://huggingface.co/docs/transformers.js/en/tutorials/vanilla-js",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#extensions-extra-curriculum",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#extensions-extra-curriculum",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "18 Extensions + Extra-Curriculum",
    "text": "18 Extensions + Extra-Curriculum\n\nExtension: possibly improve the model with synthetic data? e.g.¬†on classes/bins not visible in the model\nExtension: train the model for longer and see how it improves, this could be model v4\n\nBaselines:\n\nV1 = model no data augmentaiton\nV2 = model with data augmentation\nV3 = model with NMS (post processing)\n\nExtensions:\n\nV4 = model trained for longer with NMS\nV5 = synthetic data scaled up‚Ä¶?\n\n\nExtension: Zero-shot object detection - but what if I don‚Äôt have labels?\n\nThis could discuss the use of zero-shot object detection models such as GroundingDINO and OmDet\nSee OmDet - https://huggingface.co/omlab/omdet-turbo-swin-tiny-hf\nSee GroundingDINO - https://huggingface.co/docs/transformers/en/model_doc/grounding-dino\n\nExtension: Try to repeat the workflow we‚Äôve gone through with another model such as https://huggingface.co/IDEA-Research/dab-detr-resnet-50-dc5-pat3 (apparently it is slightly better performing on COCO too)",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#tk---overview",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#tk---overview",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "1 TK - Overview",
    "text": "1 TK - Overview\nTK - Make an intro about being on the Trashify üöÆ team with a mission to make the world a cleaner place, trashify = using ML to incentivize people to pick up trash in their local area\nWelcome to the Learn Hugging Face Object Detection project!\nInside this project, we‚Äôll learn bits and pieces about the Hugging Face ecosystem as well as how to build our own custom object detection model.\nWe‚Äôll start with a collection of images with bounding box files as our dataset, fine-tune an existing computer vision model to detect items in an image and then share our model as a demo others can use.\nTK image - update cover image for object detection  \n\nWe‚Äôre going to put on our internship hats and build a food not food text classification model using tools from the Hugging Face ecosystem.\n\n\n‚Äì&gt;\n\n\n\n\n\n\nNote\n\n\n\nFeel to keep reading through the notebook but if you‚Äôd like to run the code yourself, be sure to go through the setup guide first.\n\n\n\n1.1 TK - What we‚Äôre going to build\nWe‚Äôre going to be bulding Trashify üöÆ, an object detection model which incentivises people to pick up trash in their local area by detecting bin, trash, hand.\nIf all three items are detected, a person gets +1 point!\nFor example, say you were going for a walk around your neighbourhood and took a photo of yourself picking up a piece (with your hand or trash arm) of trash and putting it in the bin, you would get a point.\nWith this object detection model, you could deploy it to an application which would automatically detect the target classes and then save the result to an online leaderboard.\nThe incentive would be to score the most points, in turn, picking up the most piecces of trash, in a given area.\nMore specifically, we‚Äôre going to follow the following steps:\n\nData: Problem defintion and dataset preparation - Getting a dataset/setting up the problem space.\nModel: Finding, training and evaluating a model - Finding an object detection model suitable for our problem on Hugging Face and customizing it to our own dataset.\nDemo: Creating a demo and put our model into the real world - Sharing our trained model in a way others can access and use.\n\nBy the end of this project, you‚Äôll have a trained model and demo on Hugging Face you can share with others:\n\nfrom IPython.display import HTML \n\nHTML(\"\"\"\n&lt;iframe\n    src=\"https://mrdbourke-trashify-demo-v3.hf.space\"\n    frameborder=\"0\"\n    width=\"850\"\n    height=\"850\"\n&gt;&lt;/iframe&gt;\n\"\"\")\n\n\n\n\n\n\n\n1.2 TK - What is object detection?\nObject detection is the process of identifying and locating an item in an image.\nWhere item can mean almost anything.\nFor example:\n\nDetecting car licence plates in a video feed (videos are a series of images) for a parking lot entrance.\nDetecting delivery people walking towards your front door on a security camera.\nDetecting defects on a manufacturing line.\nDetecting pot holes in the road so repair works can automatically be scheduled.\nDetecting small pests (Varroa Mite) on the bodies of bees.\nDetecting weeds in a field so you know what to remove and what to keep.\n\n‚Äì\nTK - add examples of actual trash identification projects, see:\n\nGoogle using machine learning for trash identification ‚Äî https://sustainability.google/operating-sustainably/stories/circular-economy-marketplace/\nTrashify website for identifying trash ‚Äî https://www.trashify.tech/\nWaste management with deep learning ‚Äî https://www.sciencedirect.com/science/article/abs/pii/S0956053X23001915\nLabel Studio being used for labelling a trash dataset ‚Äî https://labelstud.io/blog/ameru-labeling-for-a-greener-world/\n\n‚Äì\n\n\n\n\n\n\nNote\n\n\n\nNote: Object detection is also sometimes referred to as image localization or object localization. For consistency, I will use the term object detection, however, either of these terms could substitute.\n\n\n\n\n\n* TK image - examples of where object detection is used\n\n\n\nImage classification deals with classifying an image as a whole into a single class, object detection endeavours to find the specific target item and where it is in an image.\nOne of the most common ways of showing where an item is in an image is by displaying a bounding box (a rectangle-like box around the target item).\nAn object detection model will often take an input image tensor in the shape [3, 640, 640] ([colour_channels, height, width]) and output a tensor in the form [class_name, x_min, y_min, x_max, y_max] or [class_name, x1, y1, x2, y2] (this is two ways to write the same example format, there are more formats, we‚Äôll see these below in Table¬†1).\nWhere:\n\nclass_name = The classification of the target item (e.g.¬†\"car\", \"person\", \"banana\", \"piece_of_trash\", this could be almost anything).\nx_min = The x value of the top left corner of the box.\ny_min = The y value of the top left corner of the box.\nx_max = The x value of the bottom right corner of the box.\ny_max = The y value of the bottom right corner of the box.\n\n‚Äì TK image ‚Äì example of a bounding box on an image\n\n\n\n\n\n\nObject detection bounding box formats\n\n\n\nWhen you get into the world of object detection, you will find that there are several different bounding box formats.\nThere are three major formats you should be familiar with: XYXY, XYWH, CXCYWH (there are more but these are the most common).\nKnowing which bounding box format you‚Äôre working with can be the difference between a good model and a very poor model (wrong bounding boxes = wrong outcome).\nWe‚Äôll get hands-on with a couple of these in this project.\nBut for an in-depth example of all three, I created a guide on different bounding box formats and how to draw them, reading this should give a good intuition behind each style of bounding box.\n\n\n\n\n1.3 TK - Why train your own object detection models?\nYou can customize pre-trained models for object detection as well as API-powered models and LLMs such as Gemini, LandingAI and DINO-X.\nDepending on your requirements, there are several pros and cons for using your own model versus using an API.\nTraining/fine-tuning your own model:\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nControl: Full control over model lifecycle.\nCan be complex to get setup.\n\n\nNo usage limits (aside from compute constraints).\nRequires dedicated compute resources for training/inference.\n\n\nCan train once and deploy everywhere/whenever you want (for example, Tesla deploying a model to all self-driving cars).\nRequires maintenance over time to ensure performance remains up to par.\n\n\nPrivacy: Data can be kept in-house/app and doesn‚Äôt need to go to a third party.\nCan require longer development cycles compared to using existing APIs.\n\n\nSpeed: Customizing a small model for a specific use case often means it runs much faster on local hardware, for example, modern object detection models can achieve 70-100+ FPS (frames per second) on modern GPU hardware.\n\n\n\n\nUsing a pre-built model API:\n\n\n\n\n\n\n\nPros\nCons\n\n\n\n\nEase of use: often can be setup within a few lines of code.\nIf the model API goes down, your service goes down.\n\n\nNo maintenance of compute resources.\nData is required to be sent to a third-party for processing.\n\n\nAccess to the most advanced models.\nThe API may have usage limits per day/time period.\n\n\nCan scale if usage increases.\nCan be much slower than using dedicated models due to requiring an API call.\n\n\n\nFor this project, we‚Äôre going to focus on fine-tuning our own model.\n\n\n1.4 TK - Workflow we‚Äôre going to follow\nThe good news for us is that the Hugging Face ecosystem makes working on custom machine learning projects an absolute blast.\nAnd workflow is reproducible across several kinds of projects.\nStart with data (or skip this step and go straight to a model) -&gt; get/customize a model -&gt; build and share a demo.\nWith this in mind, our motto is data, model, demo!\nMore specifically, we‚Äôre going to follow the rough workflow of:\n\nCreate, preprocess and load data using Hugging Face Datasets.\nDefine the model we‚Äôd like use with transformers.AutoModelForObjectDetection (or another similar model class).\nDefine training arguments (these are hyperparameters for our model) with transformers.TrainingArguments.\nPass TrainingArguments from 3 and target datasets to an instance of transformers.Trainer.\nTrain the model by calling Trainer.train().\nSave the model (to our local machine or to the Hugging Face Hub).\nEvaluate the trained model by making and inspecting predctions on the test data.\nTurn the model into a shareable demo.\n\nI say rough because machine learning projects are often non-linear in nature.\nAs in, because machine learning projects involve many experiments, they can kind of be all over the place.\nBut this worfklow will give us some good guidelines to follow.\n\n\n\n\nA general Hugging Face workflow from idea to shared model and demo using tools from the Hugging Face ecosystem. You‚Äôll notice some of the steps don‚Äôt match with our workflow outline above. This is because the text-based workflow outline above breaks some of the steps down for educational purposes. These kind of workflows are not set in stone and are more of guide than specific directions. See information on each of the tools in the Hugging Face documentation.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#tk---importing-necessary-libraries",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#tk---importing-necessary-libraries",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "2 TK - Importing necessary libraries",
    "text": "2 TK - Importing necessary libraries\nLet‚Äôs get started!\nFirst, we‚Äôll import the required libraries.\nIf you‚Äôre running on your local computer, be sure to check out the getting setup guide to make sure you have everything you need.\nIf you‚Äôre using Google Colab, many of them the following libraries will be installed by default.\nHowever, we‚Äôll have to install a few extras to get everything working.\n\n\n\n\n\n\nNote\n\n\n\nIf you‚Äôre running on Google Colab, this notebook will work best with access to a GPU. To enable a GPU, go to Runtime ‚û°Ô∏è Change runtime type ‚û°Ô∏è Hardware accelerator ‚û°Ô∏è GPU.\n\n\nWe‚Äôll need to install the following libraries from the Hugging Face ecosystem:\n\ntransformers - comes pre-installed on Google Colab but if you‚Äôre running on your local machine, you can install it via pip install transformers.\ndatasets - a library for accessing and manipulating datasets on and off the Hugging Face Hub, you can install it via pip install datasets.\nevaluate - a library for evaluating machine learning model performance with various metrics, you can install it via pip install evaluate.\naccelerate - a library for training machine learning models faster, you can install it via pip install accelerate.\ngradio - a library for creating interactive demos of machine learning models, you can install it via pip install gradio.\n\nAnd the following library is not part of the Hugging Face ecosystem but it is helpful for evaluating our models:\n\ntorchmetrics - a library containing many evaluation metrics compatible with PyTorch/Transformers, you can install it via pip install torchmetrics.\n\nWe can also check the versions of our software with package_name.__version__.\n\n# Install/import dependencies (this is mostly for Google Colab, as the other dependences are available by default in Colab)\ntry:\n  import datasets, evaluate, accelerate\n  import gradio as gr\nexcept ModuleNotFoundError:\n  !pip install -U datasets evaluate accelerate gradio # -U stands for \"upgrade\" so we'll get the latest version by default\n  import datasets, evaluate, accelerate\n  import gradio as gr\n\nimport random\n\nimport numpy as np\n\nimport torch\nimport transformers\n\n# Required for evaluation\n# Can install with !pip install torchmetrics[detection]\nimport torchmetrics\nimport pycocotools\n\n# Check versions (as long as you've got the following versions or higher, you should be good)\nprint(f\"Using transformers version: {transformers.__version__}\")\nprint(f\"Using datasets version: {datasets.__version__}\")\nprint(f\"Using torch version: {torch.__version__}\")\nprint(f\"Using torchmetrics version: {torchmetrics.__version__}\")\n\nUsing transformers version: 4.49.0.dev0\nUsing datasets version: 3.1.0\nUsing torch version: 2.6.0+cu124\nUsing torchmetrics version: 1.4.1\n\n\nWonderful, as long as your versions are the same or higher to the versions above, you should be able to run the code below.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#extra-resources",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#extra-resources",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "20 Extra resources",
    "text": "20 Extra resources\n\nA Guide to Bounding Box Formats and How to Draw Them by Daniel Bourke.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#getting-a-dataset",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#getting-a-dataset",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "3 Getting a dataset",
    "text": "3 Getting a dataset\nOkay, now we‚Äôre got the required libraries, let‚Äôs get a dataset.\nGetting a dataset is one of the most important things a machine learning project.\nThe dataset you often determines the type of model you use as well as the quality of the outputs of that model.\nMeaning, if you have a high quality dataset, chances are, your future model could also have high quality outputs.\nIt also means if your dataset is of poor quality, your model will likely also have poor quality outputs.\nFor an object detection problem, your dataset will likely come in the form of a group of images as well as a file with annotations belonging to those images.\nFor example, you might have the following setup:\nfolder_of_images/\n    image_1.jpeg\n    image_2.jpeg\n    image_3.jpeg\nannotations.json\nWhere the annotations.json contains details about the contains of each image:\n\n\nannotations.json\n\n[\n    {\n        'image_path': 'image_1.jpeg', \n        'image_id': 42,\n        'annotations': \n            {\n                'file_name': ['image_1.jpeg'],\n                'image_id': [42],\n                'category_id': [1],\n                'bbox': [\n                            [360.20001220703125, 528.5, 177.1999969482422, 261.79998779296875],\n                        ],\n                'area': [46390.9609375]\n            },\n        'label_source': 'manual_prodigy_label',\n        'image_source': 'manual_taken_photo'\n    },\n\n    ...(more labels down here)\n]\n\nDon‚Äôt worry too much about the exact meaning of everything in the above annotations.json file for now (this is only one example, there are many different ways object detection information could be displayed).\nThe main point is that each target image is paired with an assosciated label.\nNow like all good machine learning cooking shows, I‚Äôve prepared a dataset from earlier.\nTK image - dataset on Hugging Face\nIt‚Äôs stored on Hugging Face Datasets (also called the Hugging Face Hub) under the name mrdbourke/trashify_manual_labelled_images.\nThis is a dataset I‚Äôve collected manually by hand (yes, by picking up 1000+ pieces of trash and photographing it) as well as labelled by hand (by drawing boxes on each image with a labelling tool called Prodigy).\n\n3.1 Loading the dataset\nTo load a dataset stored on the Hugging Face Hub we can use the datasets.load_dataset(path=NAME_OR_PATH_OF_DATASET) function and pass it the name/path of the dataset we want to load.\nIn our case, our dataset name is mrdbourke/trashify_manual_labelled_images (you can also change this for your own dataset).\nAnd since our dataset is hosted on Hugging Face, when we run the following code for the first time, it will download it.\nIf your target dataset is quite large, this download may take a while.\nHowever, once the dataset is downloaded, subsequent reloads will be mush faster.\n\n\n\n\n\n\nGetting information about a function/method\n\n\n\nOne way to find out what a function or method does is to lookup the documentation.\nAnother way is to write the function/method name with a question mark afterwards.\nFor example:\nfrom datasets import load_dataset\n\nload_dataset?\nGive it a try.\nYou should see some helpful information about what inputs the method takes and how they are used.\n\n\nLet‚Äôs load our dataset and check it out.\n\nfrom datasets import load_dataset\n\n# Load our Trashify dataset\ndataset = load_dataset(path=\"mrdbourke/trashify_manual_labelled_images\")\n\ndataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],\n        num_rows: 1128\n    })\n})\n\n\nBeautiful!\nWe can see that there is a train split of the dataset already which currently contains all of the samples (1128 in total).\nThere are also some features that come with our dataset which are related to our object detection goal.\n\nprint(f\"[INFO] Length of original dataset: {len(dataset['train'])}\")\nprint(f\"[INFO] Dataset features:\") \n\nfrom pprint import pprint\n\npprint(dataset['train'].features)\n\n[INFO] Length of original dataset: 1128\n[INFO] Dataset features:\n{'annotations': Sequence(feature={'area': Value(dtype='float32', id=None),\n                                  'bbox': Sequence(feature=Value(dtype='float32',\n                                                                 id=None),\n                                                   length=4,\n                                                   id=None),\n                                  'category_id': ClassLabel(names=['bin',\n                                                                   'hand',\n                                                                   'not_bin',\n                                                                   'not_hand',\n                                                                   'not_trash',\n                                                                   'trash',\n                                                                   'trash_arm'],\n                                                            id=None),\n                                  'file_name': Value(dtype='string', id=None),\n                                  'image_id': Value(dtype='int64', id=None),\n                                  'iscrowd': Value(dtype='int64', id=None)},\n                         length=-1,\n                         id=None),\n 'image': Image(mode=None, decode=True, id=None),\n 'image_id': Value(dtype='int64', id=None),\n 'image_source': Value(dtype='string', id=None),\n 'label_source': Value(dtype='string', id=None)}\n\n\nNice!\nWe can see our dataset features contain the following fields:\n\nannotations - A sequence of values including a bbox field (short for bounding box) as well as category_id field which contains the target objects we‚Äôd like to identify in our images (['bin', 'hand', 'not_bin', 'not_hand', 'not_trash', 'trash', 'trash_arm']).\nimage - This contains the target image assosciated with a given set of annotations (in our case, images and annotations have been uploaded to the Hugging Face Hub together).\nimage_id - A unique ID assigned to a given sample.\nimage_source - Where the image came from (all of our images have been manually collected).\nlabel_source - Where the image label came from (all of our images have been manually labelled).\n\n\n\n3.2 Viewing a single sample from our data\nNow we‚Äôve seen the features, let‚Äôs check out a single sample from our dataset.\nWe can index on a single sample of the \"train\" set just like indexing on a Python list.\n\n# View a single sample of the dataset\ndataset[\"train\"][42]\n\n{'image': &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,\n 'image_id': 745,\n 'annotations': {'file_name': ['094f4f41-dc07-4704-96d7-8d5e82c9edb9.jpeg',\n   '094f4f41-dc07-4704-96d7-8d5e82c9edb9.jpeg',\n   '094f4f41-dc07-4704-96d7-8d5e82c9edb9.jpeg'],\n  'image_id': [745, 745, 745],\n  'category_id': [5, 1, 0],\n  'bbox': [[333.1000061035156,\n    611.2000122070312,\n    244.89999389648438,\n    321.29998779296875],\n   [504.0, 612.9000244140625, 451.29998779296875, 650.7999877929688],\n   [202.8000030517578,\n    366.20001220703125,\n    532.9000244140625,\n    555.4000244140625]],\n  'iscrowd': [0, 0, 0],\n  'area': [78686.3671875, 293706.03125, 295972.65625]},\n 'label_source': 'manual_prodigy_label',\n 'image_source': 'manual_taken_photo'}\n\n\nWe see a few more details here compared to just looking at the features.\nWe notice the image is a PIL.Image with size 960x1280 (width x height).\nAnd the file_name is a UUID (Universially Unique Identifier, made with uuid.uuid4()).\nThe bbox field in the annotations key contains a list of bounding boxes assosciated with the image.\nIn this case, there are 3 different bounding boxes.\nWith the category_id values of 5, 1, 0 (we‚Äôll map these to class names shortly).\nLet‚Äôs inspect a single bounding box.\n\ndataset[\"train\"][42][\"annotations\"][\"bbox\"][0]\n\n[333.1000061035156, 611.2000122070312, 244.89999389648438, 321.29998779296875]\n\n\nThis array gives us the coordinates of a single bounding box in the format XYWH.\nWhere:\n\nX is the x-coordinate of the top left corner of the box (333.1).\nY is the y-coordinate of the top left corner of the box (611.2).\nW is the width of the box (244.9).\nH is the height of the box (321.3).\n\nAll of these values are in absolute pixel values (meaning an x-coordinate of 333.1 is 333.1 pixels across on the x-axis).\nHow do I know this?\nI know this because I created the box labels and this is the default value Prodigy (the labelling tool I used) outputs boxes.\nHowever, if you were to come across another bouding box dataset, one of the first steps would be to figure out what format your bounding boxes are in.\nWe‚Äôll see more on bounding box formats shortly.\n\n\n3.3 Extracting the category names from our data\nBefore we start to visualize our sample image and bounding boxes, let‚Äôs extract the category names from our dataset.\nWe can do so by accessing the features attribute our of dataset and then following it through to find the category_id feature, this contains a list of our text-based class names.\n\n\n\n\n\n\nNote\n\n\n\nWhen working with different categories, it‚Äôs good practice to get a list or mapping (e.g.¬†a Python dictionary) from category name to ID and vice versa.\nFor example:\n# Category to ID\n{\"class_name\": 0}\n\n# ID to Category\n{0: \"class_name\"}\nNot all datasets will have this implemented in an easy to access way, so it might take a bit of research to get it created.\n\n\nLet‚Äôs access the class names in our dataset and save them to a variable categories.\n\n# Get the categories from the dataset\n# Note: This requires the dataset to have been uploaded with this information setup, not all datasets will have this available.\ncategories = dataset[\"train\"].features[\"annotations\"].feature[\"category_id\"]\n\n# Get the names attribute\ncategories.names\n\n['bin', 'hand', 'not_bin', 'not_hand', 'not_trash', 'trash', 'trash_arm']\n\n\nBeautiful!\nWe get the following class names:\n\nbin - A rubbish bin or trash can.\nhand - A person‚Äôs hand.\nnot_bin - Negative version of bin for items that look like a bin but shouldn‚Äôt be identified as one.\nnot_hand - Negative version of hand for items that look like a hand but shouldn‚Äôt be identified as one.\nnot_trash - Negative version of trash for items that look like trash but shouldn‚Äôt be identified as it.\ntrash - An item of trash you might find on a walk such as an old plastic bottle, food wrapper, cigarette butt or used coffee cup.\ntrash_arm - A mechanical arm used for picking up trash.\n\nThe goal of our computer vision model will be: given an image, detect items belonging to these target classes if they are present.\n\n\n3.4 Creating a mapping from numbers to labels\nNow we‚Äôve got our text-based class names, let‚Äôs create a mapping from label to ID and ID to label.\nFor each of these, Hugging Face use the terminology label2id and id2label respectively.\n\n# Map ID's to class names and vice versa\nid2label = {i: class_name for i, class_name in enumerate(categories.names)}\nlabel2id = {value: key for key, value in id2label.items()}\n\nprint(f\"Label to ID mapping:\\n{label2id}\\n\")\nprint(f\"ID to label mapping:\\n{id2label}\")\n# id2label, label2id\n\nLabel to ID mapping:\n{'bin': 0, 'hand': 1, 'not_bin': 2, 'not_hand': 3, 'not_trash': 4, 'trash': 5, 'trash_arm': 6}\n\nID to label mapping:\n{0: 'bin', 1: 'hand', 2: 'not_bin', 3: 'not_hand', 4: 'not_trash', 5: 'trash', 6: 'trash_arm'}\n\n\n\n\n3.5 Creating a colour palette\nOk we know which class name matches to which ID, now let‚Äôs create a dictionary of different colours we can use to display our bounding boxes.\nIt‚Äôs one thing to plot bounding boxes, it‚Äôs another thing to make them look nice.\nAnd we always want our plots looking nice!\nWe‚Äôll colour the positive classes bin, hand, trash, trash_arm in nice bright colours.\nAnd the negative classes not_bin, not_hand, not_trash in a light red colour to indicate they‚Äôre the negative versions.\nOur colour dictionary will map class_name -&gt; (red, green, blue) (or RGB) colour values.\n\n# Make colour dictionary\ncolour_palette = {\n    'bin': (0, 0, 224),         # Bright Blue (High contrast with greenery) in format (red, green, blue)\n    'not_bin': (255, 80, 80),   # Light Red to indicate negative class\n\n    'hand': (148, 0, 211),      # Dark Purple (Contrasts well with skin tones)\n    'not_hand': (255, 80, 80),  # Light Red to indicate negative class\n\n    'trash': (0, 255, 0),       # Bright Green (For trash-related items)\n    'not_trash': (255, 80, 80), # Light Red to indicate negative class\n\n    'trash_arm': (255, 140, 0), # Deep Orange (Highly visible)\n}\n\nLet‚Äôs check out what these colours look like!\nIt‚Äôs the ABV motto: Always Be Visualizing!\nWe can plot our colours with matplotlib.\nWe‚Äôll just have to write a small function to normalize our colour values from [0, 255] to [0, 1] (matplotlib expects our colour values to be between 0 and 1).\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Normalize RGB values to 0-1 range\ndef normalize_rgb(rgb_tuple):\n    return tuple(x/255 for x in rgb_tuple)\n\n# Turn colors into normalized RGB values for matplotlib\ncolors_and_labels_rgb = [(key, normalize_rgb(value)) for key, value in colour_palette.items()]\n\n# Create figure and axis\nfig, ax = plt.subplots(1, 7, figsize=(8, 1))\n\n# Flatten the axis array for easier iteration\nax = ax.flatten()\n\n# Plot each color square\nfor idx, (label, color) in enumerate(colors_and_labels_rgb):\n    ax[idx].add_patch(plt.Rectangle(xy=(0, 0), \n                                    width=1, \n                                    height=1, \n                                    facecolor=color))\n    ax[idx].set_title(label)\n    ax[idx].set_xlim(0, 1)\n    ax[idx].set_ylim(0, 1)\n    ax[idx].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSensational!\nNow we know what colours to look out for when we visualize our bounding boxes.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#tk---different-bounding-box-formats",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#tk---different-bounding-box-formats",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "5 TK - Different bounding box formats",
    "text": "5 TK - Different bounding box formats\nWhen drawing our bounding box, we discussed the terms XYXY and XYWH.\nWell, we didn‚Äôt really discuss these at all‚Ä¶\nBut that‚Äôs why we‚Äôre here.\nOne of the most confusing things in the world of object detection is the different formats bounding boxes come in.\nAre your boxes in XYXY, XYWH or CXCYWH?\nAre they in absolute format?\nOr normalized format?\nPerhaps a table will help us.\nThe following table contains a non-exhaustive list of some of the most common bounding box formats you‚Äôll come across in the wild.\n\n\n\nTable¬†1: Different bounding box formats\n\n\n\n\n\nBox format\nDescription\nAbsolute Example\nNormalized Example\nSource\n\n\n\n\nXYXY\nDescribes the top left corner coordinates (x1, y1) as well as the bottom right corner coordinates of a box.  Also referred to as:  [x1, y1, x2, y2]  or  [x_min, y_min, x_max, y_max]\n[8.9, 275.3, 867.5, 964.0]\n[0.009, 0.215, 0.904, 0.753]\nPASCAL VOC Dataset uses the absolute version of this format, torchvision.utils.draw_bounding_boxes defaults to the absolute version of this format.\n\n\nXYWH\nDescribes the top left corner coordinates (x1, y1) as well as the width (box_width) and height (box_height) of the target box. The bottom right corners (x2, y2) are found by adding the width and height to the top left corner coordinates (x1 + box_width, y1 + box_height).  Also referred to as:  [x1, y1, box_width, box_height]  or  [x_min, y_min, box_width, box_height]\n[8.9, 275.3, 858.6, 688.7]\n[0.009, 0.215, 0.894, 0.538]\nThe COCO (Common Objects in Context) dataset uses the absolute version of this format, see the section under ‚Äúbbox‚Äù.\n\n\nCXCYWH\nDescribes the center coordinates of the bounding box (center_x, center_y) as well as the width (box_width) and height (box_height) of the target box.  Also referred to as:  [center_x, center_y, box_width, box_height]\n[438.2, 619.65, 858.6, 688.7]\n[0.456, 0.484, 0.894, 0.538]\nNormalized version introduced in the YOLOv3 (You Only Look Once) paper and is used by many later forms of YOLO.\n\n\n\n\n\n\n\n5.1 Absolute or normalized format?\nIn absolute coordinate form, bounding box values are in the same format as the width and height dimensions (e.g.¬†our image is 960x1280 pixels).\nFor example in XYXY format: [\"bin\", 8.9, 275.3, 867.5, 964.0]\nAn (x1, y1) (or (x_min, y_min)) coordinate of (8.9, 275.3) means the top left corner is 8.9 pixels in on the x-axis, and 275.3 pixels down on the y-axis.\nIn normalized coordinate form, values are between [0, 1] and are proportions of the image width and height.\nFor example in XYXY format: [\"bin\", 0.009, 0.215, 0.904, 0.753]\nA normalized (x1, y1) (or (x_min, y_min)) coordinate of (0.009, 0.215) means the top left corner is 0.009 * image_width pixels in on the x-axis and 0.215 * image_height down on the y-axis.\nTo convert absolute coordinates to normalized, you can divide x-axis values by the image width and y-axis values by the image height.\n\\[\nx_{\\text{normalized}} = \\frac{x_{\\text{absolute}}}{\\text{image\\_width}} \\quad y_{\\text{normalized}} = \\frac{y_{\\text{absolute}}}{\\text{image\\_height}}\n\\]\n\n\n5.2 Which bounding box format should you use?\nThe bounding box format you use will depend on the framework, model and existing data you‚Äôre trying to use.\nFor example, the take the following frameworks:\n\nPyTorch - If you‚Äôre using PyTorch pre-trained models, for example, torchvision.models.detection.fasterrcnn_resnet50_fpn, you‚Äôll want absolute XYXY ([x1, y1, x2, y2]) format.\nHugging Face Transformers - If you‚Äôre using a Hugging Face Transformers model such as Conditional DETR, you‚Äôll want to take note that outputs from the model can be of one type (e.g.¬†CXCYWH) but they can be post-processed into another type (e.g.¬†absolute XYXY).\nUltralytics YOLO - If you‚Äôre using a YOLO-like model such as Ultralytics YOLO, you‚Äôll want normalized CXCYWH ([center_x, center_y, width, height]) format.\nGoogle Gemini - If you‚Äôre using Google Gemini to predict bounding boxes on your images, then you‚Äôll want to pay attention to the special [y_min, x_min, y_max, x_max] (YXYX) normalized coordinates.\n\nOr if you note that someone has said their model is pre-trained on the COCO dataset, chances are the data has been formatted in XYWH format (see Table¬†1).\n\n\n\n\n\n\nNote\n\n\n\nFor more on different bounding box formats and how to draw them, see A Guide to Bounding Box Formats and How to Draw Them.\n\n\n\n# TK - should I functionize the plotting of boxes and image so we can do input/output with tensors + data augmentations on that (E.g. original: image, augmented: image),\n# - is this needed?",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#summary",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#summary",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "19 Summary",
    "text": "19 Summary\n\nBounding box formats: An important step in any object detection project is to figure out what format your bounding boxes are in.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#plotting-a-single-image-and-visualizing-the-boxes",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#plotting-a-single-image-and-visualizing-the-boxes",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "4 Plotting a single image and visualizing the boxes",
    "text": "4 Plotting a single image and visualizing the boxes\n\n# Plot a single image\nimport random\n\nimport torch\n\nfrom torchvision.ops import box_convert\nfrom torchvision.utils import draw_bounding_boxes\n\nfrom torchvision.transforms.functional import pil_to_tensor, to_pil_image \n\nfrom PIL import ImageFont\n\nrandom_index = random.randint(0, len(dataset[\"train\"]))\nprint(f\"[INFO] Showing training sample from index: {random_index}\")\nrandom_sample = dataset[\"train\"][random_index]\n\n# Get box coordinates\nboxes_xywh = torch.tensor(random_sample[\"annotations\"][\"bbox\"])\nprint(f\"Boxes in XYWH format: {boxes_xywh}\")\n\n# Convert boxes from XYWH -&gt; XYXY \n# torchvision.utils.draw_bounding_boxes requires input boxes in XYXY format (X_min, y_min, X_max, y_max)\nboxes_xyxy = box_convert(boxes=boxes_xywh,\n                         in_fmt=\"xywh\",\n                         out_fmt=\"xyxy\")\nprint(f\"Boxes XYXY: {boxes_xyxy}\")\n\n# Get label names of target boxes and colours to match\nrandom_sample_label_names = [categories.int2str(x) for x in random_sample[\"annotations\"][\"category_id\"]]\nrandom_sample_colours = [colour_palette[label_name] for label_name in random_sample_label_names]\nprint(f\"Label names: {random_sample_label_names}\")\nprint(f\"Colour names: {random_sample_colours}\")\n\n# Draw the image as a tensor and then turn it into a PIL image\nto_pil_image(\n    pic=draw_bounding_boxes(\n        image=pil_to_tensor(pic=random_sample[\"image\"]),\n        boxes=boxes_xyxy,\n        colors=random_sample_colours,\n        labels=random_sample_label_names,\n        width=3,\n        # font=font_filename,\n        font_size=30,\n        label_colors=random_sample_colours\n    )\n)",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#tk---plotting-a-single-image-and-visualizing-the-boxes",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#tk---plotting-a-single-image-and-visualizing-the-boxes",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "4 TK - Plotting a single image and visualizing the boxes",
    "text": "4 TK - Plotting a single image and visualizing the boxes\nOkay, okay, finally time to plot an image!\nLet‚Äôs take a random sample from our dataset and plot the image as well as the box on it.\nTo save some space in our notebook (plotting many images can increase the size of our notebook dramatically), we‚Äôll create two small helper functions:\n\nhalf_image - Halves the size of a given image.\nhalf_boxes - Divides the input coordinates of a given input box by 2.\n\nThese functions aren‚Äôt 100% necessary in our workflow.\nThey‚Äôre just to make the images slightly smaller so they fit better in the notebook.\n\nimport PIL\n\ndef half_image(image: PIL.Image) -&gt; PIL.Image:\n    \"\"\"\n    Resizes a given input image by half and returns the smaller version.\n    \"\"\"\n    return image.resize(size=(image.size[0] // 2, image.size[1] // 2))\n\ndef half_boxes(boxes):\n    \"\"\"\n    Halves an array of input boxes and returns them. Necessary for plotting them on a half-sized image.\n\n    For example:\n\n    boxes = [100, 100, 100, 100]\n    half_boxes = half_boxes(boxes)\n    print(half_boxes)\n\n    &gt;&gt;&gt; [50, 50, 50, 50]\n    \"\"\"\n    return np.array(boxes) // 2\n\n# Test the functions \nimage_test = dataset[\"train\"][42][\"image\"]\nimage_test_half = half_image(image_test)\nprint(f\"[INFO] Original image size: {image_test.size} | Half image size: {image_test_half.size}\")\n\nboxes_test = [100, 100, 100, 100]\nprint(f\"[INFO] Original boxes: {boxes_test} | Half boxes: {half_boxes(boxes_test)}\")\n\n[INFO] Original image size: (960, 1280) | Half image size: (480, 640)\n[INFO] Original boxes: [100, 100, 100, 100] | Half boxes: [50 50 50 50]\n\n\nTo plot an image and its assosciated boxes, we‚Äôll do the following steps:\n\nSelect a random sample from the dataset.\nExtract the \"image\" (our image is in PIL format) and \"bbox\" keys from the random sample.\n\nWe can also optionally halve the size of our image/boxes to save space. In our case, we will halve our image and boxes.\n\nTurn the box coordinates into a torch.tensor (we‚Äôll be using torchvision utilities to plot the image and boxes).\nConvert the box format from XYXY to XYWH using torchvision.ops.box_convert (we do this because torchvision.utils.draw_bounding_boxes requires XYXY format as input).\nGet a list of label names (e.g.¬†\"bin\", \"trash\", etc) assosciated with each of the boxes as well as a list of colours to match (these will be from our colour_palette).\nDraw the boxes on the target image by:\n\nTurning the image into a tensor with torchvision.transforms.functional.pil_to_tensor.\nDraw the bounding boxes on our image tensor with torchvision.utils.draw_bounding_boxes.\nTurn the image and bounding box tensors back into a PIL image with torchvision.transforms.functional.pil_to_tensor.\n\n\nPhew!\nA fair few steps‚Ä¶\nBut we‚Äôve got this!\n\n\n\n\n\n\nNote\n\n\n\nIf the terms XYXY or XYWH or all of the drawing methods sound a bit confusing or intimidating, don‚Äôt worry, there‚Äôs a fair bit going on here.\nWe‚Äôll cover bounding box formats, such as XYXY shortly.\nIn the meantime, if you want to learn more about different bounding box formats and how to draw them, I wrote A Guide to Bounding Box Formats and How to Draw Them which you might find helpful.\n\n\n\n# Plotting a bounding box on a single image\nimport random\n\nimport torch\n\nfrom torchvision.ops import box_convert\nfrom torchvision.utils import draw_bounding_boxes\n\nfrom torchvision.transforms.functional import pil_to_tensor, to_pil_image \n\n# 1. Select a random sample from our dataset\nrandom_index = random.randint(0, len(dataset[\"train\"]))\nprint(f\"[INFO] Showing training sample from index: {random_index}\")\nrandom_sample = dataset[\"train\"][random_index]\n\n# 2. Get image and boxes from random sample\nrandom_sample_image = random_sample[\"image\"]\nrandom_sample_boxes = random_sample[\"annotations\"][\"bbox\"]\n\n# Optional: Half the image and boxes for space saving (all of the following code will work with/without half size images)\nhalf_random_sample_image = half_image(random_sample_image)\nhalf_random_sample_boxes = half_boxes(random_sample_boxes)\n\n# 3. Turn box coordinates in a tensor\nboxes_xywh = torch.tensor(half_random_sample_boxes)\nprint(f\"Boxes in XYWH format: {boxes_xywh}\")\n\n# 4. Convert boxes from XYWH -&gt; XYXY \n# torchvision.utils.draw_bounding_boxes requires input boxes in XYXY format (X_min, y_min, X_max, y_max)\nboxes_xyxy = box_convert(boxes=boxes_xywh,\n                         in_fmt=\"xywh\",\n                         out_fmt=\"xyxy\")\nprint(f\"Boxes XYXY: {boxes_xyxy}\")\n\n# 5. Get label names of target boxes and colours to match\nrandom_sample_label_names = [categories.int2str(x) for x in random_sample[\"annotations\"][\"category_id\"]]\nrandom_sample_colours = [colour_palette[label_name] for label_name in random_sample_label_names]\nprint(f\"Label names: {random_sample_label_names}\")\nprint(f\"Colour names: {random_sample_colours}\")\n\n# 6. Draw the boxes on the image as a tensor and then turn it into a PIL image\nto_pil_image(\n    pic=draw_bounding_boxes(\n        image=pil_to_tensor(pic=half_random_sample_image),\n        boxes=boxes_xyxy,\n        colors=random_sample_colours,\n        labels=random_sample_label_names,\n        width=2,\n        label_colors=random_sample_colours\n    )\n)\n\n[INFO] Showing training sample from index: 231\nBoxes in XYWH format: tensor([[236., 274.,  68.,  75.],\n        [230., 326., 222., 300.],\n        [  0., 143., 477., 385.]], dtype=torch.float64)\nBoxes XYXY: tensor([[236., 274., 304., 349.],\n        [230., 326., 452., 626.],\n        [  0., 143., 477., 528.]], dtype=torch.float64)\nLabel names: ['trash', 'hand', 'bin']\nColour names: [(0, 255, 0), (148, 0, 211), (0, 0, 224)]\n\n\n\n\n\n\n\n\n\nOutstanding!\nOur first official bounding boxes plotted on an image!\nNow the idea of Trashify üöÆ is coming to life.\nDepending on the random sample you‚Äôre looking at, you should see some combination of ['bin', 'hand', 'not_bin', 'not_hand', 'not_trash', 'trash', 'trash_arm'].\nOur goal will be to build an object detection model to replicate these boxes on a given image.\n\n\n\n\n\n\nGetting familiar with a dataset: viewing 100 random samples\n\n\n\nWhenever working with a new dataset, I find it good practice to view 100+ random samples of the data.\nIn our case, this would mean viewing 100 random images with their bounding boxes drawn on them.\nDoing so starts to build your own intuition of the data.\nUsing this intuition, along with evaluation metrics, you can start to get a better idea of how your model might be performing later on.\nKeep this in mind for any new dataset or problem space you‚Äôre working on.\nStart by looking at 100+ random samples.\nAnd yes, generally more is better.\nSo you can practice by running the code cell above a number of times to see the different kinds of images and boxes in the dataset.\nCan you think of any scenarios which the dataset might be missing?",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#getting-an-object-detection-model",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#getting-an-object-detection-model",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "6 Getting an object detection model",
    "text": "6 Getting an object detection model\nThere are two main ways of getting an object detection model:\n\nBuilding it yourself. For example, constructing it layer by layer, testing it and training it on your target problem.\nUsing an existing one. For example, find an existing model on a problem space similar to your own and then adapt it via transfer learning (TK - add link to glossary) to your own task.\n\nIn our case, we‚Äôre going to focus on the latter.\nWe‚Äôll be taking a pre-trained object detection model and fine-tuning it on our Trashify üöÆ dataset so it outputs the boxes and labels we‚Äôre after.\n\n6.1 Places to get object detection models\nInstead of building your own machine learning model from scratch, it‚Äôs common practice to take an existing model that works on similar problem space to yours and then fine-tune (TK - add link to glossary) it to your own use case.\nThere are several places to get object detection models:\n\n\n\nTable¬†2: Places to get pre-trained object detection models\n\n\n\n\n\nLocation\nDescription\n\n\n\n\nHugging Face Hub\nOne the best places on the internet to find open-source machine learning models of nearly any kind. You can find pre-trained object detection models here such as facebook/detr-resnet-50, a model from Facebook (Meta) and microsoft/conditional-detr-resnet-50, a model from Microsoft and the model we‚Äôre going to use as our base model. Many of the models are permissively licensed, meaning you can use them for your own projects.\n\n\ntorchvision\nPyTorch‚Äôs built-in domain library for computer vision has several pre-trained object detection models which you can use in your own workflows.\n\n\npaperswithcode.com/task/object-detection\nWhilst not a direct place to download object detection models from, paperswithcode contains benchmarks for many machine learning tasks (including object detection) which shows the current state of the art (best performing) models and usually includes links to where to get the code.\n\n\nDetectron2\nDetectron2 is an open-source library to help with many of the tasks in detecting items in images. Inside you‚Äôll find several pre-trained and adaptable models as well as utilities such as data loaders for object detection and segmentation tasks.\n\n\nYOLO Series\nA running series of ‚ÄúYou Only Look Once‚Äù models. Usually, the higher the number, the better performing. For example, YOLOv11 by Ultralytics should outperform YOLOv10, however, this often requires testing on your own dataset. Beware of the license, it is under the AGPL-3.0 license which may cause issues in some organizations.\n\n\nmmdetection library\nAn open-source library from the OpenMMLab which contains many different open-source models as well as detection-specific utilties.\n\n\n\n\n\n\nWhen you find a pre-trained object detection model, you‚Äôll often see statements such as:\n\nConditional DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images).\nSource: https://huggingface.co/microsoft/conditional-detr-resnet-50\n\nThis means the model has already been trained on the COCO object detection dataset which contains 118,000 images and 80 classes such as [\"cake\", \"person\", \"skateboard\"...].\nThis is a good thing.\nIt means that the model should have a fairly good starting point when we try to adapt it to our own project.\n\n\n6.2 Downloading our model from Hugging Face\nFor our Trashify üöÆ project we‚Äôre going to be using the pre-trained object detection model microsoft/conditional-detr-resnet-50 which was originally introduced in the paper Conditional DETR for Fast Training Convergence.\n\n\n\n\n\n\nNote\n\n\n\nThe term ‚ÄúDETR‚Äù stands for ‚ÄúDEtection TRansformer‚Äù.\nWhere ‚ÄúTransformer‚Äù refers to the Transformer neural network architecture, specifically the Vision Transformer (or ViT) rather than the Hugging Face transformers library (quite confusing, yes).\nSo DETR means ‚Äúperforming detection with the Transformer architecture‚Äù.\nAnd the ‚ÄúResNet‚Äù part stands for ‚ÄúResidual Neural Network‚Äù which is a common computer vision backbone. The ‚Äú50‚Äù refers to the number of layers in the network. Saying ‚ÄúResNet-50‚Äù means the 50 layer version of ResNet. ResNet-101 and ResNet-18 are two other larger and smaller variants.\n\n\nTo use this model, there are some helpful documentation resources we should be aware of:\n\n\n\nTable¬†3: Model documentation resources\n\n\n\n\n\nResource\nDescription\n\n\n\n\nConditional DETR documentation\nContains detailed information on each of the transformers.ConditionalDetr classes.\n\n\ntransformers.ConditionalDetrConfig\nContains the configuration settings for our model such as number of layers and other hyperparameters.\n\n\ntransformers.ConditionalDetrImageProcessor\nContains several preprocessing on post processing functions and settings for data going into and out of our model. Here we can set values such as size in the preprocess method which will resize our images to a certain size. We can also use the post_process_object_detection method to process the raw outputs of our model into a more usable format.\n\n\ntransformers.ConditionalDetrModelForObjectdetection\nThis will enable us to load the Conditional DETR model weights and enable to pass data through them via the forward method.\n\n\ntransformers.AutoImageProcessor\nThis will enable us to create an instance of transformers.ConditionalDetrImageProcessor by passing the model name microsoft/conditional-detr-resnet-50 to the from_pretrained method. Hugging Face Transformers uses several Auto Classes for various problem spaces and models.\n\n\ntransformers.AutoModelForObjectDetection\nEnables us to load the model architecture and weights for the Conditional DETR architecture by passing the model name microsoft/conditional-detr-resnet-50 to the from_pretrained method.\n\n\n\n\n\n\nWe‚Äôll get hands-on which each of these throughout the project.\nFor now, if you‚Äôd like to read up more on each, I‚Äôd highly recommend it.\nKnowing how to navigate and read through a framework‚Äôs documentation is a very helpful skill to have.\n\n\n\n\n\n\nNote\n\n\n\nThere are other object detection models we could try on the Hugging Face Hub such as facebook/detr-resnet-50 or IDEA-Research/dab-detr-resnet-50-dc5-pat3.\nFor now, we‚Äôll stick with microsoft/conditional-detr-resnet-50.\nIt‚Äôs easy to get stuck figuring out which model to use instead of just trying one and seeing how it goes.\nBest to get something small working with one model and try another one later as part of a series of experiments to try and improve your results.\n\n\n\n\n6.3 Loading our model‚Äôs processor\nTo begin, let‚Äôs first load our model‚Äôs processor.\nWe‚Äôll use this to prepare our input images for the model.\nTo do so, we‚Äôll use transformers.AutoImageProcessor and pass our target model name to the from_pretrained method.\n\nfrom transformers import AutoImageProcessor\n\nMODEL_NAME = \"microsoft/conditional-detr-resnet-50\"\n# MODEL_NAME = \"facebook/detr-resnet-50\" # Could also use this model as an another experiment\n\n# Load the image processor\nimage_processor = AutoImageProcessor.from_pretrained(pretrained_model_name_or_path=MODEL_NAME)\n\n# Check out the image processor\nimage_processor\n\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n\n\nConditionalDetrImageProcessor {\n  \"do_convert_annotations\": true,\n  \"do_normalize\": true,\n  \"do_pad\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"format\": \"coco_detection\",\n  \"image_mean\": [\n    0.485,\n    0.456,\n    0.406\n  ],\n  \"image_processor_type\": \"ConditionalDetrImageProcessor\",\n  \"image_std\": [\n    0.229,\n    0.224,\n    0.225\n  ],\n  \"pad_size\": null,\n  \"resample\": 2,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"longest_edge\": 1333,\n    \"shortest_edge\": 800\n  }\n}\n\n\nOk, a few things going on here.\nThese parameters will transform our input images before we pass them to our model.\nOne of the first things to see is the image_processor is expecting our bounding boxes to be in COCO (or coco_detection) format (this is the default).\nWe‚Äôll see what this looks like later on but our processor wants this format because that‚Äôs the format our model has been trained on (it‚Äôs generally best practice to input data to a model in the same way its been trained on, otherwise you might get poor results).\nAnother thing to notice is that our input images will be resized to the values of the size parameter.\nIn our case, it‚Äôs currently:\n\"size\": {\n    \"longest_edge\": 1333,\n    \"shortest_edge\": 800\n}\nWhich means that the longest edge will have size less or equal to 1333 and the shortest edge less or equal to 800.\nFor simplicity, we‚Äôll change this shortly to make both sides the same size.\nYou can read more about what each of these does in the transformers.ConditionalDetrImageProcessor documentation.\nLet‚Äôs update our instance of transformers.ConditionalDetrImageProcessor with a few custom parameters:\n\ndo_convert_annotations=True - This is the default and it will convert our boxes to the format CXCYWH or (center_x, center_y, width, height) (see Table¬†1) in the range [0, 1].\nsize - We‚Äôll update the size dictionary so all of our images have \"longest_edge\": 640 and \"shortest_edge: 640\". We‚Äôll use a value of 640 which is a common size in world of object detection. But there are also other sizes such as 300x300, 480x480, 512x512, 800x800 and more.\n\n\n\n\n\n\n\nNote\n\n\n\nDepending on what task you‚Äôre working on, you might want to tweak the image resolution you‚Äôre working with.\nFor example, I like this quote from Lucas Beyer, a former research scientist at DeepMind and engineer at OpenAI:\n\nMy conservative claim is that you can always stretch to a square, and for:\nnatural images, meaning most photos, 224px¬≤ is enough; text in photos, phone screens, diagrams and charts, 448px¬≤ is enough; desktop screens and single-page documents, 896px¬≤ is enough.\n\nTypically, in the case of object detection, you‚Äôll want to use a higher value.\nBut this is another thing that is open to experimentation.\n\n\n\n# Set image size\nIMAGE_SIZE = 640 # we could try other sizes here: 300x300, 480x480, 512x512, 640x640, 800x800 (best to experiment and see which works best)\n\n# Create a new instance of the image processor with the desired image size\nimage_processor = AutoImageProcessor.from_pretrained(\n    pretrained_model_name_or_path=MODEL_NAME,\n    format=\"coco_detection\", # this is the default\n    do_convert_annotations=True, # defaults to True, converts boxes to (center_x, center_y, width, height) in range [0, 1]\n    size={\"shortest_edge\": IMAGE_SIZE, \n          \"longest_edge\": IMAGE_SIZE}\n)\n\n# Optional: View the docstring of our image_processor.preprocess function\n# image_processor.preprocess?\n\n# Check out our new image processor size\nimage_processor.size\n\n{'shortest_edge': 640, 'longest_edge': 640}\n\n\nBeautiful!\nNow our images will be resized to a square of size 640x640 when we pass them to our model.\nHow about we try to preprocess our random_sample?\nTo do so, we can pass its \"image\" key and \"annotations\" key to our image_processor‚Äôs preprocess method.\nLet‚Äôs try!\n\n# Try to process a single image and annotation pair (spoiler: this will error)\nrandom_sample_preprocessed = image_processor.preprocess(images=random_sample[\"image\"],\n                                                        annotations=random_sample[\"annotations\"])\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[16], line 2\n      1 # Try to process a single image and annotation pair (spoiler: this will error)\n----&gt; 2 random_sample_preprocessed = image_processor.preprocess(images=random_sample[\"image\"],\n      3                                                         annotations=random_sample[\"annotations\"])\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/conditional_detr/image_processing_conditional_detr.py:1421, in ConditionalDetrImageProcessor.preprocess(self, images, annotations, return_segmentation_masks, masks_path, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, do_convert_annotations, image_mean, image_std, do_pad, format, return_tensors, data_format, input_data_format, pad_size, **kwargs)\n   1419 format = AnnotationFormat(format)\n   1420 if annotations is not None:\n-&gt; 1421     validate_annotations(format, SUPPORTED_ANNOTATION_FORMATS, annotations)\n   1423 if (\n   1424     masks_path is not None\n   1425     and format == AnnotationFormat.COCO_PANOPTIC\n   1426     and not isinstance(masks_path, (pathlib.Path, str))\n   1427 ):\n   1428     raise ValueError(\n   1429         \"The path to the directory containing the mask PNG files should be provided as a\"\n   1430         f\" `pathlib.Path` or string object, but is {type(masks_path)} instead.\"\n   1431     )\n\nFile ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/image_utils.py:1269, in validate_annotations(annotation_format, supported_annotation_formats, annotations)\n   1267 if annotation_format is AnnotationFormat.COCO_DETECTION:\n   1268     if not valid_coco_detection_annotations(annotations):\n-&gt; 1269         raise ValueError(\n   1270             \"Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts \"\n   1271             \"(batch of images) with the following keys: `image_id` and `annotations`, with the latter \"\n   1272             \"being a list of annotations in the COCO format.\"\n   1273         )\n   1275 if annotation_format is AnnotationFormat.COCO_PANOPTIC:\n   1276     if not valid_coco_panoptic_annotations(annotations):\n\nValueError: Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts (batch of images) with the following keys: `image_id` and `annotations`, with the latter being a list of annotations in the COCO format.\n\n\n\nOh no!\nWe get an error:\n\nValueError: Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts (batch of images) with the following keys: image_id and annotations, with the latter being a list of annotations in the COCO format.\n\nOkay so it turns out that our annotations aren‚Äôt in the format that the preprocess method was expecting.\nSince our pre-trained model was trained on the COCO dataset, the preprocess method expects input data to be in line with the COCO format.\nWe can fix this later on by adjusting our annotations.\nHow about we try to preprocess just a single image instead?\n\n# Preprocess our target sample\nrandom_sample_preprocessed_image_only = image_processor.preprocess(images=random_sample[\"image\"],\n                                                                   annotations=None, # no annotations this time \n                                                                   return_tensors=\"pt\") # return as PyTorch tensors\n\n# Uncomment to see the full output\n# print(random_sample_preprocessed_image_only)\n\n# Print out the keys of the preprocessed image\nprint(random_sample_preprocessed_image_only.keys())\n\ndict_keys(['pixel_values', 'pixel_mask'])\n\n\nNice! It looks like the preprocess method works on a single image.\nAnd it seems like we get a dictionary output with the following keys:\n\npixel_values - the processed pixel values of the input image.\npixel_mask - a mask multiplier for the pixel values as to whether they should be paid attention to or not (a value of 0 means the pixel value should be ignored by the model and a value of 1 means the pixel value should be paid attention to by the model).\n\nIn our case, all values of the pixel_mask are 1 since we‚Äôre not using any masks.\nLet‚Äôs check.\n\n# Check all values of the pixel_mask are 1\ntorch.all(random_sample_preprocessed_image_only[\"pixel_mask\"][0]) == 1\n\ntensor(True)\n\n\nBeautiful!\nNow how about we inspect our processed image‚Äôs shape?\n\n# Uncomment to inspect all preprocessed pixel values\n# print(random_sample_preprocessed_image_only[\"pixel_values\"][0])\n\nprint(f\"[INFO] Original image shape: {random_sample['image'].size} -&gt; [width, height]\")\nprint(f\"[INFO] Preprocessed image shape: {random_sample_preprocessed_image_only['pixel_values'][0].shape} -&gt; [colour_channles, height, width]\")\n\n[INFO] Original image shape: (960, 1280) -&gt; [width, height]\n[INFO] Preprocessed image shape: torch.Size([3, 640, 480]) -&gt; [colour_channles, height, width]\n\n\nOk wonderful, it looks like our image has been downsized to [3, 640, 480] (3 colour channels, 640 pixels high, 480 pixels wide).\nThis is down from its original size of [960, 1280] (1280 pixels high, 960 pixels wide).\n\n\n\n\n\n\nNote\n\n\n\nThe order of image dimensions can differ between libraries and frameworks.\nFor example, image tensors in PyTorch typically follow the format [colour_channels, height, width] whereas in TensorFlow they follow [height, width, colour_channels].\nAnd in PIL, the format is [width, height].\nAs you can imagine, this can get confusing.\nHowever, with some practice, you‚Äôll be able to decipher which is which.\nAnd if your images and bounding boxes start looking strange, perhaps checking the image dimension and format can help.\n\n\n\n\n6.4 Discussing how to convert our annotations into COCO format\nOur image_processor.processor method expects input annotations in COCO format.\nIn the documentation we can read that the annotations parameter taks in a list of dictionaries with the following keys:\n\n\"image_id\" (int): The image id.\n\"annotations\" (List[Dict]): List of annotations for an image. Each annotation should be a dictionary. An image can have no annotations, in which case the list should be empty.\n\nAs for the \"annotations\" field, this should be a list of dictionaries containing individual annotations in COCO format:\n# COCO format, see: https://cocodataset.org/#format-data  \n[{\n    \"image_id\": 42,\n    \"annotations\": [{\n        \"id\": 123456,\n        \"category_id\": 1,\n        \"iscrowd\": 0,\n        \"segmentation\": [\n            [42.0, 55.6, ... 99.3, 102.3]\n        ],\n        \"image_id\": 42, # this matches the 'image_id' field above\n        \"area\": 135381.07,\n        \"bbox\": [523.70,\n                 545.09,\n                 402.79,\n                 336.11]\n    },\n    # Next annotation in the same format as the previous one (one annotation per dict).\n    # For example, if an image had 4 bounding boxes, there would be a list of 4 dictionaries\n    # each containing a single annotation.\n    ...]\n}]\nLet‚Äôs breakdown each of the fields in the COCO annotation:\n\n\n\nTable¬†4: COCO data format keys breakdown\n\n\n\n\n\n\n\n\n\n\n\nField\nRequirement\nData Type\nDescription\n\n\n\n\nimage_id (top-level)\nRequired\nInteger\nID of the target image.\n\n\nannotations\nRequired\nList[Dict]\nList of dictionaries with one box annotation per dict. Can be empty if there are no boxes.\n\n\nid\nNot required\nInteger\nID of the particular annotation.\n\n\ncategory_id\nRequired\nInteger\nID of the class the box relates to (e.g.¬†{0: 'bin', 1: 'hand', 2: 'not_bin', 3: 'not_hand', 4: 'not_trash', 5: 'trash'}).\n\n\nsegmentation\nNot required\nList or None\nSegmentation mask related to an annotation instance. Focus is on boxes, not segmentation.\n\n\nimage_id (inside annotations field)\nRequired\nInteger\nID of the target image the particular box relates to, should match image_id on the top-level field.\n\n\narea\nNot required\nFloat\nArea of the target bounding box (e.g.¬†box height * width).\n\n\nbbox\nRequired\nList[Float]\nCoordinates of the target bounding box in XYWH ([x, y, width, height]) format. (x, y) are the top left corner coordinates, width and height are dimensions.\n\n\nis_crowd\nNot required\nInt\nBoolean flag (0 or 1) to indicate whether or not an object is multiple (a crowd) of the same thing. For example, a crowd of ‚Äúpeople‚Äù or a group of ‚Äúapples‚Äù rather than a single apple.\n\n\n\n\n\n\nAnd now our annotation data comes in the format:\n{'image': &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,\n 'image_id': 292,\n 'annotations': {'file_name': ['00347467-13f1-4cb9-94aa-4e4369457e0c.jpeg',\n   '00347467-13f1-4cb9-94aa-4e4369457e0c.jpeg'],\n  'image_id': [292, 292],\n  'category_id': [1, 0],\n  'bbox': [[523.7000122070312,\n    545.0999755859375,\n    402.79998779296875,\n    336.1000061035156],\n   [10.399999618530273,\n    163.6999969482422,\n    943.4000244140625,\n    1101.9000244140625]],\n  'iscrowd': [0, 0],\n  'area': [135381.078125, 1039532.4375]},\n 'label_source': 'manual_prodigy_label',\n 'image_source': 'manual_taken_photo'}\nHow about we write some code to convert our current annotation format to COCO format?\n\n\n\n\n\n\nNote\n\n\n\nIt‚Äôs common practice to get a dataset in a certain format and then have to preprocess it into another format before you can use it with a model.\nWe‚Äôre getting hands-on and practicing here so when it comes to working on converting another dataset, you‚Äôve already had some practice.\n\n\n\n\n6.5 TK - Creating dataclasses to represent the COCO bounding box format\nLet‚Äôs write some code to transform our existing annotation data into the format required by image_processor.\nWe‚Äôll start by creating two Python dataclasses to house our desired COCO annotation format.\nTo do this we‚Äôll:\n\nCreate SingleCOCOAnnotation which contains the format structure of a single COCO annotation.\nCreate ImageCOCOAnnotations which contains all of the annotations for a given image in COCO format. This may be a single instance of SingleCOCOAnnotation or multiple.\n\nWe‚Äôll decorate both of these with the @dataclass decorator.\nUsing a @dataclass gives several benefits:\n\nType hints - we can define the types of objects we want in the class definition, for example, we want image_id to be an int.\nHelpful built-in methods - we can use methods such as asdict to convert our @dataclass into a dictionary (COCO wants lists of dictionaries).\nData validation - we can use methods such as __post_init__ to run checks on our @dataclass as it‚Äôs initialized, for example, we always want the length of bbox to be 4 (bounding box coordinates in XYWH format).\n\n\nfrom dataclasses import dataclass, asdict\nfrom typing import List, Tuple, Optional\n\n# 1. Create a dataclass for a single COCO annotation\n@dataclass\nclass SingleCOCOAnnotation:\n    \"\"\"An instance of a single COCO annotation. \n    \n    Represent a COCO-formatted (see: https://cocodataset.org/#format-data) single instance of an object \n    in an image. \n\n    Attributes:\n        image_id: Unique integer identifier for the image which the annotation belongs to.\n        category_id: Integer identifier for the target object label/category (e.g. \"0\" for \"bin\").\n        bbox: List of floats containing target bounding box coordinates in absolute XYWH format ([x_top_left, y_top_left, width, height]).\n        area: Optional; Area of the target bounding box. Defaults to None.\n        iscrowd: Optional; Boolean flag (0 or 1) indicating whether the target is a crowd of objects, for example, a group of \n            apples rather than a single apple. Defaults to None.\n    \"\"\"\n    image_id: int\n    category_id: int\n    bbox: List[float] # bboxes in XYWH format ([x_top_left, y_top_left, width, height])\n    area: Optional[float] = None\n    iscrowd: Optional[int] = None\n\n    # Make sure the bbox is always a list of 4 values (XYWH format)\n    def __post_init__(self):\n        if len(self.bbox) != 4:\n            raise ValueError(f\"bbox must contain exactly 4 values, current length: {len(self.bbox)}\")\n\n\n# 2. Create a dataclass for a collection of COCO annotations for a single image\n@dataclass\nclass ImageCOCOAnnotations:\n    \"\"\"A collection of COCO annotations for a single image_id.\n\n    Attributes:\n        image_id: Unique integer identifier for the image which the annotations belong to.\n        annotations: List of SingleCOCOAnnotation instances.\n    \"\"\"\n    image_id: int\n    annotations: List[SingleCOCOAnnotation]\n\nBeautiful!\nLet‚Äôs now inspect our SingleCOCOAnnotation dataclass.\nWe can use the SingleCOCOAnnotation? syntax to view the docstring of the class.\n\n# One of the benefits of using a dataclass is that we can inspect the attributes with the `?` syntax\nSingleCOCOAnnotation?\n\nInit signature:\nSingleCOCOAnnotation(\n    image_id: int,\n    category_id: int,\n    bbox: List[float],\n    area: Optional[float] = None,\n    iscrowd: Optional[int] = None,\n) -&gt; None\nDocstring:     \nAn instance of a single COCO annotation. \n\nRepresent a COCO-formatted (see: https://cocodataset.org/#format-data) single instance of an object \nin an image. \n\nAttributes:\n    image_id: Unique integer identifier for the image which the annotation belongs to.\n    category_id: Integer identifier for the target object label/category (e.g. \"0\" for \"bin\").\n    bbox: List of floats containing target bounding box coordinates in absolute XYWH format ([x_top_left, y_top_left, width, height]).\n    area: Optional; Area of the target bounding box. Defaults to None.\n    iscrowd: Optional; Boolean flag (0 or 1) indicating whether the target is a crowd of objects, for example, a group of \n        apples rather than a single apple. Defaults to None.\nType:           type\nSubclasses:     \n\n\nWe can also see the error handling of our __post_init__ method in action by trying to create an instance of SingleCOCOAnnotation with an incorrect number of bbox values.\n\n# Let's try our SingleCOCOAnnotation dataclass (this will error since the bbox doesn't have 4 values)\nSingleCOCOAnnotation(image_id=42, \n                     category_id=0, \n                     bbox=[100, 100, 100]) # missing a 4th value\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[34], line 2\n      1 # Let's try our SingleCOCOAnnotation dataclass (this will error since the bbox doesn't have 4 values)\n----&gt; 2 SingleCOCOAnnotation(image_id=42, \n      3                      category_id=0, \n      4                      bbox=[100, 100, 100]) # missing a 4th value\n\nFile &lt;string&gt;:8, in __init__(self, image_id, category_id, bbox, area, iscrowd)\n\nCell In[31], line 29, in SingleCOCOAnnotation.__post_init__(self)\n     27 def __post_init__(self):\n     28     if len(self.bbox) != 4:\n---&gt; 29         raise ValueError(f\"bbox must contain exactly 4 values, current length: {len(self.bbox)}\")\n\nValueError: bbox must contain exactly 4 values, current length: 3\n\n\n\nAnd now if we pass the correct number of values to our SingleCOCOAnnotation, it should work.\n\nSingleCOCOAnnotation(image_id=42, \n                     category_id=0, \n                     bbox=[100, 100, 100, 100])\n\nSingleCOCOAnnotation(image_id=42, category_id=0, bbox=[100, 100, 100, 100], area=None, iscrowd=None)\n\n\n\n\n6.6 Creating a function to format our annotations as COCO format\nNow we‚Äôve got the COCO data format in our SingleCOCOAnnotation and ImageCOCOAnnotation dataclasses, let‚Äôs write a function to take our existing image annotations and format them in COCO style.\nOur format_image_annotations_as_coco function will:\n\nTake in an image_id to represent a unique identifier for the image as well as lists of category integers, area values and bounding box coordinates.\nPerform a list comprehension on a zipped version of each category, area and bounding box coordinate value in the input lists creating an instance of SingleCOCOAnnotation as a dictionary (using the asdict method) each time, this will give us a list of SingleCOCOAnnotation formatted dictionaries.\nReturn a dictionary version of ImageCOCOAnnotations using asdict passing it the image_id as well as list of SingleCOCOAnnotation dictionaries from 2.\n\nWhy does our function take in lists of categories, areas and bounding boxes?\nBecause that‚Äôs the current format our existing annotations are in (how we downloaded them from Hugging Face).\nLet‚Äôs do it!\n\n# 1. Take in a unique image_id as well as lists of categories, areas, and bounding boxes\ndef format_image_annotations_as_coco(\n        image_id: int,\n        categories: List[int],\n        areas: List[float],\n        bboxes: List[Tuple[float, float, float, float]] # bboxes in XYWH format ([x_top_left, y_top_left, width, height])\n) -&gt; dict:\n    \"\"\"Formats lists of image annotations into COCO format.\n    \n    Takes in parallel lists of categories, areas, and bounding boxes and\n    then formats them into a COCO-style dictionary of annotations.\n\n    Args:\n        image_id: Unique integer identifier for an image.\n        categories: List of integer category IDs for each annotation.\n        areas: List of float areas for each annotation.\n        bboxes: List of tuples containing bounding box coordinates in XYWH format \n            ([x_top_left, y_top_left, width, height]).\n    \n    Returns:\n        A dictionary of image annotations in COCO format with the following structure:\n        {\n            \"image_id\": int,\n            \"annotations\": [\n                {\n                    \"image_id\": int,\n                    \"category_id\": int,\n                    \"bbox\": List[float],\n                    \"area\": float\n                },\n                ...more annotations here\n            ]\n        }\n    \n    Note:\n        All input lists much be the same length and in the same order.\n        Otherwise, there will be mismatched annotations.\n    \"\"\"\n    \n    # 2. Turn input lists into a list of dicts in SingleCOCOAnnotation format\n    coco_format_annotations = [\n        asdict(SingleCOCOAnnotation(\n            image_id=image_id,\n            category_id=category,\n            bbox=list(bbox),\n            area=area,\n        ))\n        for category, area, bbox in zip(categories, areas, bboxes)\n    ]\n\n    # 3. Return a of annotations with format {\"image_id\": ..., \"annotations\": [...]} (required COCO format)\n    return asdict(ImageCOCOAnnotations(image_id=image_id,\n                                       annotations=coco_format_annotations))\n\nNice!\nHaving those pre-built dataclasses makes everything else fall into place.\nNow let‚Äôs try our format_image_annotations_as_coco function on our random_sample from before.\nFirst, we‚Äôll remind ourselves what our random_sample looks like.\n\n# Inpsect our random sample (in original format)\nrandom_sample\n\n{'image': &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,\n 'image_id': 460,\n 'annotations': {'file_name': ['36496d24-8e3c-49ff-8e5f-73e74da1e383.jpeg',\n   '36496d24-8e3c-49ff-8e5f-73e74da1e383.jpeg',\n   '36496d24-8e3c-49ff-8e5f-73e74da1e383.jpeg'],\n  'image_id': [460, 460, 460],\n  'category_id': [5, 1, 0],\n  'bbox': [[473.8999938964844,\n    549.7999877929688,\n    137.8000030517578,\n    151.39999389648438],\n   [461.79998779296875,\n    652.7999877929688,\n    445.20001220703125,\n    600.7999877929688],\n   [1.5, 286.5, 955.5, 770.4000244140625]],\n  'iscrowd': [0, 0, 0],\n  'area': [20862.919921875, 267476.15625, 736117.1875]},\n 'label_source': 'manual_prodigy_label',\n 'image_source': 'manual_taken_photo'}\n\n\nOk wonderful, looks like we can extract the image_id, category_id bbox and area fields from our random_sample to get the required inputs to our format_image_annotations_as_coco function.\nLet‚Äôs try it out.\n\n# Extract image_id, categories, areas, and bboxes from the random sample\nrandom_sample_image_id = random_sample[\"image_id\"]\nrandom_sample_categories = random_sample[\"annotations\"][\"category_id\"]\nrandom_sample_areas = random_sample[\"annotations\"][\"area\"]\nrandom_sample_bboxes = random_sample[\"annotations\"][\"bbox\"]\n\n# Format the random sample annotations as COCO format\nrandom_sample_coco_annotations = format_image_annotations_as_coco(image_id=random_sample_image_id,\n                                                                  categories=random_sample_categories,\n                                                                  areas=random_sample_areas,\n                                                                  bboxes=random_sample_bboxes)\nrandom_sample_coco_annotations\n\n{'image_id': 460,\n 'annotations': [{'image_id': 460,\n   'category_id': 5,\n   'bbox': [473.8999938964844,\n    549.7999877929688,\n    137.8000030517578,\n    151.39999389648438],\n   'area': 20862.919921875,\n   'iscrowd': None},\n  {'image_id': 460,\n   'category_id': 1,\n   'bbox': [461.79998779296875,\n    652.7999877929688,\n    445.20001220703125,\n    600.7999877929688],\n   'area': 267476.15625,\n   'iscrowd': None},\n  {'image_id': 460,\n   'category_id': 0,\n   'bbox': [1.5, 286.5, 955.5, 770.4000244140625],\n   'area': 736117.1875,\n   'iscrowd': None}]}\n\n\nWoohoo!\nLooks like we just fixed our ValueError from before:\n\nValueError: Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts (batch of images) with the following keys: image_id and annotations, with the latter being a list of annotations in the COCO format.\n\nOur COCO formatted annotations have the image_id and annotations keys and our annotations are a list of annotations in COCO format.\nPerfect!\n\n\n6.7 TK - Preprocess a single image and set of COCO format annotations\nNow we‚Äôve preprocessed our annotations to be in COCO format, we can use them with image_processor.preprocess.\n\nSee docs for preprocess: https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.preprocess\n\n\n# Preprocess random sample image and assosciated annotations\nrandom_sample_preprocessed = image_processor.preprocess(images=random_sample[\"image\"],\n                                                        annotations=random_sample_coco_annotations,\n                                                        return_tensors=\"pt\") # can return as tensors or not \n\n\n# Check the keys of our preprocessed example\nrandom_sample_preprocessed.keys()\n\ndict_keys(['pixel_values', 'pixel_mask', 'labels'])\n\n\nTK - break down each of the above\n\npixel_values = preprocessed pixels (the preprocessed image)\npixel_mask = whether or not to mask the pixels (e.g.¬†0 = mask, 1 = no mask)\nlabels = preprocessed labels (the preprocessed annotations)\n\n\nrandom_sample_preprocessed[\"pixel_values\"]\n\ntensor([[[[-1.2445, -1.0904, -1.0048,  ..., -0.4911, -0.5082, -0.5253],\n          [-1.2788, -1.2959, -1.2617,  ..., -0.1828, -0.1657, -0.0801],\n          [-1.2959, -1.2788, -1.2617,  ...,  0.7077,  0.3823,  0.7419],\n          ...,\n          [ 1.0331,  0.6221,  0.8961,  ...,  1.3755,  1.4269,  1.2385],\n          [ 1.0502,  0.1768,  0.7419,  ...,  1.3755,  1.4440,  1.3584],\n          [ 0.3652,  0.1426,  1.2043,  ...,  1.3755,  1.3927,  1.3755]],\n\n         [[-1.3529, -1.1954, -1.1253,  ..., -0.3550, -0.3550, -0.3550],\n          [-1.3880, -1.4230, -1.3880,  ..., -0.0924, -0.0399,  0.0651],\n          [-1.3704, -1.3880, -1.3704,  ...,  0.7654,  0.4678,  0.8529],\n          ...,\n          [ 0.6954,  0.2577,  0.5203,  ...,  0.5903,  0.6429,  0.5028],\n          [ 0.7479, -0.1800,  0.3627,  ...,  0.5903,  0.6779,  0.6254],\n          [ 0.0476, -0.1975,  0.8529,  ...,  0.5728,  0.6429,  0.6429]],\n\n         [[-1.3861, -1.2467, -1.1944,  ..., -0.1487, -0.1661, -0.1661],\n          [-1.4036, -1.4384, -1.4210,  ...,  0.0605,  0.0953,  0.1999],\n          [-1.3513, -1.3687, -1.3687,  ...,  0.8099,  0.5311,  0.9319],\n          ...,\n          [ 0.4962,  0.0431,  0.2522,  ...,  0.3568,  0.4091,  0.2348],\n          [ 0.6008, -0.3230,  0.1476,  ...,  0.3568,  0.4265,  0.3393],\n          [-0.0441, -0.3230,  0.6531,  ...,  0.3393,  0.3916,  0.3568]]]])\n\n\n\nrandom_sample_preprocessed[\"pixel_values\"][0].shape\n\ntorch.Size([3, 640, 480])\n\n\n\n# What keys are in the labels?\nrandom_sample_preprocessed[\"labels\"][0].keys()\n\ndict_keys(['size', 'image_id', 'class_labels', 'boxes', 'area', 'iscrowd', 'orig_size'])\n\n\nTK - break it down what‚Äôs in the labels\n\nsize = image size in format [height, width]\nimage_id = ID of image passed in\nclass_labels = list of labels assosciated with image e.g.¬†tensor([5, 1, 0, 0, 4]) -&gt; {0: 'bin', 1: 'hand', 2: 'not_bin', 3: 'not_hand', 4: 'not_trash', 5: 'trash'}\nboxes = list of boxes with coordinates for where the box is on the image in format CXCYWH (normalized)\n\n\nrandom_sample_preprocessed[\"labels\"][0]\n\n{'size': tensor([640, 480]), 'image_id': tensor([460]), 'class_labels': tensor([], dtype=torch.int64), 'boxes': tensor([], size=(0, 4)), 'area': tensor([]), 'iscrowd': tensor([], dtype=torch.int64), 'orig_size': tensor([1280,  960])}\n\n\n\nrandom_sample_preprocessed[\"pixel_mask\"][0].shape\n\ntorch.Size([640, 480])\n\n\n\nid2label\n\n{0: 'bin',\n 1: 'hand',\n 2: 'not_bin',\n 3: 'not_hand',\n 4: 'not_trash',\n 5: 'trash',\n 6: 'trash_arm'}\n\n\n\n\n6.8 TK - Creating a function to build our model\nWe may want to make multiple instances of our model‚Ä¶ so let‚Äôs functionize the instantiation of a new model\n\n# Setup the model\ndef create_model():\n    \"\"\"Docstring here\"\"\"\n    model = AutoModelForObjectDetection.from_pretrained(\n        pretrained_model_name_or_path=MODEL_NAME,\n        label2id=label2id,\n        id2label=id2label,\n        ignore_mismatched_sizes=True,\n        backbone=\"resnet50\",\n    )\n    return model\n\nmodel = create_model()\nmodel\n\nSome weights of ConditionalDetrForObjectDetection were not initialized from the model checkpoint at microsoft/conditional-detr-resnet-50 and are newly initialized because the shapes did not match:\n- class_labels_classifier.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([7]) in the model instantiated\n- class_labels_classifier.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nConditionalDetrForObjectDetection(\n  (model): ConditionalDetrModel(\n    (backbone): ConditionalDetrConvModel(\n      (conv_encoder): ConditionalDetrConvEncoder(\n        (model): FeatureListNet(\n          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n          (bn1): ConditionalDetrFrozenBatchNorm2d()\n          (act1): ReLU(inplace=True)\n          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n          (layer1): Sequential(\n            (0): Bottleneck(\n              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n              (downsample): Sequential(\n                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (1): ConditionalDetrFrozenBatchNorm2d()\n              )\n            )\n            (1): Bottleneck(\n              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (2): Bottleneck(\n              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n          )\n          (layer2): Sequential(\n            (0): Bottleneck(\n              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n              (downsample): Sequential(\n                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n                (1): ConditionalDetrFrozenBatchNorm2d()\n              )\n            )\n            (1): Bottleneck(\n              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (2): Bottleneck(\n              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (3): Bottleneck(\n              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n          )\n          (layer3): Sequential(\n            (0): Bottleneck(\n              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n              (downsample): Sequential(\n                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n                (1): ConditionalDetrFrozenBatchNorm2d()\n              )\n            )\n            (1): Bottleneck(\n              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (2): Bottleneck(\n              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (3): Bottleneck(\n              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (4): Bottleneck(\n              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (5): Bottleneck(\n              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n          )\n          (layer4): Sequential(\n            (0): Bottleneck(\n              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n              (downsample): Sequential(\n                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n                (1): ConditionalDetrFrozenBatchNorm2d()\n              )\n            )\n            (1): Bottleneck(\n              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n            (2): Bottleneck(\n              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn1): ConditionalDetrFrozenBatchNorm2d()\n              (act1): ReLU(inplace=True)\n              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn2): ConditionalDetrFrozenBatchNorm2d()\n              (drop_block): Identity()\n              (act2): ReLU(inplace=True)\n              (aa): Identity()\n              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (bn3): ConditionalDetrFrozenBatchNorm2d()\n              (act3): ReLU(inplace=True)\n            )\n          )\n        )\n      )\n      (position_embedding): ConditionalDetrSinePositionEmbedding()\n    )\n    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n    (query_position_embeddings): Embedding(300, 256)\n    (encoder): ConditionalDetrEncoder(\n      (layers): ModuleList(\n        (0-5): 6 x ConditionalDetrEncoderLayer(\n          (self_attn): DetrAttention(\n            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): ReLU()\n          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (decoder): ConditionalDetrDecoder(\n      (layers): ModuleList(\n        (0): ConditionalDetrDecoderLayer(\n          (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)\n          (self_attn): ConditionalDetrAttention(\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)\n          (encoder_attn): ConditionalDetrAttention(\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        )\n        (1-5): 5 x ConditionalDetrDecoderLayer(\n          (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)\n          (self_attn): ConditionalDetrAttention(\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_qpos_proj): None\n          (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)\n          (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)\n          (encoder_attn): ConditionalDetrAttention(\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (query_scale): MLP(\n        (layers): ModuleList(\n          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n        )\n      )\n      (ref_point_head): MLP(\n        (layers): ModuleList(\n          (0): Linear(in_features=256, out_features=256, bias=True)\n          (1): Linear(in_features=256, out_features=2, bias=True)\n        )\n      )\n    )\n  )\n  (class_labels_classifier): Linear(in_features=256, out_features=7, bias=True)\n  (bbox_predictor): ConditionalDetrMLPPredictionHead(\n    (layers): ModuleList(\n      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n      (2): Linear(in_features=256, out_features=4, bias=True)\n    )\n  )\n)\n\n\nTK - callout: many models on Hugging Face follow this pattern of ‚Äúprocessor‚Äù -&gt; ‚Äúmodel‚Äù\n\nNote: TK - This may output some information about the model not being prepared for a custom dataset due to it originally being prepared for a certain number of classes (e.g.¬†the model can only recognize what it was trained on). We‚Äôve initialized it with an output head to have 4\n\n\nmodel.forward?\n\nSignature:\nmodel.forward(\n    pixel_values: torch.FloatTensor,\n    pixel_mask: Optional[torch.LongTensor] = None,\n    decoder_attention_mask: Optional[torch.LongTensor] = None,\n    encoder_outputs: Optional[torch.FloatTensor] = None,\n    inputs_embeds: Optional[torch.FloatTensor] = None,\n    decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n    labels: Optional[List[dict]] = None,\n    output_attentions: Optional[bool] = None,\n    output_hidden_states: Optional[bool] = None,\n    return_dict: Optional[bool] = None,\n) -&gt; Union[Tuple[torch.FloatTensor], transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrObjectDetectionOutput]\nDocstring:\nThe [`ConditionalDetrForObjectDetection`] forward method, overrides the `__call__` special method.\n\n&lt;Tip&gt;\n\nAlthough the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\ninstance afterwards instead of this since the former takes care of running the pre and post processing steps while\nthe latter silently ignores them.\n\n&lt;/Tip&gt;\n\nArgs:\n    pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n        Pixel values. Padding will be ignored by default should you provide it.\n\n        Pixel values can be obtained using [`AutoImageProcessor`]. See [`ConditionalDetrImageProcessor.__call__`]\n        for details.\n\n    pixel_mask (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):\n        Mask to avoid performing attention on padding pixel values. Mask values selected in `[0, 1]`:\n\n        - 1 for pixels that are real (i.e. **not masked**),\n        - 0 for pixels that are padding (i.e. **masked**).\n\n        [What are attention masks?](../glossary#attention-mask)\n\n    decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n        Not used by default. Can be used to mask object queries.\n    encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n        Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n        `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n        hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n    inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n        Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n        can choose to directly pass a flattened representation of an image.\n    decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n        Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n        embedded representation.\n    output_attentions (`bool`, *optional*):\n        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n        tensors for more detail.\n    output_hidden_states (`bool`, *optional*):\n        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n        more detail.\n    return_dict (`bool`, *optional*):\n        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\n    labels (`List[Dict]` of len `(batch_size,)`, *optional*):\n        Labels for computing the bipartite matching loss. List of dicts, each dictionary containing at least the\n        following 2 keys: 'class_labels' and 'boxes' (the class labels and bounding boxes of an image in the batch\n        respectively). The class labels themselves should be a `torch.LongTensor` of len `(number of bounding boxes\n        in the image,)` and the boxes a `torch.FloatTensor` of shape `(number of bounding boxes in the image, 4)`.\n\n\n    Returns:\n        [`transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrObjectDetectionOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrObjectDetectionOutput`] or a tuple of\n        `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n        elements depending on the configuration ([`ConditionalDetrConfig`]) and inputs.\n\n        - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)) -- Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n          bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n          scale-invariant IoU loss.\n        - **loss_dict** (`Dict`, *optional*) -- A dictionary containing the individual losses. Useful for logging.\n        - **logits** (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`) -- Classification logits (including no-object) for all queries.\n        - **pred_boxes** (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`) -- Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n          values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n          possible padding). You can use [`~ConditionalDetrImageProcessor.post_process_object_detection`] to retrieve the\n          unnormalized bounding boxes.\n        - **auxiliary_outputs** (`list[Dict]`, *optional*) -- Optional, only returned when auxilary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n          and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n          `pred_boxes`) for each decoder layer.\n        - **last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) -- Sequence of hidden-states at the output of the last layer of the decoder of the model.\n        - **decoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n          shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the decoder at the output of each\n          layer plus the initial embedding outputs.\n        - **decoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n          sequence_length)`. Attentions weights of the decoder, after the attention softmax, used to compute the\n          weighted average in the self-attention heads.\n        - **cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n          sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n          used to compute the weighted average in the cross-attention heads.\n        - **encoder_last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) -- Sequence of hidden-states at the output of the last layer of the encoder of the model.\n        - **encoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n          shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n          layer plus the initial embedding outputs.\n        - **encoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n          sequence_length)`. Attentions weights of the encoder, after the attention softmax, used to compute the\n          weighted average in the self-attention heads.\n  \n\n    Examples:\n\n    ```python\n    &gt;&gt;&gt; from transformers import AutoImageProcessor, AutoModelForObjectDetection\n    &gt;&gt;&gt; from PIL import Image\n    &gt;&gt;&gt; import requests\n\n    &gt;&gt;&gt; url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n    &gt;&gt;&gt; image = Image.open(requests.get(url, stream=True).raw)\n\n    &gt;&gt;&gt; image_processor = AutoImageProcessor.from_pretrained(\"microsoft/conditional-detr-resnet-50\")\n    &gt;&gt;&gt; model = AutoModelForObjectDetection.from_pretrained(\"microsoft/conditional-detr-resnet-50\")\n\n    &gt;&gt;&gt; inputs = image_processor(images=image, return_tensors=\"pt\")\n\n    &gt;&gt;&gt; outputs = model(**inputs)\n\n    &gt;&gt;&gt; # convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n    &gt;&gt;&gt; target_sizes = torch.tensor([image.size[::-1]])\n    &gt;&gt;&gt; results = image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[\n    ...     0\n    ... ]\n    &gt;&gt;&gt; for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n    ...     box = [round(i, 2) for i in box.tolist()]\n    ...     print(\n    ...         f\"Detected {model.config.id2label[label.item()]} with confidence \"\n    ...         f\"{round(score.item(), 3)} at location {box}\"\n    ...     )\n    Detected remote with confidence 0.833 at location [38.31, 72.1, 177.63, 118.45]\n    Detected cat with confidence 0.831 at location [9.2, 51.38, 321.13, 469.0]\n    Detected cat with confidence 0.804 at location [340.3, 16.85, 642.93, 370.95]\n    Detected remote with confidence 0.683 at location [334.48, 73.49, 366.37, 190.01]\n    Detected couch with confidence 0.535 at location [0.52, 1.19, 640.35, 475.1]\n    ```\nFile:      ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/conditional_detr/modeling_conditional_detr.py\nType:      method\n\n\n\nrandom_sample_preprocessed[\"pixel_values\"][0].shape # [color_channels, height, width]\n\ntorch.Size([3, 640, 480])\n\n\n\n# Do a single forward pass with the model\nrandom_sample_outputs = model(pixel_values=random_sample_preprocessed[\"pixel_values\"][0].unsqueeze(0), # model expects input [batch_size, color_channels, height, width]\n                              pixel_mask=None)\nrandom_sample_outputs\n\nConditionalDetrObjectDetectionOutput(loss=None, loss_dict=None, logits=tensor([[[ 0.0454, -0.0711, -0.4182,  ...,  0.2894,  0.0483,  0.0123],\n         [-0.1012, -0.1597, -0.1998,  ..., -0.0486, -0.1782, -0.2652],\n         [ 0.1434,  0.0662, -0.1789,  ...,  0.0542, -0.0454, -0.0935],\n         ...,\n         [-0.3237, -0.4062, -0.1989,  ...,  0.2875, -0.0910,  0.2941],\n         [ 0.1114, -0.0177, -0.3141,  ..., -0.0593, -0.1495, -0.1393],\n         [-0.1669, -0.1889,  0.1891,  ...,  0.1096, -0.2838, -0.0589]]],\n       grad_fn=&lt;ViewBackward0&gt;), pred_boxes=tensor([[[0.8267, 0.6865, 0.3329, 0.6065],\n         [0.6527, 0.1801, 0.0381, 0.0135],\n         [0.8987, 0.5712, 0.2006, 0.2254],\n         ...,\n         [0.3474, 0.3090, 0.6915, 0.1174],\n         [0.8373, 0.5285, 0.3022, 0.1941],\n         [0.0810, 0.2927, 0.1605, 0.0432]]], grad_fn=&lt;SigmoidBackward0&gt;), auxiliary_outputs=None, last_hidden_state=tensor([[[ 0.2234,  0.0444,  0.9698,  ..., -1.0443, -0.1137,  0.3582],\n         [ 0.2838, -0.6804,  0.3960,  ...,  0.7212,  0.3551,  0.3658],\n         [ 0.5051, -0.0147,  0.5885,  ..., -1.2090, -0.0941, -0.0717],\n         ...,\n         [ 0.4280, -1.5612,  0.3054,  ..., -0.8336,  0.0790, -0.3486],\n         [ 0.2858, -0.0132,  0.5693,  ..., -1.1525, -0.1821, -0.1940],\n         [ 0.2017,  0.1479, -0.3311,  ..., -1.1814, -0.0651, -0.0979]]],\n       grad_fn=&lt;NativeLayerNormBackward0&gt;), decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[-0.3918,  0.4741, -0.3829,  ..., -0.5659,  0.4583,  0.3095],\n         [ 0.1083,  0.5762, -0.0826,  ...,  0.2379,  0.1619,  0.3629],\n         [ 0.1359,  0.6453, -0.1079,  ..., -0.1028,  0.1878,  0.3184],\n         ...,\n         [ 0.1694,  0.8391, -0.1381,  ...,  0.1942,  0.0713,  0.2323],\n         [ 0.1709,  0.6931, -0.0919,  ...,  0.2428,  0.0508,  0.1932],\n         [-0.1842,  0.4742, -0.1434,  ..., -0.1434,  0.2518,  0.2516]]],\n       grad_fn=&lt;NativeLayerNormBackward0&gt;), encoder_hidden_states=None, encoder_attentions=None)\n\n\n\n# Check the keys of the output\nrandom_sample_outputs.keys()\n\nodict_keys(['logits', 'pred_boxes', 'last_hidden_state', 'encoder_last_hidden_state'])\n\n\n\n# We get 300 total boxes with shape the same as our number of labels\nrandom_sample_outputs.logits.shape\n\ntorch.Size([1, 300, 7])\n\n\n\nrandom_sample_outputs.pred_boxes.shape\n\ntorch.Size([1, 300, 4])\n\n\nTK - note: see forward() method for output format of boxes -&gt; https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection.forward\nFrom the docs:\n\nReturns ‚Ä¶ pred_boxes (torch.FloatTensor of shape (batch_size, num_queries, 4)) ‚Äî Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding possible padding). You can use post_process_object_detection() to retrieve the unnormalized bounding boxes.\n\n\n# Example pred box output\n# Box output comes in the form CXCYWH normalized (e.g. [center_X, center_Y, width, height]) to be between 0 and 1, this is in the docs\nrandom_sample_outputs.pred_boxes[:, 0]\n\ntensor([[0.8267, 0.6865, 0.3329, 0.6065]], grad_fn=&lt;SelectBackward0&gt;)\n\n\n\n# Model outputs one logit per category value (e.g. 6 categories = 6 logits)\nlen(categories.names)\n\n7\n\n\n\n# For example, one value for each of the following:\nlabel2id\n\n{'bin': 0,\n 'hand': 1,\n 'not_bin': 2,\n 'not_hand': 3,\n 'not_trash': 4,\n 'trash': 5,\n 'trash_arm': 6}\n\n\n\nrandom_sample\n\n{'image': &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,\n 'image_id': 384,\n 'annotations': {'file_name': ['3e85a851-513d-40b8-8b16-240b365132d8.jpeg',\n   '3e85a851-513d-40b8-8b16-240b365132d8.jpeg',\n   '3e85a851-513d-40b8-8b16-240b365132d8.jpeg',\n   '3e85a851-513d-40b8-8b16-240b365132d8.jpeg'],\n  'image_id': [384, 384, 384, 384],\n  'category_id': [5, 1, 0, 0],\n  'bbox': [[452.70001220703125,\n    485.3999938964844,\n    265.29998779296875,\n    174.1999969482422],\n   [625.5, 459.5, 180.1999969482422, 238.10000610351562],\n   [221.3000030517578,\n    371.8999938964844,\n    447.8999938964844,\n    496.3999938964844],\n   [7.699999809265137, 328.0, 301.3999938964844, 440.5]],\n  'iscrowd': [0, 0, 0, 0],\n  'area': [46215.26171875, 42905.62109375, 222337.5625, 132766.703125]},\n 'label_source': 'manual_prodigy_label',\n 'image_source': 'manual_taken_photo'}\n\n\n\n\n6.9 Post process a single output\nAlways a good step to get your model working end-to-end on a single sample and then upgrading it.\nBox formats:\n\nStarting data (the input data) -&gt; [x_top_left, y_top_left, width, height] -&gt; XYWH (absolute)\nOut of image_processor.preprocess() -&gt; [center_x, center_y, width, height] -&gt; CXCYWH (normalized) -&gt; into model\n\nSee docs: https://huggingface.co/docs/transformers.js/en/custom_usage\n\nOut of model -&gt; [center_x, center_y, width, height] -&gt; CXCYWH (normalized)\n\nSee docs for forward() and output pred_boxes: https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection.forward\n\nOut of image_processor.post_process_object_detection() -&gt; [x_top_left, y_top_left, x_bottom_right, y_bottom_right] -&gt; XYXY\n\nThis is PASCL VOC format - (xmin, ymin, xmax, ymax)\nSee docs: https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.post_process_object_detection\n\n\n\n# Check the keys of the labels for the image\nrandom_sample_preprocessed[\"labels\"][0].keys()\n\ndict_keys(['size', 'image_id', 'class_labels', 'boxes', 'area', 'iscrowd', 'orig_size'])\n\n\n\nprint(f\"[INFO] Image original size: {random_sample_preprocessed.labels[0].orig_size} (height, width)\")\nprint(f\"[INFO] Image size after preprocessing: {random_sample_preprocessed.labels[0].size} (height, width)\")\n\n[INFO] Image original size: tensor([1280,  960]) (height, width)\n[INFO] Image size after preprocessing: tensor([640, 480]) (height, width)\n\n\n\n# Output logits will be post-processed to turn into prediction probabilities as well as boxes\n\n# Get pred probs from logits, this will be used for our threshold parameter in post_process_object_detection \ntorch.softmax(random_sample_outputs.logits, dim=-1)\n\ntensor([[[0.1471, 0.1309, 0.0925,  ..., 0.1878, 0.1475, 0.1423],\n         [0.1330, 0.1255, 0.1205,  ..., 0.1402, 0.1232, 0.1129],\n         [0.1611, 0.1492, 0.1167,  ..., 0.1474, 0.1334, 0.1272],\n         ...,\n         [0.0988, 0.0910, 0.1119,  ..., 0.1821, 0.1247, 0.1833],\n         [0.1683, 0.1479, 0.1100,  ..., 0.1419, 0.1297, 0.1310],\n         [0.1238, 0.1211, 0.1767,  ..., 0.1632, 0.1101, 0.1379]]],\n       grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\n\nrandom_sample_outputs_post_processed = image_processor.post_process_object_detection(\n    outputs=random_sample_outputs,\n    threshold=0.3, # prediction probability threshold for boxes (note: boxes from an untrained model will likely be bad)\n    target_sizes=[random_sample_preprocessed[\"labels\"][0][\"orig_size\"]] # original input image size (or whichever target size you'd like), required to be same number of input items in a list\n)\n\nrandom_sample_outputs_post_processed\n\n[{'scores': tensor([0.6839, 0.6737, 0.6616, 0.6614, 0.6574, 0.6541, 0.6478, 0.6476, 0.6475,\n          0.6475, 0.6472, 0.6472, 0.6448, 0.6444, 0.6436, 0.6434, 0.6426, 0.6419,\n          0.6416, 0.6408, 0.6404, 0.6383, 0.6382, 0.6374, 0.6372, 0.6359, 0.6354,\n          0.6352, 0.6346, 0.6338, 0.6310, 0.6308, 0.6302, 0.6280, 0.6277, 0.6273,\n          0.6272, 0.6272, 0.6271, 0.6265, 0.6265, 0.6265, 0.6259, 0.6255, 0.6248,\n          0.6243, 0.6242, 0.6241, 0.6237, 0.6229, 0.6223, 0.6221, 0.6215, 0.6213,\n          0.6207, 0.6207, 0.6203, 0.6199, 0.6196, 0.6195, 0.6185, 0.6184, 0.6183,\n          0.6177, 0.6163, 0.6160, 0.6150, 0.6144, 0.6144, 0.6139, 0.6139, 0.6137,\n          0.6135, 0.6129, 0.6125, 0.6124, 0.6108, 0.6106, 0.6104, 0.6101, 0.6100,\n          0.6099, 0.6097, 0.6092, 0.6089, 0.6089, 0.6085, 0.6085, 0.6079, 0.6076,\n          0.6070, 0.6070, 0.6068, 0.6063, 0.6057, 0.6057, 0.6056, 0.6055, 0.6053,\n          0.6051], grad_fn=&lt;IndexBackward0&gt;),\n  'labels': tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n          3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 4, 3, 3, 3, 3, 4,\n          3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 4, 4, 3, 6, 3, 4, 3, 3, 3, 3,\n          3, 3, 3, 3, 3, 3, 4, 3, 4, 3, 3, 4, 3, 3, 3, 3, 4, 3, 4, 4, 3, 3, 4, 4,\n          3, 3, 6, 3]),\n  'boxes': tensor([[ 1.5021e+02,  2.2275e+02,  9.2612e+02,  3.1100e+02],\n          [ 3.0236e+02,  2.3473e+02,  8.8168e+02,  3.0422e+02],\n          [ 5.0880e+02,  2.1115e+02,  5.5853e+02,  2.3331e+02],\n          [ 4.7810e+02,  1.9805e+02,  5.2191e+02,  2.2018e+02],\n          [ 4.3730e+02,  2.4722e+02,  9.2742e+02,  3.1818e+02],\n          [ 9.0321e+02,  3.0775e+02,  9.6117e+02,  3.3864e+02],\n          [ 7.1403e+02,  2.3229e+02,  7.5484e+02,  2.4963e+02],\n          [ 6.4448e+02,  2.3185e+02,  7.0163e+02,  2.4743e+02],\n          [ 9.7196e+01,  2.4972e+02,  8.0122e+02,  3.4492e+02],\n          [ 6.9588e+02,  2.2955e+02,  7.2308e+02,  2.4626e+02],\n          [ 6.8868e+02,  2.3618e+02,  7.3616e+02,  2.5229e+02],\n          [ 6.1711e+02,  2.2698e+02,  6.7091e+02,  2.4342e+02],\n          [ 1.1697e+02,  2.0081e+02,  5.3524e+02,  2.5107e+02],\n          [ 4.8558e+02,  2.0865e+02,  5.4637e+02,  2.3481e+02],\n          [ 7.4454e+02,  2.2994e+02,  7.8603e+02,  2.5308e+02],\n          [ 6.8199e+02,  2.3242e+02,  7.2622e+02,  2.4876e+02],\n          [ 5.3925e+02,  2.2372e+02,  5.9300e+02,  2.4156e+02],\n          [ 4.4184e+02,  2.0462e+02,  4.7448e+02,  2.2387e+02],\n          [ 6.6260e+02,  2.3085e+02,  7.0588e+02,  2.4724e+02],\n          [ 7.2336e+02,  2.3609e+02,  7.7372e+02,  2.5295e+02],\n          [ 5.9377e+02,  2.3010e+02,  6.5768e+02,  2.4654e+02],\n          [ 6.5659e+02,  2.3656e+02,  7.1577e+02,  2.5289e+02],\n          [ 2.1040e+02,  1.9925e+02,  3.2352e+02,  2.3685e+02],\n          [ 1.8045e+02,  1.5578e+02,  3.1142e+02,  1.9401e+02],\n          [ 4.2432e+02,  2.0136e+02,  4.5325e+02,  2.1855e+02],\n          [ 7.8032e+02,  2.2787e+02,  8.3353e+02,  2.5188e+02],\n          [ 6.4112e+02,  2.1902e+02,  6.7090e+02,  2.3924e+02],\n          [ 3.7899e+02,  2.4951e+02,  7.8740e+02,  3.0072e+02],\n          [ 1.6532e+02,  2.3143e+02,  3.2209e+02,  2.6452e+02],\n          [ 1.0642e+02,  2.3275e+02,  2.2752e+02,  2.5950e+02],\n          [-2.7386e+00,  2.8250e+02,  9.5569e+02,  8.3506e+02],\n          [ 5.6764e+02,  2.2579e+02,  6.0133e+02,  2.4452e+02],\n          [ 7.2771e+02,  2.3997e+02,  7.9102e+02,  2.5931e+02],\n          [ 1.3404e+02,  4.4818e+02,  9.4260e+02,  8.4707e+02],\n          [ 4.5860e+02,  4.7699e+02,  5.8809e+02,  5.3134e+02],\n          [ 1.6226e+02,  2.1322e+02,  3.1575e+02,  2.5415e+02],\n          [ 5.8245e+02,  2.3491e+02,  7.3097e+02,  2.5595e+02],\n          [ 9.5602e+02,  2.3172e+02,  9.5996e+02,  2.8035e+02],\n          [ 7.6245e+02,  2.3498e+02,  8.0948e+02,  2.5390e+02],\n          [ 8.5605e+02,  3.1974e+02,  9.6036e+02,  3.4395e+02],\n          [ 1.4028e+01,  2.9742e+02,  6.6831e+02,  7.8667e+02],\n          [ 2.1273e+00,  8.9889e+02,  9.6114e+02,  1.2628e+03],\n          [ 3.9504e+02,  2.1397e+02,  4.3043e+02,  2.2851e+02],\n          [ 3.4446e-01,  1.8749e+02,  7.8433e+01,  2.2499e+02],\n          [ 6.7294e+02,  2.5800e+02,  7.6941e+02,  2.8344e+02],\n          [ 6.0833e+02,  2.2191e+02,  6.4493e+02,  2.3915e+02],\n          [-4.1992e+00,  3.3792e+02,  9.5432e+02,  1.1725e+03],\n          [ 3.5863e+02,  2.0646e+02,  3.8990e+02,  2.3022e+02],\n          [ 4.6378e+02,  2.2039e+02,  5.2133e+02,  2.3814e+02],\n          [-5.5702e+00,  2.8453e+02,  8.7009e+02,  7.9807e+02],\n          [ 7.9385e+02,  2.3517e+02,  8.4780e+02,  2.5737e+02],\n          [ 8.8628e+02,  2.7585e+02,  9.6165e+02,  3.1819e+02],\n          [-1.4085e+01,  7.4535e+02,  9.4306e+02,  1.2644e+03],\n          [ 7.6982e-01,  4.5878e+02,  9.5951e+02,  8.9642e+02],\n          [ 3.8868e+02,  1.9604e+02,  4.2339e+02,  2.1367e+02],\n          [ 1.0205e+02,  2.3049e+02,  1.9162e+02,  2.5180e+02],\n          [ 1.5999e+02,  1.4399e+02,  2.6803e+02,  1.9518e+02],\n          [ 8.3596e+02,  2.8685e+02,  9.6574e+02,  3.4110e+02],\n          [ 4.0079e+02,  2.0236e+02,  4.3893e+02,  2.2204e+02],\n          [ 2.2829e-02,  1.7629e+02,  7.5778e+00,  2.1321e+02],\n          [ 4.0679e+02,  2.0669e+02,  4.4338e+02,  2.2396e+02],\n          [-1.5052e+00,  3.2000e+02,  6.8636e+02,  8.1026e+02],\n          [-1.0880e+01,  4.2899e+02,  9.4746e+02,  8.7154e+02],\n          [ 8.5131e+00,  5.3675e+02,  6.7816e+02,  8.2705e+02],\n          [ 5.8866e+02,  2.2367e+02,  6.3574e+02,  2.4067e+02],\n          [ 1.4028e+01,  2.9742e+02,  6.6831e+02,  7.8667e+02],\n          [ 7.6864e+02,  3.0461e+02,  8.5804e+02,  3.3697e+02],\n          [ 3.2264e+02,  7.8797e+02,  3.9313e+02,  8.3845e+02],\n          [ 1.2273e+02,  3.3624e+02,  2.9674e+02,  3.8669e+02],\n          [ 4.9169e+02,  6.0451e+02,  5.7784e+02,  6.6474e+02],\n          [ 7.3638e+01,  1.6488e+02,  1.4301e+02,  2.1829e+02],\n          [ 8.8692e+02,  2.2889e+02,  9.5606e+02,  2.5846e+02],\n          [ 2.5381e+02,  1.2226e+03,  9.6300e+02,  1.2849e+03],\n          [-1.0880e+01,  4.2899e+02,  9.4746e+02,  8.7154e+02],\n          [-2.6988e+00,  2.1334e+02,  5.3616e+01,  2.4198e+02],\n          [ 8.3390e+02,  2.2830e+02,  8.8685e+02,  2.5244e+02],\n          [ 9.3645e+00,  2.3032e+02,  1.1699e+02,  2.6169e+02],\n          [ 5.2715e+02,  2.8053e+02,  5.8582e+02,  3.0946e+02],\n          [ 4.9539e+02,  2.8255e+02,  5.5250e+02,  3.0303e+02],\n          [ 8.7392e+02,  3.0168e+02,  9.6059e+02,  3.3615e+02],\n          [ 2.7833e+02,  4.6456e+02,  9.4896e+02,  9.1551e+02],\n          [ 4.8947e+02,  4.1488e+02,  6.1218e+02,  4.9252e+02],\n          [ 7.5197e+02,  2.5210e+02,  8.2422e+02,  2.7998e+02],\n          [ 2.5285e+02,  4.3588e+02,  9.3822e+02,  8.7275e+02],\n          [ 8.7054e+02,  2.2011e+02,  9.3370e+02,  2.4845e+02],\n          [ 9.0750e+02,  3.1742e+02,  9.6316e+02,  3.5016e+02],\n          [-4.3446e-01,  2.1810e+02,  2.3509e+01,  2.4937e+02],\n          [ 1.1742e+02,  2.1277e+02,  3.1611e+02,  2.5940e+02],\n          [ 9.6207e+01,  3.5631e+02,  9.5135e+02,  9.5458e+02],\n          [ 2.7833e+02,  4.6456e+02,  9.4896e+02,  9.1551e+02],\n          [ 4.0679e+02,  2.0669e+02,  4.4338e+02,  2.2396e+02],\n          [-1.1057e+01,  2.5194e+02,  3.7717e+02,  7.1443e+02],\n          [ 3.7007e+02,  1.9513e+02,  4.0063e+02,  2.1357e+02],\n          [ 4.0936e+02,  5.2738e+02,  6.3709e+02,  6.3685e+02],\n          [ 6.5659e+02,  2.3656e+02,  7.1577e+02,  2.5289e+02],\n          [ 7.2771e+02,  2.3997e+02,  7.9102e+02,  2.5931e+02],\n          [ 2.6593e+02,  1.8549e+02,  3.8638e+02,  2.2928e+02],\n          [ 4.9952e+02,  1.2543e+03,  8.9540e+02,  1.2808e+03],\n          [-1.1057e+01,  2.5194e+02,  3.7717e+02,  7.1443e+02],\n          [-4.4207e+00,  2.8687e+02,  9.5472e+02,  1.0455e+03]],\n         grad_fn=&lt;IndexBackward0&gt;)}]\n\n\nTK - let‚Äôs visualize, visualize, visualize!\n\n# Extract scores, labels and boxes\nrandom_sample_pred_scores = random_sample_outputs_post_processed[0][\"scores\"]\nrandom_sample_pred_labels = random_sample_outputs_post_processed[0][\"labels\"]\nrandom_sample_pred_boxes = random_sample_outputs_post_processed[0][\"boxes\"]\n\n# Create a list of labels to plot on the boxes \nrandom_sample_labels_to_plot = [f\"Pred: {id2label[label_pred.item()]} ({round(score_pred.item(), 4)})\" \n                  for label_pred, score_pred in zip(random_sample_pred_labels, random_sample_pred_scores)]\n\nprint(f\"[INFO] Labels with scores: {random_sample_labels_to_plot}\")\n\n# Plot the random sample image with randomly predicted boxes (these will be very poor since the model is not trained on our data yet)\nto_pil_image(\n    pic=draw_bounding_boxes(\n        image=pil_to_tensor(pic=random_sample[\"image\"]),\n        boxes=random_sample_pred_boxes,\n        labels=random_sample_labels_to_plot,\n        width=3\n    )\n)\n\n[INFO] Labels with scores: ['Pred: not_hand (0.6839)', 'Pred: not_hand (0.6737)', 'Pred: not_hand (0.6616)', 'Pred: not_hand (0.6614)', 'Pred: not_hand (0.6574)', 'Pred: not_hand (0.6541)', 'Pred: not_hand (0.6478)', 'Pred: not_hand (0.6476)', 'Pred: not_hand (0.6475)', 'Pred: not_hand (0.6475)', 'Pred: not_hand (0.6472)', 'Pred: not_hand (0.6472)', 'Pred: not_hand (0.6448)', 'Pred: not_hand (0.6444)', 'Pred: not_hand (0.6436)', 'Pred: not_hand (0.6434)', 'Pred: not_hand (0.6426)', 'Pred: not_hand (0.6419)', 'Pred: not_hand (0.6416)', 'Pred: not_hand (0.6408)', 'Pred: not_hand (0.6404)', 'Pred: not_hand (0.6383)', 'Pred: not_hand (0.6382)', 'Pred: not_hand (0.6374)', 'Pred: not_hand (0.6372)', 'Pred: not_hand (0.6359)', 'Pred: not_hand (0.6354)', 'Pred: not_hand (0.6352)', 'Pred: not_hand (0.6346)', 'Pred: not_hand (0.6338)', 'Pred: not_hand (0.631)', 'Pred: not_hand (0.6308)', 'Pred: not_hand (0.6302)', 'Pred: not_hand (0.628)', 'Pred: not_hand (0.6277)', 'Pred: not_hand (0.6273)', 'Pred: not_hand (0.6272)', 'Pred: not_hand (0.6272)', 'Pred: not_hand (0.6271)', 'Pred: not_hand (0.6265)', 'Pred: not_trash (0.6265)', 'Pred: not_hand (0.6265)', 'Pred: not_trash (0.6259)', 'Pred: not_hand (0.6255)', 'Pred: not_hand (0.6248)', 'Pred: not_hand (0.6243)', 'Pred: not_hand (0.6242)', 'Pred: not_trash (0.6241)', 'Pred: not_hand (0.6237)', 'Pred: not_trash (0.6229)', 'Pred: not_hand (0.6223)', 'Pred: not_hand (0.6221)', 'Pred: not_hand (0.6215)', 'Pred: not_hand (0.6213)', 'Pred: not_hand (0.6207)', 'Pred: not_hand (0.6207)', 'Pred: not_hand (0.6203)', 'Pred: not_hand (0.6199)', 'Pred: not_hand (0.6196)', 'Pred: not_hand (0.6195)', 'Pred: not_hand (0.6185)', 'Pred: trash_arm (0.6184)', 'Pred: not_trash (0.6183)', 'Pred: not_trash (0.6177)', 'Pred: not_hand (0.6163)', 'Pred: trash_arm (0.616)', 'Pred: not_hand (0.615)', 'Pred: not_trash (0.6144)', 'Pred: not_hand (0.6144)', 'Pred: not_hand (0.6139)', 'Pred: not_hand (0.6139)', 'Pred: not_hand (0.6137)', 'Pred: not_hand (0.6135)', 'Pred: not_hand (0.6129)', 'Pred: not_hand (0.6125)', 'Pred: not_hand (0.6124)', 'Pred: not_hand (0.6108)', 'Pred: not_hand (0.6106)', 'Pred: not_trash (0.6104)', 'Pred: not_hand (0.6101)', 'Pred: not_trash (0.61)', 'Pred: not_hand (0.6099)', 'Pred: not_hand (0.6097)', 'Pred: not_trash (0.6092)', 'Pred: not_hand (0.6089)', 'Pred: not_hand (0.6089)', 'Pred: not_hand (0.6085)', 'Pred: not_hand (0.6085)', 'Pred: not_trash (0.6079)', 'Pred: not_hand (0.6076)', 'Pred: not_trash (0.607)', 'Pred: not_trash (0.607)', 'Pred: not_hand (0.6068)', 'Pred: not_hand (0.6063)', 'Pred: not_trash (0.6057)', 'Pred: not_trash (0.6057)', 'Pred: not_hand (0.6056)', 'Pred: not_hand (0.6055)', 'Pred: trash_arm (0.6053)', 'Pred: not_hand (0.6051)']\n\n\n\n\n\n\n\n\n\nOur predictions are poor since our model hasn‚Äôt been specifically trained on our data.\nBut we can improve them by fine-tuning the model to our dataset.",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  },
  {
    "objectID": "notebooks/hugging_face_object_detection_tutorial.html#different-bounding-box-formats",
    "href": "notebooks/hugging_face_object_detection_tutorial.html#different-bounding-box-formats",
    "title": "[Work in Progress] Object Detection with Hugging Face Transformers Tutorial",
    "section": "5 Different bounding box formats",
    "text": "5 Different bounding box formats\nWhen drawing our bounding box, we discussed the terms XYXY and XYWH.\nWell, we didn‚Äôt really discuss these at all‚Ä¶\nBut that‚Äôs why we‚Äôre here.\nOne of the most confusing things in the world of object detection is the different formats bounding boxes come in.\nAre your boxes in XYXY, XYWH or CXCYWH?\nAre they in absolute format?\nOr normalized format?\nPerhaps a table will help us.\nThe following table contains a non-exhaustive list of some of the most common bounding box formats you‚Äôll come across in the wild.\n\n\n\nTable¬†1: Different bounding box formats\n\n\n\n\n\nBox format\nDescription\nAbsolute Example\nNormalized Example\nSource\n\n\n\n\nXYXY\nDescribes the top left corner coordinates (x1, y1) as well as the bottom right corner coordinates of a box.  Also referred to as:  [x1, y1, x2, y2]  or  [x_min, y_min, x_max, y_max]\n[8.9, 275.3, 867.5, 964.0]\n[0.009, 0.215, 0.904, 0.753]\nPASCAL VOC Dataset uses the absolute version of this format, torchvision.utils.draw_bounding_boxes defaults to the absolute version of this format.\n\n\nXYWH\nDescribes the top left corner coordinates (x1, y1) as well as the width (box_width) and height (box_height) of the target box. The bottom right corners (x2, y2) are found by adding the width and height to the top left corner coordinates (x1 + box_width, y1 + box_height).  Also referred to as:  [x1, y1, box_width, box_height]  or  [x_min, y_min, box_width, box_height]\n[8.9, 275.3, 858.6, 688.7]\n[0.009, 0.215, 0.894, 0.538]\nThe COCO (Common Objects in Context) dataset uses the absolute version of this format, see the section under ‚Äúbbox‚Äù.\n\n\nCXCYWH\nDescribes the center coordinates of the bounding box (center_x, center_y) as well as the width (box_width) and height (box_height) of the target box.  Also referred to as:  [center_x, center_y, box_width, box_height]\n[438.2, 619.65, 858.6, 688.7]\n[0.456, 0.484, 0.894, 0.538]\nNormalized version introduced in the YOLOv3 (You Only Look Once) paper and is used by many later forms of YOLO.\n\n\n\n\n\n\n\n5.1 Absolute or normalized format?\nIn absolute coordinate form, bounding box values are in the same format as the width and height dimensions (e.g.¬†our image is 960x1280 pixels).\nFor example in XYXY format: [\"bin\", 8.9, 275.3, 867.5, 964.0]\nAn (x1, y1) (or (x_min, y_min)) coordinate of (8.9, 275.3) means the top left corner is 8.9 pixels in on the x-axis, and 275.3 pixels down on the y-axis.\nIn normalized coordinate form, values are between [0, 1] and are proportions of the image width and height.\nFor example in XYXY format: [\"bin\", 0.009, 0.215, 0.904, 0.753]\nA normalized (x1, y1) (or (x_min, y_min)) coordinate of (0.009, 0.215) means the top left corner is 0.009 * image_width pixels in on the x-axis and 0.215 * image_height down on the y-axis.\nTo convert absolute coordinates to normalized, you can divide x-axis values by the image width and y-axis values by the image height.\n\\[\nx_{\\text{normalized}} = \\frac{x_{\\text{absolute}}}{\\text{image\\_width}} \\quad y_{\\text{normalized}} = \\frac{y_{\\text{absolute}}}{\\text{image\\_height}}\n\\]\n\n\n5.2 Which bounding box format should you use?\nThe bounding box format you use will depend on the framework, model and existing data you‚Äôre trying to use.\nFor example, the take the following frameworks:\n\nPyTorch - If you‚Äôre using PyTorch pre-trained models, for example, torchvision.models.detection.fasterrcnn_resnet50_fpn, you‚Äôll want absolute XYXY ([x1, y1, x2, y2]) format.\nHugging Face Transformers - If you‚Äôre using a Hugging Face Transformers model such as Conditional DETR, you‚Äôll want to take note that outputs from the model can be of one type (e.g.¬†CXCYWH) but they can be post-processed into another type (e.g.¬†absolute XYXY).\nUltralytics YOLO - If you‚Äôre using a YOLO-like model such as Ultralytics YOLO, you‚Äôll want normalized CXCYWH ([center_x, center_y, width, height]) format.\nGoogle Gemini - If you‚Äôre using Google Gemini to predict bounding boxes on your images, then you‚Äôll want to pay attention to the special [y_min, x_min, y_max, x_max] (YXYX) normalized coordinates.\n\nOr if you note that someone has said their model is pre-trained on the COCO dataset, chances are the data has been formatted in XYWH format (see Table¬†1).\n\n\n\n\n\n\nNote\n\n\n\nFor more on different bounding box formats and how to draw them, see A Guide to Bounding Box Formats and How to Draw Them.\n\n\n\n# TK - should I functionize the plotting of boxes and image so we can do input/output with tensors + data augmentations on that (E.g. original: image, augmented: image),\n# - is this needed?",
    "crumbs": [
      "Home",
      "Computer Vision",
      "(Work in progress) Build a custom object detection model and demo"
    ]
  }
]