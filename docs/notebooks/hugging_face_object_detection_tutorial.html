<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Learn how to create a custom object detection model with Hugging Face Transformers.">

<title>Learn Hugging Face 🤗 – Object Detection with Hugging Face Transformers Tutorial</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../images/learn-hf-favicon.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="Learn Hugging Face 🤗 - Object Detection with Hugging Face Transformers Tutorial">
<meta property="og:description" content="Learn how to create a custom object detection model with Hugging Face Transformers.">
<meta property="og:image" content="https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-trashify-object-detection/00-what-were-going-to-build.png">
<meta property="og:site_name" content="Learn Hugging Face 🤗">
<meta name="twitter:title" content="Learn Hugging Face 🤗 - Object Detection with Hugging Face Transformers Tutorial">
<meta name="twitter:description" content="Learn how to create a custom object detection model with Hugging Face Transformers.">
<meta name="twitter:image" content="https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-trashify-object-detection/00-what-were-going-to-build.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Learn Hugging Face 🤗</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../extras/setup.html"> 
<span class="menu-text">Setup</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../extras/glossary.html"> 
<span class="menu-text">Glossary</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
    <a href="https://github.com/mrdbourke/learn-huggingface" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notebooks/hugging_face_object_detection_tutorial.html">Computer Vision</a></li><li class="breadcrumb-item"><a href="../notebooks/hugging_face_object_detection_tutorial.html">Build a custom object detection model and demo</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Natural Language Processing (NLP)</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/hugging_face_text_classification_tutorial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Build a custom text classification model and demo</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Computer Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/hugging_face_object_detection_tutorial.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Build a custom object detection model and demo</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview"><span class="header-section-number">1</span> Overview</a>
  <ul class="collapse">
  <li><a href="#what-were-going-to-build" id="toc-what-were-going-to-build" class="nav-link" data-scroll-target="#what-were-going-to-build"><span class="header-section-number">1.1</span> What we’re going to build</a></li>
  <li><a href="#what-is-object-detection" id="toc-what-is-object-detection" class="nav-link" data-scroll-target="#what-is-object-detection"><span class="header-section-number">1.2</span> What is object detection?</a></li>
  <li><a href="#why-train-your-own-object-detection-models" id="toc-why-train-your-own-object-detection-models" class="nav-link" data-scroll-target="#why-train-your-own-object-detection-models"><span class="header-section-number">1.3</span> Why train your own object detection models?</a></li>
  <li><a href="#workflow-were-going-to-follow" id="toc-workflow-were-going-to-follow" class="nav-link" data-scroll-target="#workflow-were-going-to-follow"><span class="header-section-number">1.4</span> Workflow we’re going to follow</a></li>
  </ul></li>
  <li><a href="#importing-necessary-libraries" id="toc-importing-necessary-libraries" class="nav-link" data-scroll-target="#importing-necessary-libraries"><span class="header-section-number">2</span> Importing necessary libraries</a></li>
  <li><a href="#getting-a-dataset" id="toc-getting-a-dataset" class="nav-link" data-scroll-target="#getting-a-dataset"><span class="header-section-number">3</span> Getting a dataset</a>
  <ul class="collapse">
  <li><a href="#loading-the-dataset" id="toc-loading-the-dataset" class="nav-link" data-scroll-target="#loading-the-dataset"><span class="header-section-number">3.1</span> Loading the dataset</a></li>
  <li><a href="#viewing-a-single-sample-from-our-data" id="toc-viewing-a-single-sample-from-our-data" class="nav-link" data-scroll-target="#viewing-a-single-sample-from-our-data"><span class="header-section-number">3.2</span> Viewing a single sample from our data</a></li>
  <li><a href="#extracting-the-category-names-from-our-data" id="toc-extracting-the-category-names-from-our-data" class="nav-link" data-scroll-target="#extracting-the-category-names-from-our-data"><span class="header-section-number">3.3</span> Extracting the category names from our data</a></li>
  <li><a href="#creating-a-mapping-from-numbers-to-labels" id="toc-creating-a-mapping-from-numbers-to-labels" class="nav-link" data-scroll-target="#creating-a-mapping-from-numbers-to-labels"><span class="header-section-number">3.4</span> Creating a mapping from numbers to labels</a></li>
  <li><a href="#creating-a-colour-palette" id="toc-creating-a-colour-palette" class="nav-link" data-scroll-target="#creating-a-colour-palette"><span class="header-section-number">3.5</span> Creating a colour palette</a></li>
  </ul></li>
  <li><a href="#plotting-a-single-image-and-visualizing-the-boxes" id="toc-plotting-a-single-image-and-visualizing-the-boxes" class="nav-link" data-scroll-target="#plotting-a-single-image-and-visualizing-the-boxes"><span class="header-section-number">4</span> Plotting a single image and visualizing the boxes</a>
  <ul class="collapse">
  <li><a href="#creating-helper-functions-to-help-with-visualization" id="toc-creating-helper-functions-to-help-with-visualization" class="nav-link" data-scroll-target="#creating-helper-functions-to-help-with-visualization"><span class="header-section-number">4.1</span> Creating helper functions to help with visualization</a></li>
  <li><a href="#plotting-boxes-on-a-single-image-step-by-step" id="toc-plotting-boxes-on-a-single-image-step-by-step" class="nav-link" data-scroll-target="#plotting-boxes-on-a-single-image-step-by-step"><span class="header-section-number">4.2</span> Plotting boxes on a single image step by step</a></li>
  </ul></li>
  <li><a href="#different-bounding-box-formats" id="toc-different-bounding-box-formats" class="nav-link" data-scroll-target="#different-bounding-box-formats"><span class="header-section-number">5</span> Different bounding box formats</a>
  <ul class="collapse">
  <li><a href="#absolute-or-normalized-format" id="toc-absolute-or-normalized-format" class="nav-link" data-scroll-target="#absolute-or-normalized-format"><span class="header-section-number">5.1</span> Absolute or normalized format?</a></li>
  <li><a href="#which-bounding-box-format-should-you-use" id="toc-which-bounding-box-format-should-you-use" class="nav-link" data-scroll-target="#which-bounding-box-format-should-you-use"><span class="header-section-number">5.2</span> Which bounding box format should you use?</a></li>
  </ul></li>
  <li><a href="#getting-an-object-detection-model" id="toc-getting-an-object-detection-model" class="nav-link" data-scroll-target="#getting-an-object-detection-model"><span class="header-section-number">6</span> Getting an object detection model</a>
  <ul class="collapse">
  <li><a href="#places-to-get-object-detection-models" id="toc-places-to-get-object-detection-models" class="nav-link" data-scroll-target="#places-to-get-object-detection-models"><span class="header-section-number">6.1</span> Places to get object detection models</a></li>
  <li><a href="#downloading-our-model-from-hugging-face" id="toc-downloading-our-model-from-hugging-face" class="nav-link" data-scroll-target="#downloading-our-model-from-hugging-face"><span class="header-section-number">6.2</span> Downloading our model from Hugging Face</a></li>
  <li><a href="#inspecting-our-models-layers" id="toc-inspecting-our-models-layers" class="nav-link" data-scroll-target="#inspecting-our-models-layers"><span class="header-section-number">6.3</span> Inspecting our model’s layers</a></li>
  <li><a href="#counting-the-number-of-parameters-in-our-model" id="toc-counting-the-number-of-parameters-in-our-model" class="nav-link" data-scroll-target="#counting-the-number-of-parameters-in-our-model"><span class="header-section-number">6.4</span> Counting the number of parameters in our model</a></li>
  <li><a href="#creating-a-function-to-build-our-model" id="toc-creating-a-function-to-build-our-model" class="nav-link" data-scroll-target="#creating-a-function-to-build-our-model"><span class="header-section-number">6.5</span> Creating a function to build our model</a></li>
  <li><a href="#trying-to-pass-a-single-sample-through-our-model-part-1" id="toc-trying-to-pass-a-single-sample-through-our-model-part-1" class="nav-link" data-scroll-target="#trying-to-pass-a-single-sample-through-our-model-part-1"><span class="header-section-number">6.6</span> Trying to pass a single sample through our model (part 1)</a></li>
  </ul></li>
  <li><a href="#aside-processor-to-model-pattern" id="toc-aside-processor-to-model-pattern" class="nav-link" data-scroll-target="#aside-processor-to-model-pattern"><span class="header-section-number">7</span> Aside: Processor to Model Pattern</a></li>
  <li><a href="#loading-our-models-processor" id="toc-loading-our-models-processor" class="nav-link" data-scroll-target="#loading-our-models-processor"><span class="header-section-number">8</span> Loading our model’s processor</a>
  <ul class="collapse">
  <li><a href="#preprocessing-a-single-image" id="toc-preprocessing-a-single-image" class="nav-link" data-scroll-target="#preprocessing-a-single-image"><span class="header-section-number">8.1</span> Preprocessing a single image</a></li>
  <li><a href="#trying-to-pass-a-single-sample-through-our-model-part-2" id="toc-trying-to-pass-a-single-sample-through-our-model-part-2" class="nav-link" data-scroll-target="#trying-to-pass-a-single-sample-through-our-model-part-2"><span class="header-section-number">8.2</span> Trying to pass a single sample through our model (part 2)</a></li>
  </ul></li>
  <li><a href="#preprocessing-our-annotations" id="toc-preprocessing-our-annotations" class="nav-link" data-scroll-target="#preprocessing-our-annotations"><span class="header-section-number">9</span> Preprocessing our annotations</a>
  <ul class="collapse">
  <li><a href="#trying-to-preprocess-a-single-annotation" id="toc-trying-to-preprocess-a-single-annotation" class="nav-link" data-scroll-target="#trying-to-preprocess-a-single-annotation"><span class="header-section-number">9.1</span> Trying to preprocess a single annotation</a></li>
  <li><a href="#discussing-the-format-our-annotations-need-to-be-in" id="toc-discussing-the-format-our-annotations-need-to-be-in" class="nav-link" data-scroll-target="#discussing-the-format-our-annotations-need-to-be-in"><span class="header-section-number">9.2</span> Discussing the format our annotations need to be in</a></li>
  <li><a href="#creating-dataclasses-to-represent-the-coco-bounding-box-format" id="toc-creating-dataclasses-to-represent-the-coco-bounding-box-format" class="nav-link" data-scroll-target="#creating-dataclasses-to-represent-the-coco-bounding-box-format"><span class="header-section-number">9.3</span> Creating dataclasses to represent the COCO bounding box format</a></li>
  <li><a href="#creating-a-function-to-format-our-annotations-as-coco-format" id="toc-creating-a-function-to-format-our-annotations-as-coco-format" class="nav-link" data-scroll-target="#creating-a-function-to-format-our-annotations-as-coco-format"><span class="header-section-number">9.4</span> Creating a function to format our annotations as COCO format</a></li>
  <li><a href="#preprocess-a-single-image-and-set-of-coco-format-annotations" id="toc-preprocess-a-single-image-and-set-of-coco-format-annotations" class="nav-link" data-scroll-target="#preprocess-a-single-image-and-set-of-coco-format-annotations"><span class="header-section-number">9.5</span> Preprocess a single image and set of COCO format annotations</a></li>
  </ul></li>
  <li><a href="#postprocessing-a-single-output" id="toc-postprocessing-a-single-output" class="nav-link" data-scroll-target="#postprocessing-a-single-output"><span class="header-section-number">10</span> Postprocessing a single output</a></li>
  <li><a href="#going-end-to-end-on-a-single-sample" id="toc-going-end-to-end-on-a-single-sample" class="nav-link" data-scroll-target="#going-end-to-end-on-a-single-sample"><span class="header-section-number">11</span> Going end-to-end on a single sample</a></li>
  <li><a href="#reproducing-our-postprocessed-box-scores-by-hand" id="toc-reproducing-our-postprocessed-box-scores-by-hand" class="nav-link" data-scroll-target="#reproducing-our-postprocessed-box-scores-by-hand"><span class="header-section-number">11.1</span> Reproducing our postprocessed box scores by hand</a></li>
  <li><a href="#reproducing-our-postprocessed-box-labels-by-hand" id="toc-reproducing-our-postprocessed-box-labels-by-hand" class="nav-link" data-scroll-target="#reproducing-our-postprocessed-box-labels-by-hand"><span class="header-section-number">11.2</span> Reproducing our postprocessed box labels by hand</a></li>
  <li><a href="#reproducing-our-postprocessed-box-coordinates-by-hand" id="toc-reproducing-our-postprocessed-box-coordinates-by-hand" class="nav-link" data-scroll-target="#reproducing-our-postprocessed-box-coordinates-by-hand"><span class="header-section-number">11.3</span> Reproducing our postprocessed box coordinates by hand</a></li>
  <li><a href="#plotting-our-models-first-box-predictions-on-an-image" id="toc-plotting-our-models-first-box-predictions-on-an-image" class="nav-link" data-scroll-target="#plotting-our-models-first-box-predictions-on-an-image"><span class="header-section-number">11.4</span> Plotting our model’s first box predictions on an image</a></li>
  <li><a href="#aside-bounding-box-formats-in-and-out-of-our-model" id="toc-aside-bounding-box-formats-in-and-out-of-our-model" class="nav-link" data-scroll-target="#aside-bounding-box-formats-in-and-out-of-our-model"><span class="header-section-number">12</span> Aside: Bounding box formats in and out of our model</a></li>
  <li><a href="#preparing-data-at-scale" id="toc-preparing-data-at-scale" class="nav-link" data-scroll-target="#preparing-data-at-scale"><span class="header-section-number">13</span> Preparing data at scale</a>
  <ul class="collapse">
  <li><a href="#splitting-the-data-into-training-and-test-sets" id="toc-splitting-the-data-into-training-and-test-sets" class="nav-link" data-scroll-target="#splitting-the-data-into-training-and-test-sets"><span class="header-section-number">13.1</span> Splitting the data into training and test sets</a></li>
  <li><a href="#writing-a-function-for-preprocessing-multiple-samples-at-a-time" id="toc-writing-a-function-for-preprocessing-multiple-samples-at-a-time" class="nav-link" data-scroll-target="#writing-a-function-for-preprocessing-multiple-samples-at-a-time"><span class="header-section-number">13.2</span> Writing a function for preprocessing multiple samples at a time</a></li>
  <li><a href="#applying-our-preprocessing-function-to-each-data-split" id="toc-applying-our-preprocessing-function-to-each-data-split" class="nav-link" data-scroll-target="#applying-our-preprocessing-function-to-each-data-split"><span class="header-section-number">13.3</span> Applying our preprocessing function to each data split</a></li>
  <li><a href="#creating-a-collation-function" id="toc-creating-a-collation-function" class="nav-link" data-scroll-target="#creating-a-collation-function"><span class="header-section-number">13.4</span> Creating a collation function</a></li>
  </ul></li>
  <li><a href="#setting-up-trainingarguments-and-a-trainer-instance-to-train-our-model" id="toc-setting-up-trainingarguments-and-a-trainer-instance-to-train-our-model" class="nav-link" data-scroll-target="#setting-up-trainingarguments-and-a-trainer-instance-to-train-our-model"><span class="header-section-number">14</span> Setting up TrainingArguments and a Trainer instance to train our model</a>
  <ul class="collapse">
  <li><a href="#setting-up-our-trainingarguments" id="toc-setting-up-our-trainingarguments" class="nav-link" data-scroll-target="#setting-up-our-trainingarguments"><span class="header-section-number">14.1</span> Setting up our TrainingArguments</a></li>
  <li><a href="#optional-setting-up-an-optimizer-for-multiple-learning-rates" id="toc-optional-setting-up-an-optimizer-for-multiple-learning-rates" class="nav-link" data-scroll-target="#optional-setting-up-an-optimizer-for-multiple-learning-rates"><span class="header-section-number">14.2</span> Optional: Setting up an optimizer for multiple learning rates</a></li>
  <li><a href="#creating-an-evaluation-function" id="toc-creating-an-evaluation-function" class="nav-link" data-scroll-target="#creating-an-evaluation-function"><span class="header-section-number">14.3</span> Creating an evaluation function</a></li>
  <li><a href="#training-our-model-with-trainer" id="toc-training-our-model-with-trainer" class="nav-link" data-scroll-target="#training-our-model-with-trainer"><span class="header-section-number">14.4</span> Training our model with Trainer</a></li>
  <li><a href="#plotting-our-models-loss-curves" id="toc-plotting-our-models-loss-curves" class="nav-link" data-scroll-target="#plotting-our-models-loss-curves"><span class="header-section-number">14.5</span> Plotting our model’s loss curves</a></li>
  </ul></li>
  <li><a href="#making-predictions-on-the-test-dataset" id="toc-making-predictions-on-the-test-dataset" class="nav-link" data-scroll-target="#making-predictions-on-the-test-dataset"><span class="header-section-number">15</span> Making predictions on the test dataset</a>
  <ul class="collapse">
  <li><a href="#evaluating-our-test-predictions" id="toc-evaluating-our-test-predictions" class="nav-link" data-scroll-target="#evaluating-our-test-predictions"><span class="header-section-number">15.1</span> Evaluating our test predictions</a></li>
  <li><a href="#visualizing-our-test-dataset-evaluation-mertics" id="toc-visualizing-our-test-dataset-evaluation-mertics" class="nav-link" data-scroll-target="#visualizing-our-test-dataset-evaluation-mertics"><span class="header-section-number">15.2</span> Visualizing our test dataset evaluation mertics</a></li>
  <li><a href="#evaluating-and-visualizing-predictions-one-by-one" id="toc-evaluating-and-visualizing-predictions-one-by-one" class="nav-link" data-scroll-target="#evaluating-and-visualizing-predictions-one-by-one"><span class="header-section-number">15.3</span> Evaluating and visualizing predictions one by one</a></li>
  <li><a href="#comparing-our-models-predicted-boxes-to-the-ground-truth-boxes" id="toc-comparing-our-models-predicted-boxes-to-the-ground-truth-boxes" class="nav-link" data-scroll-target="#comparing-our-models-predicted-boxes-to-the-ground-truth-boxes"><span class="header-section-number">15.4</span> Comparing our model’s predicted boxes to the ground truth boxes</a></li>
  <li><a href="#predict-on-image-from-the-wild" id="toc-predict-on-image-from-the-wild" class="nav-link" data-scroll-target="#predict-on-image-from-the-wild"><span class="header-section-number">15.5</span> Predict on image from the wild</a></li>
  </ul></li>
  <li><a href="#uploading-our-trained-model-to-hugging-face-hub" id="toc-uploading-our-trained-model-to-hugging-face-hub" class="nav-link" data-scroll-target="#uploading-our-trained-model-to-hugging-face-hub"><span class="header-section-number">16</span> Uploading our trained model to Hugging Face Hub</a></li>
  <li><a href="#creating-a-demo-of-our-model-with-gradio" id="toc-creating-a-demo-of-our-model-with-gradio" class="nav-link" data-scroll-target="#creating-a-demo-of-our-model-with-gradio"><span class="header-section-number">17</span> Creating a demo of our model with Gradio</a>
  <ul class="collapse">
  <li><a href="#making-an-app-file" id="toc-making-an-app-file" class="nav-link" data-scroll-target="#making-an-app-file"><span class="header-section-number">17.1</span> Making an app file</a></li>
  <li><a href="#making-a-requirements-file" id="toc-making-a-requirements-file" class="nav-link" data-scroll-target="#making-a-requirements-file"><span class="header-section-number">17.2</span> Making a requirements file</a></li>
  <li><a href="#making-a-readme-file" id="toc-making-a-readme-file" class="nav-link" data-scroll-target="#making-a-readme-file"><span class="header-section-number">17.3</span> Making a README file</a></li>
  <li><a href="#making-an-examples-folder" id="toc-making-an-examples-folder" class="nav-link" data-scroll-target="#making-an-examples-folder"><span class="header-section-number">17.4</span> Making an examples folder</a></li>
  <li><a href="#uploading-our-demo-to-hugging-face-spaces" id="toc-uploading-our-demo-to-hugging-face-spaces" class="nav-link" data-scroll-target="#uploading-our-demo-to-hugging-face-spaces"><span class="header-section-number">17.5</span> Uploading our demo to Hugging Face Spaces</a></li>
  <li><a href="#testing-the-hosted-demo" id="toc-testing-the-hosted-demo" class="nav-link" data-scroll-target="#testing-the-hosted-demo"><span class="header-section-number">17.6</span> Testing the hosted demo</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">18</span> Summary</a>
  <ul class="collapse">
  <li><a href="#extensions" id="toc-extensions" class="nav-link" data-scroll-target="#extensions"><span class="header-section-number">18.1</span> Extensions</a></li>
  <li><a href="#extra-curriculum" id="toc-extra-curriculum" class="nav-link" data-scroll-target="#extra-curriculum"><span class="header-section-number">18.2</span> Extra-Curriculum</a></li>
  <li><a href="#extra-resources" id="toc-extra-resources" class="nav-link" data-scroll-target="#extra-resources"><span class="header-section-number">18.3</span> Extra resources</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/mrdbourke/learn-huggingface/issues" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../notebooks/hugging_face_object_detection_tutorial.html">Computer Vision</a></li><li class="breadcrumb-item"><a href="../notebooks/hugging_face_object_detection_tutorial.html">Build a custom object detection model and demo</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Object Detection with Hugging Face Transformers Tutorial</h1>
</div>

<div>
  <div class="description">
    Learn how to create a custom object detection model with Hugging Face Transformers.
  </div>
</div>


<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<p><a target="_blank" href="https://colab.research.google.com/github/mrdbourke/learn-huggingface/blob/main/notebooks/hugging_face_object_detection_tutorial.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a></p>
<p><strong>Note:</strong> If you’re running in Google Colab, make sure to enable GPU usage by going to Runtime -&gt; Change runtime type -&gt; select GPU.</p>
<p><a href="https://github.com/mrdbourke/learn-huggingface/blob/main/notebooks/hugging_face_object_detection_tutorial.ipynb">Source code on GitHub</a> | <a href="https://www.learnhuggingface.com/notebooks/hugging_face_object_detection_tutorial">Online book version</a> | <a href="https://www.learnhuggingface.com/extras/setup">Setup guide</a> | Video Course (coming soon)</p>
<section id="overview" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="overview"><span class="header-section-number">1</span> Overview</h2>
<p>Welcome to the Learn Hugging Face Object Detection project!</p>
<p>Inside this project, we’ll learn bits and pieces about the Hugging Face ecosystem as well as how to build our own custom object detection model.</p>
<p>We’ll start with a collection of images with bounding box files as our dataset, fine-tune an existing computer vision model to detect items in an image and then share our model as a demo others can use.</p>
<figure style="text-align: center;" class="figure">
<img src="https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-trashify-object-detection/00-what-were-going-to-build.png" alt="A presentation slide titled 'Project: Trashify' with a trash bin emoji illustrates a machine learning workflow. The first stage, 'Data,' shows an image of a hand holding trash near a bin, with bounding boxes identifying 'bin,' 'trash,' and 'hand,' and is labeled with '🤗 Hugging Face Datasets.' This feeds into the 'Model' stage, represented by a neural network diagram and labeled '🤗 Hugging Face Transformers.' Finally, the 'Demo' stage displays a screenshot of the 'Trashify Object Detection Demo V4' web application, where the model successfully identifies these elements in an input image, labeled '🤗 Hugging Face Hub/Spaces + Gradio.' Arrows indicate the flow from Data to Model to Demo." style="width: 100%; max-width: 900px; height: auto;" class="figure-img">
<figcaption>
We’re going to put on our startup hats and build a Trashify object detection model using tools from the Hugging Face ecosystem.
</figcaption>
</figure>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Feel to keep reading through the notebook but if you’d like to run the code yourself, be sure to go through the <a href="https://www.learnhuggingface.com/extras/setup">setup guide</a> first.</p>
</div>
</div>
<section id="what-were-going-to-build" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="what-were-going-to-build"><span class="header-section-number">1.1</span> What we’re going to build</h3>
<p>We’re going to be bulding Trashify 🚮, an <strong>object detection model</strong> which incentivises people to pick up trash in their local area by detecting <code>bin</code>, <code>trash</code>, <code>hand</code>.</p>
<p>If all three items are detected, a person gets +1 point!</p>
<p>For example, say you were going for a walk around your neighbourhood and took a photo of yourself picking up a piece (with your <strong>hand</strong> or <strong>trash arm</strong>) of <strong>trash</strong> and putting it in the <strong>bin</strong>, you would get a point.</p>
<p>With this object detection model, you could deploy it to an application which would automatically detect the target classes and then save the result to an online leaderboard.</p>
<p>The incentive would be to score the most points, in turn, picking up the most piecces of trash, in a given area.</p>
<p>More specifically, we’re going to follow the following steps:</p>
<ol type="1">
<li><strong><a href="https://huggingface.co/datasets/mrdbourke/trashify_manual_labelled_images">Data</a>: Problem defintion and dataset preparation</strong> - Getting a dataset/setting up the problem space.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/main/en/model_doc/rt_detr_v2">Model</a>: Finding, training and evaluating a model</strong> - Finding an object detection model suitable for our problem on Hugging Face and customizing it to our own dataset.</li>
<li><strong><a href="https://huggingface.co/spaces/mrdbourke/trashify_demo_v4">Demo</a>: Creating a demo and put our model into the real world</strong> - Sharing our trained model in a way others can access and use.</li>
</ol>
<p>By the end of this project, you’ll have a trained model and <a href="https://huggingface.co/spaces/mrdbourke/trashify_demo_v4">demo on Hugging Face</a> you can share with others:</p>
<div id="cell-4" class="cell" data-execution_count="125">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> HTML </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>HTML(<span class="st">"""</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;iframe</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="st">    src="https://mrdbourke-trashify-demo-v4.hf.space"</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="st">    frameborder="0"</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="st">    width="850"</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="st">    height="1150"</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="st">&gt;&lt;/iframe&gt;</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="125">

<iframe src="https://mrdbourke-trashify-demo-v4.hf.space" frameborder="0" width="850" height="1150"></iframe>
</div>
</div>
</section>
<section id="what-is-object-detection" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="what-is-object-detection"><span class="header-section-number">1.2</span> What is object detection?</h3>
<p>Object detection is the process of identifying and locating an item in an image.</p>
<p>Where <em>item</em> can mean almost anything.</p>
<p>For example:</p>
<ul>
<li>Detecting car <strong>licence plates</strong> in a video feed (videos are a series of images) for a parking lot entrance.</li>
<li>Detecting <strong>delivery people</strong> walking towards your front door on a security camera.</li>
<li>Detecting <strong>defects</strong> on a manufacturing line.</li>
<li>Detecting <a href="https://ieeexplore.ieee.org/abstract/document/9968423"><strong>pot holes</strong> in the road</a> so repair works can automatically be scheduled.</li>
<li>Detecting <strong>small pests (Varroa Mite)</strong> on the bodies of bees.</li>
<li>Detecting <a href="https://ai.meta.com/blog/pytorch-drives-next-gen-intelligent-farming-machines/"><strong>weeds</strong> in a field</a> so you know what to remove and what to keep.</li>
</ul>
<p>And for some trash identification examples:</p>
<ul>
<li>Google <a href="https://blog.tensorflow.org/2022/10/circularnet-reducing-waste-with-machine.html">open-sourcing CircularNet</a> for helping to detect and identify different kinds of trash in waste management facilities.</li>
<li>A <a href="https://www.sciencedirect.com/science/article/abs/pii/S0956053X23001915">machine learning paper</a> for using a computer vision model on a Raspberry Pi (a small computer) for waste identification.</li>
<li><a href="https://www.ameru.ai">Ameru</a> is a company building a trash-identifying bin to sort automatically sort trash as people put it in, see the <a href="https://labelstud.io/blog/ameru-labeling-for-a-greener-world/">case study of hose they created their own custom dataset using Label Studio</a>.</li>
</ul>
<figure style="text-align: center;" class="figure">
<img src="https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-trashify-object-detection/01-where-object-detection-is-used.png" alt="A collage of five images illustrating computer vision applications: the top left shows a car at a parking barrier with its license plate highlighted by a green bounding box; the top right displays various trash items on a conveyor belt, each identified and outlined by a colored bounding box and label; the bottom left is a close-up of a bee with a green bounding box highlighting a varroa mite on its back; the bottom middle features a damaged road with two potholes, each outlined by a green bounding box; and the image on the right shows a smart waste bin with an interactive screen and a green bounding box highlighting an item in its collection bag visible through a window." style="width: 100%; max-width: 900px; height: auto;" class="figure-img">
<figcaption>
Different problems where object detection can be used: license plate detection, trash type identification, pest detection on bees and pothole detection.
</figcaption>
</figure>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Note:</strong> Object detection is also sometimes referred to as <em>image localization</em> or <em>object localization</em>. For consistency, I will use the term object detection, however, either of these terms could substitute.</p>
</div>
</div>
<p><strong>Image classification</strong> deals with classifying an image as a whole into a single <code>class</code>, object detection endeavours to find the specific target item and <em>where</em> it is in an image.</p>
<p>One of the most common ways of showing where an item is in an image is by displaying a <strong>bounding box</strong> (a rectangle-like box around the target item).</p>
<p>An object detection model will often take an input image tensor in the shape <code>[3, 640, 640]</code> (<code>[colour_channels, height, width]</code>) and output a tensor in the form <code>[class_name, x_min, y_min, x_max, y_max]</code> or <code>[class_name, x1, y1, x2, y2]</code> (this is two ways to write the same example format, there are more formats, we’ll see these below in <a href="#tbl-bbox-formats" class="quarto-xref">Table&nbsp;1</a>).</p>
<p>Where:</p>
<ul>
<li><code>class_name</code> = The classification of the target item (e.g.&nbsp;<code>"car"</code>, <code>"person"</code>, <code>"banana"</code>, <code>"piece_of_trash"</code>, this could be almost anything).</li>
<li><code>x_min</code> = The <code>x</code> value of the top left corner of the box.</li>
<li><code>y_min</code> = The <code>y</code> value of the top left corner of the box.</li>
<li><code>x_max</code> = The <code>x</code> value of the bottom right corner of the box.</li>
<li><code>y_max</code> = The <code>y</code> value of the bottom right corner of the box.</li>
</ul>
<figure style="text-align: center;" class="figure">
<img src="https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-trashify-object-detection/02-different-bounding-box-formats.png" alt="A side-by-side comparison of two diagrams illustrating bounding box coordinates for an image of a hand opening a metal trash bin, with the trash bin enclosed in a magenta bounding box; the left diagram, titled 'Normalized Bounding Box Coordinates in Various Formats,' displays coordinates as ratios relative to the image dimensions (960px width, 1280px height) for XYXY, XYWH, and CXCYWH formats, while the right diagram, titled 'Absolute Bounding Box Coordinates in Various Formats,' shows the same bounding box coordinates in absolute pixel values for the same formats." style="width: 100%; max-width: 900px; height: auto;" class="figure-img">
<figcaption>
Normalized bounding box coordinates in CXCYWH format as well as an absolute bounding in XYXY format.
</figcaption>
</figure>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Object detection bounding box formats
</div>
</div>
<div class="callout-body-container callout-body">
<p>When you get into the world of object detection, you will find that there are several different bounding box formats.</p>
<p>There are three major formats you should be familiar with: <code>XYXY</code>, <code>XYWH</code>, <code>CXCYWH</code> (there are more but these are the most common).</p>
<p>Knowing which bounding box format you’re working with can be the difference between a good model and a <em>very</em> poor model (wrong bounding boxes = wrong outcome).</p>
<p>We’ll get hands-on with a couple of these in this project.</p>
<p>But for an in-depth example of all three, I created a <a href="https://www.learnml.io/posts/a-guide-to-bounding-box-formats/">guide on different bounding box formats and how to draw them</a>, reading this should give a good intuition behind each style of bounding box.</p>
</div>
</div>
</section>
<section id="why-train-your-own-object-detection-models" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="why-train-your-own-object-detection-models"><span class="header-section-number">1.3</span> Why train your own object detection models?</h3>
<p>You can customize <strong>pre-trained models</strong> for object detection as well as API-powered models and LLMs such as <a href="https://ai.google.dev/gemini-api/docs/vision?lang=python#bbox">Gemini</a>, <a href="https://landing.ai/agentic-object-detection">LandingAI</a> and <a href="https://github.com/IDEA-Research/DINO-X-API">DINO-X</a>.</p>
<p>Depending on your requirements, there are several pros and cons for using your own model versus using an API.</p>
<p>Training/fine-tuning your own model:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Pros</th>
<th style="text-align: left;">Cons</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Control:</strong> Full control over model lifecycle.</td>
<td style="text-align: left;">Can be complex to get setup.</td>
</tr>
<tr class="even">
<td style="text-align: left;">No usage limits (aside from compute constraints).</td>
<td style="text-align: left;">Requires dedicated compute resources for training/inference.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Can train once and deploy everywhere/whenever you want (for example, Tesla deploying a model to all self-driving cars).</td>
<td style="text-align: left;">Requires maintenance over time to ensure performance remains up to par.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Privacy:</strong> Data can be kept in-house/app and doesn’t need to go to a third party.</td>
<td style="text-align: left;">Can require longer development cycles compared to using existing APIs.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Speed:</strong> Customizing a small model for a specific use case often means it runs much faster on local hardware, for example, modern object detection models can achieve 70-100+ FPS (frames per second) on modern GPU hardware.</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Using a pre-built model API:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Pros</th>
<th style="text-align: left;">Cons</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Ease of use:</strong> often can be setup within a few lines of code.</td>
<td style="text-align: left;">If the model API goes down, your service goes down.</td>
</tr>
<tr class="even">
<td style="text-align: left;">No maintenance of compute resources.</td>
<td style="text-align: left;">Data is required to be sent to a third-party for processing.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Access to the most advanced models.</td>
<td style="text-align: left;">The API may have usage limits per day/time period.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Can scale if usage increases.</td>
<td style="text-align: left;">Can be much slower than using dedicated models due to requiring an API call.</td>
</tr>
</tbody>
</table>
<p>For this project, we’re going to focus on fine-tuning our own model.</p>
</section>
<section id="workflow-were-going-to-follow" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="workflow-were-going-to-follow"><span class="header-section-number">1.4</span> Workflow we’re going to follow</h3>
<p>The good news for us is that the Hugging Face ecosystem makes working on custom machine learning projects an absolute blast.</p>
<p>And workflow is reproducible across several kinds of projects.</p>
<p>Start with data (or skip this step and go straight to a model) -&gt; get/customize a model -&gt; build and share a demo.</p>
<p>With this in mind, our motto is <em>data, model, demo!</em></p>
<p>More specifically, we’re going to follow the rough workflow of:</p>
<ol type="1">
<li>Create, preprocess and load data using <a href="https://huggingface.co/docs/datasets/index">Hugging Face Datasets</a>.</li>
<li>Define the model we’d like use with <a href="https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForObjectDetection"><code>transformers.AutoModelForObjectDetection</code></a> (or another similar model class).</li>
<li>Define training arguments (these are hyperparameters for our model) with <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments"><code>transformers.TrainingArguments</code></a>.</li>
<li>Pass <code>TrainingArguments</code> from 3 and target datasets to an instance of <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer"><code>transformers.Trainer</code></a>.</li>
<li>Train the model by calling <a href="https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/trainer#transformers.Trainer.train"><code>Trainer.train()</code></a>.</li>
<li>Save the model (to our local machine or to the Hugging Face Hub).</li>
<li>Evaluate the trained model by making and inspecting predctions on the test data.</li>
<li>Turn the model into a shareable demo.</li>
</ol>
<p>I say rough because machine learning projects are often non-linear in nature.</p>
<p>As in, because machine learning projects involve many experiments, they can kind of be all over the place.</p>
<p>But this worfklow will give us some good guidelines to follow.</p>
<figure style="text-align: center; display: inline-block;" class="figure">
<!-- figtemplate -->
<img src="https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-text-classification/01-hugging-face-workflow.png" alt="The diagram shows the Hugging Face model development workflow, which includes the following steps: start with an idea or problem, get data ready (turn into tensors/create data splits), pick a pretrained model (to suit your problem), train/fine-tune the model on your custom data, evaluate the model, improve through experimentation, save and upload the fine-tuned model to the Hugging Face Hub, and turn your model into a shareable demo. Tools used in this workflow are Datasets/Tokenizers, Transformers/PEFT/Accelerate/timm, Hub/Spaces/Gradio, and Evaluate." style="width: 100%; max-width: 900px; height: auto;" class="figure-img">
<figcaption style="width: 100%; box-sizing: border-box;">
A general Hugging Face workflow from idea to shared model and demo using tools from the Hugging Face ecosystem. You’ll notice some of the steps don’t match with our workflow outline above. This is because the text-based workflow outline above breaks some of the steps down for educational purposes. These kind of workflows are not set in stone and are more of guide than specific directions. See information on each of the tools in the <a href="https://huggingface.co">Hugging Face documentation</a>.
</figcaption>
</figure>
</section>
</section>
<section id="importing-necessary-libraries" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="importing-necessary-libraries"><span class="header-section-number">2</span> Importing necessary libraries</h2>
<p>Let’s get started!</p>
<p>First, we’ll import the required libraries.</p>
<p>If you’re running on your local computer, be sure to check out the getting <a href="https://www.learnhuggingface.com/extras/setup">setup guide</a> to make sure you have everything you need.</p>
<p>If you’re using Google Colab, many of them the following libraries will be installed by default.</p>
<p>However, we’ll have to install a few extras to get everything working.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you’re running on Google Colab, this notebook will work best with access to a GPU. To enable a GPU, go to <code>Runtime</code> ➡️ <code>Change runtime type</code> ➡️ <code>Hardware accelerator</code> ➡️ <code>GPU</code>.</p>
</div>
</div>
<p>We’ll need to install the following libraries from the Hugging Face ecosystem:</p>
<ul>
<li><a href="https://huggingface.co/docs/transformers/en/installation"><code>transformers</code></a> - comes pre-installed on Google Colab but if you’re running on your local machine, you can install it via <code>pip install transformers</code>.</li>
<li><a href="https://huggingface.co/docs/datasets/installation"><code>datasets</code></a> - a library for accessing and manipulating datasets on and off the Hugging Face Hub, you can install it via <code>pip install datasets</code>.</li>
<li><a href="https://www.gradio.app/guides/quickstart#installation"><code>gradio</code></a> - a library for creating interactive demos of machine learning models, you can install it via <code>pip install gradio</code>.</li>
<li>Optional: <a href="https://huggingface.co/docs/evaluate/installation"><code>evaluate</code></a> - a library for evaluating machine learning model performance with various metrics, you can install it via <code>pip install evaluate</code>.</li>
<li>Optional: <a href="https://huggingface.co/docs/accelerate/basic_tutorials/install"><code>accelerate</code></a> - a library for training machine learning models faster, you can install it via <code>pip install accelerate</code>.</li>
</ul>
<p>And the following library is not part of the Hugging Face ecosystem but it is helpful for evaluating our models:</p>
<ul>
<li><a href="https://lightning.ai/docs/torchmetrics/stable/"><code>torchmetrics</code></a> - a library containing many evaluation metrics compatible with PyTorch/Transformers, you can install it via <code>pip install torchmetrics</code>.</li>
</ul>
<p>We can also check the versions of our software with <code>package_name.__version__</code>.</p>
<div id="cell-9" class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install/import dependencies (this is mostly for Google Colab, as the other dependences are available by default in Colab)</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="im">import</span> datasets</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  <span class="im">import</span> gradio <span class="im">as</span> gr</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  <span class="im">import</span> torchmetrics</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  <span class="im">import</span> pycocotools </span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">ModuleNotFoundError</span>:</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># If a module isn't found, install it </span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>  <span class="op">!</span>pip install <span class="op">-</span>U datasets gradio <span class="co"># -U stands for "upgrade" so we'll get the latest version by default</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>  <span class="op">!</span>pip install <span class="op">-</span>U torchmetrics[detection]</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>  <span class="im">import</span> datasets</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>  <span class="im">import</span> gradio <span class="im">as</span> gr</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Required for evalation</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>  <span class="im">import</span> torchmetrics</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>  <span class="im">import</span> pycocotools <span class="co"># make sure we have this for torchmetrics</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> transformers</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Check versions (as long as you've got the following versions or higher, you should be good)</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Using transformers version: </span><span class="sc">{</span>transformers<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Using datasets version: </span><span class="sc">{</span>datasets<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Using torch version: </span><span class="sc">{</span>torch<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Using torchmetrics version: </span><span class="sc">{</span>torchmetrics<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Using transformers version: 4.52.4
Using datasets version: 3.6.0
Using torch version: 2.7.0+cu126
Using torchmetrics version: 1.7.1</code></pre>
</div>
</div>
<p>Wonderful, as long as your versions are the same or higher to the versions above, you should be able to run the code below.</p>
</section>
<section id="getting-a-dataset" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="getting-a-dataset"><span class="header-section-number">3</span> Getting a dataset</h2>
<p>Okay, now we’re got the required libraries, let’s get a dataset.</p>
<p>Getting a dataset is one of the most important things a machine learning project.</p>
<p>The dataset you often determines the type of model you use as well as the quality of the outputs of that model.</p>
<p>Meaning, if you have a high quality dataset, chances are, your future model could also have high quality outputs.</p>
<p>It also means if your dataset is of poor quality, your model will likely also have poor quality outputs.</p>
<p>For an object detection problem, your dataset will likely come in the form of a group of images as well as a file with annotations belonging to those images.</p>
<p>For example, you might have the following setup:</p>
<pre><code>folder_of_images/
    image_1.jpeg
    image_2.jpeg
    image_3.jpeg
annotations.json</code></pre>
<p>Where the <code>annotations.json</code> contains details about the contains of each image:</p>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>annotations.json</strong></pre>
</div>
<div class="sourceCode" id="cb5" data-filename="annotations.json"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ot">[</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">{</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="er">'image_path'</span><span class="fu">:</span> <span class="er">'image_</span><span class="dv">1</span><span class="er">.jpeg'</span><span class="fu">,</span> </span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        <span class="er">'image_id'</span><span class="fu">:</span> <span class="dv">42</span><span class="fu">,</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="er">'annotations'</span><span class="fu">:</span> </span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>            <span class="fu">{</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>                <span class="er">'file_name'</span><span class="fu">:</span> <span class="ot">[</span><span class="er">'image_</span><span class="dv">1</span><span class="er">.jpeg'</span><span class="ot">]</span><span class="fu">,</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>                <span class="er">'image_id'</span><span class="fu">:</span> <span class="ot">[</span><span class="dv">42</span><span class="ot">]</span><span class="fu">,</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>                <span class="er">'category_id'</span><span class="fu">:</span> <span class="ot">[</span><span class="dv">1</span><span class="ot">]</span><span class="fu">,</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>                <span class="er">'bbox'</span><span class="fu">:</span> <span class="ot">[</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>                            <span class="ot">[</span><span class="fl">360.20001220703125</span><span class="ot">,</span> <span class="fl">528.5</span><span class="ot">,</span> <span class="fl">177.1999969482422</span><span class="ot">,</span> <span class="fl">261.79998779296875</span><span class="ot">],</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>                        <span class="ot">]</span><span class="fu">,</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>                <span class="er">'area'</span><span class="fu">:</span> <span class="ot">[</span><span class="fl">46390.9609375</span><span class="ot">]</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>            <span class="fu">},</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="er">'label_source'</span><span class="fu">:</span> <span class="er">'manual_prodigy_label'</span><span class="fu">,</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        <span class="er">'image_source'</span><span class="fu">:</span> <span class="er">'manual_taken_photo'</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">}</span><span class="ot">,</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    <span class="er">...(more</span> <span class="er">labels</span> <span class="er">down</span> <span class="er">here)</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="ot">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Don’t worry too much about the exact meaning of everything in the above <code>annotations.json</code> file for now (this is only one example, there are many different ways object detection information could be displayed).</p>
<p>The main point is that each target image is paired with an assosciated label.</p>
<p>Now like all good machine learning cooking shows, I’ve prepared a dataset from earlier.</p>
<figure style="text-align: center;" class="figure">
<a href="https://huggingface.co/datasets/mrdbourke/trashify_manual_labelled_images"> <img src="https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-trashify-object-detection/03-trashify-dataset-on-huggingface.png" alt="A screenshot of a Hugging Face webpage displaying the 'mrdbourke/trashify_manual_labelled_images' dataset, showing the 'Dataset card' tab with the 'Dataset Viewer' section visible, which lists image thumbnails, 'image_id', 'annotations', 'label_source', and 'image_source' for each entry; on the right sidebar, dataset statistics like 'Downloads last month' (106), 'Size of downloaded dataset files' (1.03 GB), and 'Number of rows' (1,128) are displayed, along with a 'Use this dataset' button; below the viewer, a 'Load data' section shows Python code to import and load the dataset using the 'datasets' library." style="width: 100%; max-width: 900px; height: auto;" class="figure-img"> </a>
<figcaption>
Our Trashify dataset is available on Hugging Face. These images have been labelled manually with bounding boxes for different classes.
</figcaption>
</figure>
<p>It’s stored on Hugging Face Datasets (also called the Hugging Face Hub) under the name <a href="https://huggingface.co/datasets/mrdbourke/trashify_manual_labelled_images"><code>mrdbourke/trashify_manual_labelled_images</code></a>.</p>
<p>This is a dataset I’ve collected manually by hand (yes, by picking up 1000+ pieces of trash and photographing it) as well as labelled by hand (by drawing boxes on each image with a labelling tool called <a href="https://prodi.gy/features/computer-vision">Prodigy</a>).</p>
<section id="loading-the-dataset" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="loading-the-dataset"><span class="header-section-number">3.1</span> Loading the dataset</h3>
<p>To load a dataset stored on the Hugging Face Hub we can use the <a href="https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset"><code>datasets.load_dataset(path=NAME_OR_PATH_OF_DATASET)</code></a> function and pass it the name/path of the dataset we want to load.</p>
<p>In our case, our dataset name is <code>mrdbourke/trashify_manual_labelled_images</code> (you can also change this for your own dataset).</p>
<p>And since our dataset is hosted on Hugging Face, when we run the following code for the first time, it will download it.</p>
<p>If your target dataset is quite large, this download may take a while.</p>
<p>However, once the dataset is downloaded, subsequent reloads will be mush faster.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Getting information about a function/method">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Getting information about a function/method
</div>
</div>
<div class="callout-body-container callout-body">
<p>One way to find out what a function or method does is to lookup the documentation.</p>
<p>Another way is to write the function/method name with a question mark afterwards.</p>
<p>For example:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>load_dataset?</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Give it a try.</p>
<p>You should see some helpful information about what inputs the method takes and how they are used.</p>
</div>
</div>
<p>Let’s load our dataset and check it out.</p>
<div id="cell-13" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load our Trashify dataset</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(path<span class="op">=</span><span class="st">"mrdbourke/trashify_manual_labelled_images"</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>dataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>DatasetDict({
    train: Dataset({
        features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],
        num_rows: 1128
    })
})</code></pre>
</div>
</div>
<p>Beautiful!</p>
<p>We can see that there is a <code>train</code> split of the dataset already which currently contains all of the samples (<code>1128</code> in total).</p>
<p>There are also some <code>features</code> that come with our dataset which are related to our object detection goal.</p>
<div id="cell-15" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Length of original dataset: </span><span class="sc">{</span><span class="bu">len</span>(dataset[<span class="st">'train'</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Dataset features:"</span>) </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pprint <span class="im">import</span> pprint</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>pprint(dataset[<span class="st">'train'</span>].features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Length of original dataset: 1128
[INFO] Dataset features:
{'annotations': Sequence(feature={'area': Value(dtype='float32', id=None),
                                  'bbox': Sequence(feature=Value(dtype='float32',
                                                                 id=None),
                                                   length=4,
                                                   id=None),
                                  'category_id': ClassLabel(names=['bin',
                                                                   'hand',
                                                                   'not_bin',
                                                                   'not_hand',
                                                                   'not_trash',
                                                                   'trash',
                                                                   'trash_arm'],
                                                            id=None),
                                  'file_name': Value(dtype='string', id=None),
                                  'image_id': Value(dtype='int64', id=None),
                                  'iscrowd': Value(dtype='int64', id=None)},
                         length=-1,
                         id=None),
 'image': Image(mode=None, decode=True, id=None),
 'image_id': Value(dtype='int64', id=None),
 'image_source': Value(dtype='string', id=None),
 'label_source': Value(dtype='string', id=None)}</code></pre>
</div>
</div>
<p>Nice!</p>
<p>We can see our dataset <code>features</code> contain the following fields:</p>
<ul>
<li><code>annotations</code> - A sequence of values including a <code>bbox</code> field (short for bounding box) as well as <code>category_id</code> field which contains the target objects we’d like to identify in our images (<code>['bin', 'hand', 'not_bin', 'not_hand', 'not_trash', 'trash', 'trash_arm']</code>).</li>
<li><code>image</code> - This contains the target image assosciated with a given set of <code>annotations</code> (in our case, images and annotations have been uploaded to the Hugging Face Hub together).</li>
<li><code>image_id</code> - A unique ID assigned to a given sample.</li>
<li><code>image_source</code> - Where the image came from (all of our images have been manually collected).</li>
<li><code>label_source</code> - Where the image label came from (all of our images have been manually labelled).</li>
</ul>
</section>
<section id="viewing-a-single-sample-from-our-data" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="viewing-a-single-sample-from-our-data"><span class="header-section-number">3.2</span> Viewing a single sample from our data</h3>
<p>Now we’ve seen the features, let’s check out a single sample from our dataset.</p>
<p>We can index on a single sample of the <code>"train"</code> set just like indexing on a Python list.</p>
<div id="cell-18" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># View a single sample of the dataset</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">"train"</span>][<span class="dv">42</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>{'image': &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,
 'image_id': 745,
 'annotations': {'file_name': ['094f4f41-dc07-4704-96d7-8d5e82c9edb9.jpeg',
   '094f4f41-dc07-4704-96d7-8d5e82c9edb9.jpeg',
   '094f4f41-dc07-4704-96d7-8d5e82c9edb9.jpeg'],
  'image_id': [745, 745, 745],
  'category_id': [5, 1, 0],
  'bbox': [[333.1000061035156,
    611.2000122070312,
    244.89999389648438,
    321.29998779296875],
   [504.0, 612.9000244140625, 451.29998779296875, 650.7999877929688],
   [202.8000030517578,
    366.20001220703125,
    532.9000244140625,
    555.4000244140625]],
  'iscrowd': [0, 0, 0],
  'area': [78686.3671875, 293706.03125, 295972.65625]},
 'label_source': 'manual_prodigy_label',
 'image_source': 'manual_taken_photo'}</code></pre>
</div>
</div>
<p>We see a few more details here compared to just looking at the features.</p>
<p>We notice the <code>image</code> is a <a href="https://pillow.readthedocs.io/en/stable/reference/Image.html"><code>PIL.Image</code></a> with size <code>960x1280</code> (width x height).</p>
<p>And the <code>file_name</code> is a UUID (Universially Unique Identifier, made with <a href="https://docs.python.org/3/library/uuid.html#uuid.uuid4"><code>uuid.uuid4()</code></a>).</p>
<p>The <code>bbox</code> field in the <code>annotations</code> key contains a list of bounding boxes assosciated with the image.</p>
<p>In this case, there are 3 different bounding boxes.</p>
<p>With the <code>category_id</code> values of <code>5</code>, <code>1</code>, <code>0</code> (we’ll map these to class names shortly).</p>
<p>Let’s inspect a single bounding box.</p>
<div id="cell-20" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">"train"</span>][<span class="dv">42</span>][<span class="st">"annotations"</span>][<span class="st">"bbox"</span>][<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>[333.1000061035156, 611.2000122070312, 244.89999389648438, 321.29998779296875]</code></pre>
</div>
</div>
<p>This array gives us the coordinates of a single bounding box in the format <code>XYWH</code>.</p>
<p>Where:</p>
<ul>
<li><code>X</code> is the x-coordinate of the top left corner of the box (<code>333.1</code>).</li>
<li><code>Y</code> is the y-coordinate of the top left corner of the box (<code>611.2</code>).</li>
<li><code>W</code> is the width of the box (<code>244.9</code>).</li>
<li><code>H</code> is the height of the box (<code>321.3</code>).</li>
</ul>
<p>All of these values are in absolute pixel values (meaning an x-coordinate of <code>333.1</code> is <code>333.1</code> pixels across on the x-axis).</p>
<p>How do I know this?</p>
<p>I know this because I created the box labels and this is the default value Prodigy (the labelling tool I used) outputs boxes.</p>
<p>However, if you were to come across another bouding box dataset, one of the first steps would be to <strong>figure out what format your bounding boxes are in</strong>.</p>
<p>We’ll see more on bounding box formats shortly.</p>
</section>
<section id="extracting-the-category-names-from-our-data" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="extracting-the-category-names-from-our-data"><span class="header-section-number">3.3</span> Extracting the category names from our data</h3>
<p>Before we start to visualize our sample image and bounding boxes, let’s extract the category names from our dataset.</p>
<p>We can do so by accessing the <code>features</code> attribute our of <code>dataset</code> and then following it through to find the <code>category_id</code> feature, this contains a list of our text-based class names.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>When working with different categories, it’s good practice to get a list or mapping (e.g.&nbsp;a Python dictionary) from category name to ID and vice versa.</p>
<p>For example:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Category to ID</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>{<span class="st">"class_name"</span>: <span class="dv">0</span>}</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># ID to Category</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>{<span class="dv">0</span>: <span class="st">"class_name"</span>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Not all datasets will have this implemented in an easy to access way, so it might take a bit of research to get it created.</p>
</div>
</div>
<p>Let’s access the class names in our dataset and save them to a variable <code>categories</code>.</p>
<div id="cell-23" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the categories from the dataset</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: This requires the dataset to have been uploaded with this information setup, not all datasets will have this available.</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>categories <span class="op">=</span> dataset[<span class="st">"train"</span>].features[<span class="st">"annotations"</span>].feature[<span class="st">"category_id"</span>]</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the names attribute</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>categories.names</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>['bin', 'hand', 'not_bin', 'not_hand', 'not_trash', 'trash', 'trash_arm']</code></pre>
</div>
</div>
<p>Beautiful!</p>
<p>We get the following class names:</p>
<ul>
<li><code>bin</code> - A rubbish bin or trash can.</li>
<li><code>hand</code> - A person’s hand.</li>
<li><code>not_bin</code> - Negative version of <code>bin</code> for items that look like a <code>bin</code> but shouldn’t be identified as one.</li>
<li><code>not_hand</code> - Negative version of <code>hand</code> for items that look like a <code>hand</code> but shouldn’t be identified as one.</li>
<li><code>not_trash</code> - Negative version of <code>trash</code> for items that look like <code>trash</code> but shouldn’t be identified as it.</li>
<li><code>trash</code> - An item of trash you might find on a walk such as an old plastic bottle, food wrapper, cigarette butt or used coffee cup.</li>
<li><code>trash_arm</code> - A mechanical arm used for picking up trash.</li>
</ul>
<p>The goal of our computer vision model will be: given an image, detect items belonging to these target classes if they are present.</p>
</section>
<section id="creating-a-mapping-from-numbers-to-labels" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="creating-a-mapping-from-numbers-to-labels"><span class="header-section-number">3.4</span> Creating a mapping from numbers to labels</h3>
<p>Now we’ve got our text-based class names, let’s create a mapping from label to ID and ID to label.</p>
<p>For each of these, Hugging Face use the terminology <code>label2id</code> and <code>id2label</code> respectively.</p>
<div id="cell-26" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Map ID's to class names and vice versa</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>id2label <span class="op">=</span> {i: class_name <span class="cf">for</span> i, class_name <span class="kw">in</span> <span class="bu">enumerate</span>(categories.names)}</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>label2id <span class="op">=</span> {value: key <span class="cf">for</span> key, value <span class="kw">in</span> id2label.items()}</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Label to ID mapping:</span><span class="ch">\n</span><span class="sc">{</span>label2id<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"ID to label mapping:</span><span class="ch">\n</span><span class="sc">{</span>id2label<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co"># id2label, label2id</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Label to ID mapping:
{'bin': 0, 'hand': 1, 'not_bin': 2, 'not_hand': 3, 'not_trash': 4, 'trash': 5, 'trash_arm': 6}

ID to label mapping:
{0: 'bin', 1: 'hand', 2: 'not_bin', 3: 'not_hand', 4: 'not_trash', 5: 'trash', 6: 'trash_arm'}</code></pre>
</div>
</div>
</section>
<section id="creating-a-colour-palette" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="creating-a-colour-palette"><span class="header-section-number">3.5</span> Creating a colour palette</h3>
<p>Ok we know which class name matches to which ID, now let’s create a dictionary of different colours we can use to display our bounding boxes.</p>
<p>It’s one thing to plot bounding boxes, it’s another thing to make them look nice.</p>
<p>And we always want our plots looking nice!</p>
<p>We’ll colour the positive classes <code>bin</code>, <code>hand</code>, <code>trash</code>, <code>trash_arm</code> in nice bright colours.</p>
<p>And the negative classes <code>not_bin</code>, <code>not_hand</code>, <code>not_trash</code> in a light red colour to indicate they’re the negative versions.</p>
<p>Our colour dictionary will map <code>class_name</code> -&gt; <code>(red, green, blue)</code> (or <a href="https://en.wikipedia.org/wiki/RGB_color_model">RGB</a>) colour values.</p>
<div id="cell-28" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make colour dictionary</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>colour_palette <span class="op">=</span> {</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'bin'</span>: (<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">224</span>),         <span class="co"># Bright Blue (High contrast with greenery) in format (red, green, blue)</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'not_bin'</span>: (<span class="dv">255</span>, <span class="dv">80</span>, <span class="dv">80</span>),   <span class="co"># Light Red to indicate negative class</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'hand'</span>: (<span class="dv">148</span>, <span class="dv">0</span>, <span class="dv">211</span>),      <span class="co"># Dark Purple (Contrasts well with skin tones)</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'not_hand'</span>: (<span class="dv">255</span>, <span class="dv">80</span>, <span class="dv">80</span>),  <span class="co"># Light Red to indicate negative class</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'trash'</span>: (<span class="dv">0</span>, <span class="dv">255</span>, <span class="dv">0</span>),       <span class="co"># Bright Green (For trash-related items)</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'not_trash'</span>: (<span class="dv">255</span>, <span class="dv">80</span>, <span class="dv">80</span>), <span class="co"># Light Red to indicate negative class</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'trash_arm'</span>: (<span class="dv">255</span>, <span class="dv">140</span>, <span class="dv">0</span>), <span class="co"># Deep Orange (Highly visible)</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s check out what these colours look like!</p>
<p>It’s the ABV motto: <em>Always Be Visualizing!</em></p>
<p>We can plot our colours with <code>matplotlib</code>.</p>
<p>We’ll just have to write a small function to normalize our colour values from <code>[0, 255]</code> to <code>[0, 1]</code> (<code>matplotlib</code> expects our colour values to be between 0 and 1).</p>
<div id="cell-30" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalize RGB values to 0-1 range</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> normalize_rgb(rgb_tuple):</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">tuple</span>(x<span class="op">/</span><span class="dv">255</span> <span class="cf">for</span> x <span class="kw">in</span> rgb_tuple)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Turn colors into normalized RGB values for matplotlib</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>colors_and_labels_rgb <span class="op">=</span> [(key, normalize_rgb(value)) <span class="cf">for</span> key, value <span class="kw">in</span> colour_palette.items()]</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Create figure and axis</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">7</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">1</span>))</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten the axis array for easier iteration</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> ax.flatten()</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot each color square</span></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, (label, color) <span class="kw">in</span> <span class="bu">enumerate</span>(colors_and_labels_rgb):</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>    ax[idx].add_patch(plt.Rectangle(xy<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>), </span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>                                    width<span class="op">=</span><span class="dv">1</span>, </span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>                                    height<span class="op">=</span><span class="dv">1</span>, </span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>                                    facecolor<span class="op">=</span>color))</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>    ax[idx].set_title(label)</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>    ax[idx].set_xlim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>    ax[idx].set_ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>    ax[idx].axis(<span class="st">'off'</span>)</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="hugging_face_object_detection_tutorial_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Sensational!</p>
<p>Now we know what colours to look out for when we visualize our bounding boxes.</p>
</section>
</section>
<section id="plotting-a-single-image-and-visualizing-the-boxes" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="plotting-a-single-image-and-visualizing-the-boxes"><span class="header-section-number">4</span> Plotting a single image and visualizing the boxes</h2>
<p>Okay, okay, finally time to plot an image!</p>
<p>Let’s take a random sample from our <code>dataset</code> and plot the image as well as the box on it.</p>
<section id="creating-helper-functions-to-help-with-visualization" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="creating-helper-functions-to-help-with-visualization"><span class="header-section-number">4.1</span> Creating helper functions to help with visualization</h3>
<p>To save some space in our notebook (plotting many images can increase the size of our notebook dramatically), we’ll create two small helper functions:</p>
<ol type="1">
<li><code>half_image</code> - Halves the size of a given image.</li>
<li><code>half_boxes</code> - Divides the input coordinates of a given input box by 2.</li>
</ol>
<p>These functions aren’t 100% necessary in our workflow.</p>
<p>They’re just to make the images slightly smaller so they fit better in the notebook.</p>
<div id="cell-34" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> PIL</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> half_image(image: PIL.Image) <span class="op">-&gt;</span> PIL.Image:</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Resizes a given input image by half and returns the smaller version.</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> image.resize(size<span class="op">=</span>(image.size[<span class="dv">0</span>] <span class="op">//</span> <span class="dv">2</span>, image.size[<span class="dv">1</span>] <span class="op">//</span> <span class="dv">2</span>))</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> half_boxes(boxes):</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Halves an array/tensor of input boxes and returns them. Necessary for plotting them on a half-sized image.</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="co">    For example:</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a><span class="co">    boxes = [100, 100, 100, 100]</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a><span class="co">    half_boxes = half_boxes(boxes)</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a><span class="co">    print(half_boxes)</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a><span class="co">    &gt;&gt;&gt; [50, 50, 50, 50]</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(boxes, <span class="bu">list</span>):</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If boxes are list of lists, then we have multiple boxes</span></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> box <span class="kw">in</span> boxes:</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">isinstance</span>(box, <span class="bu">list</span>):</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> [[coordinate <span class="op">//</span> <span class="dv">2</span> <span class="cf">for</span> coordinate <span class="kw">in</span> box] <span class="cf">for</span> box <span class="kw">in</span> boxes]</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> [coordinate <span class="op">//</span> <span class="dv">2</span> <span class="cf">for</span> coordinate <span class="kw">in</span> boxes]         </span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(boxes, np.ndarray):</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (boxes <span class="op">//</span> <span class="dv">2</span>)</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(boxes, torch.Tensor):</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (boxes <span class="op">//</span> <span class="dv">2</span>)</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the functions </span></span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>image_test <span class="op">=</span> dataset[<span class="st">"train"</span>][<span class="dv">42</span>][<span class="st">"image"</span>]</span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>image_test_half <span class="op">=</span> half_image(image_test)</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Original image size: </span><span class="sc">{</span>image_test<span class="sc">.</span>size<span class="sc">}</span><span class="ss"> | Half image size: </span><span class="sc">{</span>image_test_half<span class="sc">.</span>size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>boxes_test_list <span class="op">=</span> [<span class="dv">100</span>, <span class="dv">100</span>, <span class="dv">100</span>, <span class="dv">100</span>]</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Original boxes: </span><span class="sc">{</span>boxes_test_list<span class="sc">}</span><span class="ss"> | Half boxes: </span><span class="sc">{</span>half_boxes(boxes_test_list)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a>boxes_test_torch <span class="op">=</span> torch.tensor([<span class="fl">100.0</span>, <span class="fl">100.0</span>, <span class="fl">100.0</span>, <span class="fl">100.0</span>])</span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Original boxes: </span><span class="sc">{</span>boxes_test_torch<span class="sc">}</span><span class="ss"> | Half boxes: </span><span class="sc">{</span>half_boxes(boxes_test_torch)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Original image size: (960, 1280) | Half image size: (480, 640)
[INFO] Original boxes: [100, 100, 100, 100] | Half boxes: [50, 50, 50, 50]
[INFO] Original boxes: tensor([100., 100., 100., 100.]) | Half boxes: tensor([50., 50., 50., 50.])</code></pre>
</div>
</div>
</section>
<section id="plotting-boxes-on-a-single-image-step-by-step" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="plotting-boxes-on-a-single-image-step-by-step"><span class="header-section-number">4.2</span> Plotting boxes on a single image step by step</h3>
<p>To plot an image and its assosciated boxes, we’ll do the following steps:</p>
<ol type="1">
<li>Select a random sample from the <code>dataset</code>.</li>
<li>Extract the <code>"image"</code> (our image is in <code>PIL</code> format) and <code>"bbox"</code> keys from the random sample.
<ul>
<li>We can also <em>optionally</em> halve the size of our image/boxes to save space. In our case, we will halve our image and boxes.</li>
</ul></li>
<li>Turn the box coordinates into a <code>torch.tensor</code> (we’ll be using <code>torchvision</code> utilities to plot the image and boxes).</li>
<li>Convert the box format from <code>XYXY</code> to <code>XYWH</code> using <a href="https://pytorch.org/vision/main/generated/torchvision.ops.box_convert.html"><code>torchvision.ops.box_convert</code></a> (we do this because <code>torchvision.utils.draw_bounding_boxes</code> requires <code>XYXY</code> format as input).</li>
<li>Get a list of label names (e.g.&nbsp;<code>"bin", "trash"</code>, etc) assosciated with each of the boxes as well as a list of colours to match (these will be from our <code>colour_palette</code>).</li>
<li>Draw the boxes on the target image by:
<ul>
<li>Turning the image into a tensor with <a href="https://pytorch.org/vision/main/generated/torchvision.transforms.functional.pil_to_tensor.html"><code>torchvision.transforms.functional.pil_to_tensor</code></a>.</li>
<li>Draw the bounding boxes on our image tensor with <a href="https://pytorch.org/vision/main/generated/torchvision.utils.draw_bounding_boxes.html"><code>torchvision.utils.draw_bounding_boxes</code></a>.</li>
<li>Turn the image and bounding box tensors back into a <code>PIL</code> image with <a href="https://pytorch.org/vision/main/generated/torchvision.transforms.functional.pil_to_tensor.html"><code>torchvision.transforms.functional.pil_to_tensor</code></a>.</li>
</ul></li>
</ol>
<p>Phew!</p>
<p>A fair few steps…</p>
<p>But we’ve got this!</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>If the terms <code>XYXY</code> or <code>XYWH</code> or all of the drawing methods sound a bit confusing or intimidating, don’t worry, there’s a fair bit going on here.</p>
<p>We’ll cover bounding box formats, such as <code>XYXY</code> shortly.</p>
<p>In the meantime, if you want to learn more about different bounding box formats and how to draw them, I wrote <a href="https://www.learnml.io/posts/a-guide-to-bounding-box-formats/"><em>A Guide to Bounding Box Formats and How to Draw Them</em></a> which you might find helpful.</p>
</div>
</div>
<div id="cell-36" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting a bounding box on a single image</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.ops <span class="im">import</span> box_convert</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.utils <span class="im">import</span> draw_bounding_boxes</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms.functional <span class="im">import</span> pil_to_tensor, to_pil_image </span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Select a random sample from our dataset</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>random_index <span class="op">=</span> random.randint(<span class="dv">0</span>, <span class="bu">len</span>(dataset[<span class="st">"train"</span>]))</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Showing training sample from index: </span><span class="sc">{</span>random_index<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>random_sample <span class="op">=</span> dataset[<span class="st">"train"</span>][random_index]</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Get image and boxes from random sample</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>random_sample_image <span class="op">=</span> random_sample[<span class="st">"image"</span>]</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>random_sample_boxes <span class="op">=</span> random_sample[<span class="st">"annotations"</span>][<span class="st">"bbox"</span>]</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Optional: Half the image and boxes for space saving (all of the following code will work with/without half size images)</span></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>half_random_sample_image <span class="op">=</span> half_image(random_sample_image)</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>half_random_sample_boxes <span class="op">=</span> half_boxes(random_sample_boxes)</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Turn box coordinates in a tensor</span></span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>boxes_xywh <span class="op">=</span> torch.tensor(half_random_sample_boxes)</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Boxes in XYWH format: </span><span class="sc">{</span>boxes_xywh<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Convert boxes from XYWH -&gt; XYXY </span></span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a><span class="co"># torchvision.utils.draw_bounding_boxes requires input boxes in XYXY format (X_min, y_min, X_max, y_max)</span></span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>boxes_xyxy <span class="op">=</span> box_convert(boxes<span class="op">=</span>boxes_xywh,</span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>                         in_fmt<span class="op">=</span><span class="st">"xywh"</span>,</span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>                         out_fmt<span class="op">=</span><span class="st">"xyxy"</span>)</span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Boxes XYXY: </span><span class="sc">{</span>boxes_xyxy<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Get label names of target boxes and colours to match</span></span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>random_sample_label_names <span class="op">=</span> [categories.int2str(x) <span class="cf">for</span> x <span class="kw">in</span> random_sample[<span class="st">"annotations"</span>][<span class="st">"category_id"</span>]]</span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>random_sample_colours <span class="op">=</span> [colour_palette[label_name] <span class="cf">for</span> label_name <span class="kw">in</span> random_sample_label_names]</span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Label names: </span><span class="sc">{</span>random_sample_label_names<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Colour codes: </span><span class="sc">{</span>random_sample_colours<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a><span class="co"># 6. Draw the boxes on the image as a tensor and then turn it into a PIL image</span></span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a>to_pil_image(</span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a>    pic<span class="op">=</span>draw_bounding_boxes(</span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a>        image<span class="op">=</span>pil_to_tensor(pic<span class="op">=</span>half_random_sample_image),</span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a>        boxes<span class="op">=</span>boxes_xyxy,</span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a>        colors<span class="op">=</span>random_sample_colours,</span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a>        labels<span class="op">=</span>random_sample_label_names,</span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a>        width<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a>        label_colors<span class="op">=</span>random_sample_colours</span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Showing training sample from index: 1045
Boxes in XYWH format: tensor([[155., 267., 172., 216.],
        [274., 361., 154., 192.]])
Boxes XYXY: tensor([[155., 267., 327., 483.],
        [274., 361., 428., 553.]])
Label names: ['bin', 'not_bin']
Colour codes: [(0, 0, 224), (255, 80, 80)]</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="12">
<div>
<figure class="figure">
<p><img src="hugging_face_object_detection_tutorial_files/figure-html/cell-13-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Outstanding!</p>
<p>Our first official bounding boxes plotted on an image!</p>
<p>Now the idea of Trashify 🚮 is coming to life.</p>
<p>Depending on the random sample you’re looking at, you should see some combination of <code>['bin', 'hand', 'not_bin', 'not_hand', 'not_trash', 'trash', 'trash_arm']</code>.</p>
<p>Our goal will be to build an object detection model to replicate these boxes on a given image.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Getting familiar with a dataset: viewing 100 random samples
</div>
</div>
<div class="callout-body-container callout-body">
<p>Whenever working with a new dataset, I find it good practice to view 100+ random samples of the data.</p>
<p>In our case, this would mean viewing 100 random images with their bounding boxes drawn on them.</p>
<p>Doing so starts to build your own intuition of the data.</p>
<p>Using this intuition, along with evaluation metrics, you can start to get a better idea of how your model might be performing later on.</p>
<p>Keep this in mind for any new dataset or problem space you’re working on.</p>
<p>Start by looking at 100+ random samples.</p>
<p>And yes, generally more is better.</p>
<p>So you can practice by running the code cell above a number of times to see the different kinds of images and boxes in the dataset.</p>
<p>Can you think of any scenarios which the dataset might be missing?</p>
</div>
</div>
</section>
</section>
<section id="different-bounding-box-formats" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="different-bounding-box-formats"><span class="header-section-number">5</span> Different bounding box formats</h2>
<p>When drawing our bounding box, we discussed the terms <code>XYXY</code> and <code>XYWH</code>.</p>
<p>Well, we didn’t really discuss these at all…</p>
<p>But that’s why we’re here.</p>
<p>One of the most confusing things in the world of object detection is the different formats bounding boxes come in.</p>
<p>Are your boxes in <code>XYXY</code>, <code>XYWH</code> or <code>CXCYWH</code>?</p>
<p>Are they in absolute format?</p>
<p>Or normalized format?</p>
<p>Perhaps a table will help us.</p>
<p>The following table contains a non-exhaustive list of some of the most common bounding box formats you’ll come across in the wild.</p>
<div id="tbl-bbox-formats" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-bbox-formats-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Different bounding box formats
</figcaption>
<div aria-describedby="tbl-bbox-formats-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th><strong>Box format</strong></th>
<th><strong>Description</strong></th>
<th><strong>Absolute Example</strong></th>
<th><strong>Normalized Example</strong></th>
<th><strong>Source</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>XYXY</td>
<td>Describes the top left corner coordinates <code>(x1, y1)</code> as well as the bottom right corner coordinates of a box. <br> Also referred to as: <br> <code>[x1, y1, x2, y2]</code> <br> or <br> <code>[x_min, y_min, x_max, y_max]</code></td>
<td><code>[8.9, 275.3, 867.5, 964.0]</code></td>
<td><code>[0.009, 0.215, 0.904, 0.753]</code></td>
<td><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/devkit_doc.html#SECTION00053000000000000000">PASCAL VOC Dataset</a> uses the absolute version of this format, <a href="https://pytorch.org/vision/main/generated/torchvision.utils.draw_bounding_boxes.html#draw-bounding-boxes"><code>torchvision.utils.draw_bounding_boxes</code></a> defaults to the absolute version of this format.</td>
</tr>
<tr class="even">
<td>XYWH</td>
<td>Describes the top left corner coordinates <code>(x1, y1)</code> as well as the width (<code>box_width</code>) and height (<code>box_height</code>) of the target box. The bottom right corners <code>(x2, y2)</code> are found by adding the width and height to the top left corner coordinates <code>(x1 + box_width, y1 + box_height)</code>. <br> Also referred to as: <br> <code>[x1, y1, box_width, box_height]</code> <br> or <br> <code>[x_min, y_min, box_width, box_height]</code></td>
<td><code>[8.9, 275.3, 858.6, 688.7]</code></td>
<td><code>[0.009, 0.215, 0.894, 0.538]</code></td>
<td>The <a href="https://cocodataset.org/#format-data">COCO (Common Objects in Context) dataset</a> uses the absolute version of this format, see the section under “bbox”.</td>
</tr>
<tr class="odd">
<td>CXCYWH</td>
<td>Describes the center coordinates of the bounding box <code>(center_x, center_y)</code> as well as the width (<code>box_width</code>) and height (<code>box_height</code>) of the target box. <br> Also referred to as: <br> <code>[center_x, center_y, box_width, box_height]</code></td>
<td><code>[438.2, 619.65, 858.6, 688.7]</code></td>
<td><code>[0.456, 0.484, 0.894, 0.538]</code></td>
<td>Normalized version introduced in the <a href="https://arxiv.org/abs/1804.02767">YOLOv3 (You Only Look Once) paper</a> and is used by many later forms of YOLO.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<section id="absolute-or-normalized-format" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="absolute-or-normalized-format"><span class="header-section-number">5.1</span> Absolute or normalized format?</h3>
<p>In <strong>absolute</strong> coordinate form, bounding box values are in the same format as the width and height dimensions (e.g.&nbsp;our image is <code>960x1280</code> pixels).</p>
<p>For example in <code>XYXY</code> format: <code>["bin", 8.9, 275.3, 867.5, 964.0]</code></p>
<p>An <code>(x1, y1)</code> (or <code>(x_min, y_min)</code>) coordinate of <code>(8.9, 275.3)</code> means the top left corner is <code>8.9</code> pixels in on the x-axis, and <code>275.3</code> pixels down on the y-axis.</p>
<p>In <strong>normalized</strong> coordinate form, values are between <code>[0, 1]</code> and are proportions of the image width and height.</p>
<p>For example in <code>XYXY</code> format: <code>["bin", 0.009, 0.215, 0.904, 0.753]</code></p>
<p>A normalized <code>(x1, y1)</code> (or <code>(x_min, y_min)</code>) coordinate of <code>(0.009, 0.215)</code> means the top left corner is <code>0.009 * image_width</code> pixels in on the x-axis and <code>0.215 * image_height</code> down on the y-axis.</p>
<p>To convert absolute coordinates to normalized, you can divide x-axis values by the image width and y-axis values by the image height.</p>
<p><span class="math display">\[
x_{\text{normalized}} = \frac{x_{\text{absolute}}}{\text{image\_width}} \quad y_{\text{normalized}} = \frac{y_{\text{absolute}}}{\text{image\_height}}
\]</span></p>
</section>
<section id="which-bounding-box-format-should-you-use" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="which-bounding-box-format-should-you-use"><span class="header-section-number">5.2</span> Which bounding box format should you use?</h3>
<p>The bounding box format you use will depend on the framework, model and existing data you’re trying to use.</p>
<p>For example, the take the following frameworks:</p>
<ul>
<li><strong>PyTorch</strong> - If you’re using PyTorch pre-trained models, for example, <a href="https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn"><code>torchvision.models.detection.fasterrcnn_resnet50_fpn</code></a>, you’ll want <strong>absolute</strong> <code>XYXY</code> (<code>[x1, y1, x2, y2]</code>) format.</li>
<li><strong>Hugging Face Transformers</strong> - If you’re using a Hugging Face Transformers model such as <a href="https://huggingface.co/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2Model">RT-DETRv2</a>, you’ll want to take note that <a href="https://huggingface.co/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2ForObjectDetection.forward">outputs from the model can be of one type</a> (e.g.&nbsp;<code>CXCYWH</code>) but they can be <a href="https://huggingface.co/docs/transformers/en/model_doc/rt_detr#transformers.RTDetrImageProcessor.post_process_object_detection">post-processed into another type</a> (e.g.&nbsp;<strong>absolute</strong> <code>XYXY</code>).</li>
<li><strong>Ultralytics YOLO</strong> - If you’re using a YOLO-like model such as <a href="https://docs.ultralytics.com/datasets/detect/#ultralytics-yolo-format">Ultralytics YOLO</a>, you’ll want <strong>normalized</strong> <code>CXCYWH</code> (<code>[center_x, center_y, width, height]</code>) format.</li>
<li><strong>Google Gemini</strong> - If you’re using <a href="https://ai.google.dev/gemini-api/docs/vision?lang=python#bbox">Google Gemini to predict bounding boxes on your images</a>, then you’ll want to pay attention to the special <code>[y_min, x_min, y_max, x_max]</code> (<code>YXYX</code>) normalized coordinates.</li>
</ul>
<p>Or if you note that someone has said their model is pre-trained on the COCO dataset, chances are the data has been formatted in <code>XYWH</code> format (see <a href="#tbl-bbox-formats" class="quarto-xref">Table&nbsp;1</a>).</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>For more on different bounding box formats and how to draw them, see <a href="https://www.learnml.io/posts/a-guide-to-bounding-box-formats/"><em>A Guide to Bounding Box Formats and How to Draw Them</em></a>.</p>
</div>
</div>
</section>
</section>
<section id="getting-an-object-detection-model" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="getting-an-object-detection-model"><span class="header-section-number">6</span> Getting an object detection model</h2>
<p>There are two main ways of getting an object detection model:</p>
<ol type="1">
<li><strong>Building it yourself.</strong> For example, constructing it layer by layer, testing it and training it on your target problem.</li>
<li><strong>Using an existing one.</strong> For example, find an existing model on a problem space similar to your own and then adapt it via <a href="https://en.wikipedia.org/wiki/Transfer_learning"><strong>transfer learning</strong></a> to your own task.</li>
</ol>
<p>In our case, we’re going to focus on the latter.</p>
<p>We’ll be taking a pre-trained object detection model and fine-tuning it on our Trashify 🚮 dataset so it outputs the boxes and labels we’re after.</p>
<section id="places-to-get-object-detection-models" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="places-to-get-object-detection-models"><span class="header-section-number">6.1</span> Places to get object detection models</h3>
<p>Instead of building your own machine learning model from scratch, it’s common practice to take an existing model that works on similar problem space to yours and then <strong>fine-tune</strong> it to your own use case.</p>
<p>There are several places to get object detection models:</p>
<div id="tbl-detection-models" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-detection-models-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Places to get pre-trained object detection models
</figcaption>
<div aria-describedby="tbl-detection-models-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th><strong>Location</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://huggingface.co/models?pipeline_tag=object-detection&amp;sort=trending">Hugging Face Hub</a></td>
<td>One the best places on the internet to find open-source machine learning models of nearly any kind. You can find pre-trained object detection models here such as <a href="https://huggingface.co/facebook/detr-resnet-50"><code>facebook/detr-resnet-50</code></a>, a model from Facebook (Meta) and <a href="https://huggingface.co/microsoft/conditional-detr-resnet-50"><code>microsoft/conditional-detr-resnet-50</code></a>, a model from Microsoft. And <a href="https://huggingface.co/docs/transformers/main/en/model_doc/rt_detr_v2">RT-DETRv2</a>, the model we’re going to use as our base model. Many of the models are permissively licensed, meaning you can use them for your own projects.</td>
</tr>
<tr class="even">
<td><a href="https://www.learnml.io/posts/apache-object-detection-models/">Apache 2.0 Object Detection Models</a></td>
<td>A list collected by myself of open-source, permissively licenced and high performing object detection models.</td>
</tr>
<tr class="odd">
<td><a href="https://pytorch.org/vision/stable/models.html"><code>torchvision</code></a></td>
<td>PyTorch’s built-in domain library for computer vision has several <a href="https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection">pre-trained object detection models</a> which you can use in your own workflows.</td>
</tr>
<tr class="even">
<td><a href="https://paperswithcode.com/task/object-detection">paperswithcode.com/task/object-detection</a></td>
<td>Whilst not a direct place to download object detection models from, paperswithcode contains benchmarks for many machine learning tasks (including object detection) which shows the current state of the art (best performing) models and usually includes links to where to get the code.</td>
</tr>
<tr class="odd">
<td><a href="https://github.com/facebookresearch/detectron2">Detectron2</a></td>
<td>Detectron2 is an open-source library to help with many of the tasks in detecting items in images. Inside you’ll find several pre-trained and adaptable models as well as utilities such as data loaders for object detection and segmentation tasks.</td>
</tr>
<tr class="even">
<td>YOLO Series</td>
<td>A running series of <a href="https://arxiv.org/abs/1506.02640">“You Only Look Once” models</a>. Usually, the higher the number, the better performing. For example, <a href="https://github.com/ultralytics/ultralytics"><code>YOLOv11</code></a> by Ultralytics should outperform <a href="https://github.com/THU-MIG/yolov10"><code>YOLOv10</code></a>, however, this often requires testing on your own dataset. Beware of the license, it is under the <a href="https://en.wikipedia.org/wiki/GNU_Affero_General_Public_License">AGPL-3.0 license</a> which may cause issues in some organizations.</td>
</tr>
<tr class="odd">
<td><a href="https://github.com/open-mmlab/mmdetection"><code>mmdetection</code> library</a></td>
<td>An open-source library from the OpenMMLab which contains many different open-source models as well as detection-specific utilties.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>When you find a pre-trained object detection model, you’ll often see statements such as:</p>
<blockquote class="blockquote">
<p><em>Conditional DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images).</em></p>
<p>Source: <a href="https://huggingface.co/microsoft/conditional-detr-resnet-50">https://huggingface.co/microsoft/conditional-detr-resnet-50</a></p>
</blockquote>
<p>This means the model has already been trained on the <a href="https://cocodataset.org/#home">COCO object detection dataset</a> which contains 118,000 images and <a href="https://cocodataset.org/#explore">80 classes</a> such as <code>["cake", "person", "skateboard"...]</code>.</p>
<p>This is a good thing.</p>
<p>It means that the model should have a fairly good starting point when we try to adapt it to our own project.</p>
</section>
<section id="downloading-our-model-from-hugging-face" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="downloading-our-model-from-hugging-face"><span class="header-section-number">6.2</span> Downloading our model from Hugging Face</h3>
<p>For our Trashify 🚮 project we’re going to be using the pre-trained object detection model <a href="https://huggingface.co/PekingU/rtdetr_v2_r50vd"><code>PekingU/rtdetr_v2_r50vd</code></a> which was originally introduced in the paper <a href="https://arxiv.org/abs/2407.17140"><em>RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer</em></a>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The term “DETR” stands for “DEtection TRansformer”.</p>
<p>Where “Transformer” refers to the <a href="https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)">Transformer neural network architecture</a>, specifically the <a href="https://en.wikipedia.org/wiki/Vision_transformer">Vision Transformer</a> (or ViT) rather than the Hugging Face <code>transformers</code> library (quite confusing, yes).</p>
<p>So DETR means “performing detection with the Transformer architecture”.</p>
<p>And the “RT” part stands for “Real Time” as in, the model is capable at performing predictions at over 30 FPS (a standard rate for video feeds).</p>
</div>
</div>
<p>To use this model, there are some helpful documentation resources we should be aware of:</p>
<div id="tbl-model-docs" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-model-docs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: Model documentation resources
</figcaption>
<div aria-describedby="tbl-model-docs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Resource</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><a href="https://huggingface.co/docs/transformers/main/en/model_doc/rt_detr_v2">RT-DETRv2 documentation</a></td>
<td style="text-align: left;">Contains detailed information on each of the <code>transformers.RTDetrV2</code> classes.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><a href="https://huggingface.co/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2Config"><code>transformers.RTDetrV2Config</code></a></td>
<td style="text-align: left;">Contains the configuration settings for our model such as number of layers and other hyperparameters.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a href="https://huggingface.co/docs/transformers/en/model_doc/rt_detr#transformers.RTDetrImageProcessor"><code>transformers.RTDetrImageProcessor</code></a></td>
<td style="text-align: left;">Contains several preprocessing on post processing functions and settings for data going into and out of our model. Here we can set values such as <code>size</code> in the <a href="https://huggingface.co/docs/transformers/en/model_doc/rt_detr#transformers.RTDetrImageProcessor.preprocess"><code>preprocess</code></a> method which will resize our images to a certain size. We can also use the <a href="https://huggingface.co/docs/transformers/en/model_doc/rt_detr#transformers.RTDetrImageProcessor.post_process_object_detection"><code>post_process_object_detection</code></a> method to process the raw outputs of our model into a more usable format. <strong>Note:</strong> Even though our model is RT-DETRv2, it uses the original RT-DETR processor.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><a href="https://huggingface.co/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2ForObjectDetection"><code>transformers.RTDetrV2ForObjectDetection</code></a></td>
<td style="text-align: left;">This will enable us to load the RT-DETRv2 model weights and enable to pass data through them via the <a href="https://huggingface.co/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2ForObjectDetection.forward"><code>forward</code></a> method.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><a href="https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoImageProcessor"><code>transformers.AutoImageProcessor</code></a></td>
<td style="text-align: left;">This will enable us to create an instance of <code>transformers.RTDetrImageProcessor</code> by passing the model name <code>PekingU/rtdetr_v2_r50vd</code> to the <code>from_pretrained</code> method. Hugging Face Transformers uses several <a href="https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForObjectDetection">Auto Classes</a> for various problem spaces and models.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><a href="https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForObjectDetection"><code>transformers.AutoModelForObjectDetection</code></a></td>
<td style="text-align: left;">Enables us to load the model architecture and weights for the RT-DETRv2 architecture by passing the model name <code>PekingU/rtdetr_v2_r50vd</code> to the <a href="https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForObjectDetection.from_pretrained"><code>from_pretrained</code> method</a>.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>We’ll get hands-on which each of these throughout the project.</p>
<p>For now, if you’d like to read up more on each, I’d highly recommend it.</p>
<p>Knowing how to navigate and read through a framework’s documentation is a very helpful skill to have.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>There are other object detection models we could try on the Hugging Face Hub such as <a href="https://huggingface.co/facebook/detr-resnet-50"><code>facebook/detr-resnet-50</code></a> or <a href="https://huggingface.co/IDEA-Research/dab-detr-resnet-50-dc5-pat3"><code>IDEA-Research/dab-detr-resnet-50-dc5-pat3</code></a>.</p>
<p>For now, we’ll stick with <code>PekingU/rtdetr_v2_r50vd</code>.</p>
<p>It’s easy to get stuck figuring out which model to use instead of just trying one and seeing how it goes.</p>
<p>Best to get something small working with one model and try another one later as part of a series of experiments to try and improve your results.</p>
<p>Spoiler: After trying several different object detection models on our problem, I found <code>PekingU/rtdetr_v2_r50vd</code> to be one of the best. Perhaps the newer <a href="https://huggingface.co/docs/transformers/main/en/model_doc/d_fine"><code>D-FINE</code></a> model might do better but I leave this for exploration.</p>
</div>
</div>
<p>We can load our model with <a href="https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForObjectDetection.from_pretrained"><code>transformers.AutoModelForObjectDetection.from_pretrained</code></a> and passing in the following parameters:</p>
<ul>
<li><code>pretrained_model_name_or_path</code> - Our target model, which can be a local path or Hugging Face model name (e.g.&nbsp;<code>PekingU/rtdetr_v2_r50vd</code>).</li>
<li><code>label2id</code> - A dictionary mapping our class names/labels to their numerical ID, this is so our model will know how many classes to output.</li>
<li><code>id2label</code> - A dictionary mapping numerical IDs to our class names/labels, so our model will know how many classes we’re working with and what their IDs are.</li>
<li><code>ignore_mismatched_sizes=True</code> (default) - We’ll set this to <code>True</code> so that our model can be instatiated with a varying number of classes compared to what it may have been trained on (e.g.&nbsp;if our model was trained on the 91 classes from COCO, we only need 7).</li>
</ul>
<p>See the <a href="https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForObjectDetection.from_pretrained">full documentation</a> for a full list of parameters we can use.</p>
<p>Let’s create a model!</p>
<div id="cell-44" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>, category<span class="op">=</span><span class="pp">UserWarning</span>, module<span class="op">=</span><span class="st">"torch.nn.modules.module"</span>) <span class="co"># turn off warnings for loading the model (feel free to comment this if you want to see the warnings)</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForObjectDetection</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>MODEL_NAME <span class="op">=</span> <span class="st">"PekingU/rtdetr_v2_r50vd"</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForObjectDetection.from_pretrained(</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>    pretrained_model_name_or_path<span class="op">=</span>MODEL_NAME,</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    label2id<span class="op">=</span>label2id,</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>    id2label<span class="op">=</span>id2label,</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Original model was trained with a different number of output classes to ours</span></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># So we'll ignore any mismatched sizes (e.g. 91 vs. 7)</span></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Try turning this to False and see what happens</span></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>    ignore_mismatched_sizes<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Uncomment to see full model architecture</span></span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a><span class="co"># model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Some weights of RTDetrV2ForObjectDetection were not initialized from the model checkpoint at PekingU/rtdetr_v2_r50vd and are newly initialized because the shapes did not match:
- model.decoder.class_embed.0.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.0.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.decoder.class_embed.1.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.1.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.decoder.class_embed.2.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.2.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.decoder.class_embed.3.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.3.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.decoder.class_embed.4.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.4.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.decoder.class_embed.5.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.5.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.denoising_class_embed.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([8, 256]) in the model instantiated
- model.enc_score_head.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.enc_score_head.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre>
</div>
</div>
<p>Beautiful!</p>
<p>We’ve got a model ready.</p>
<p>You might’ve noticed a warning about the model needing to be trained on a down-stream task:</p>
<blockquote class="blockquote">
<p>Some weights of RTDetrV2ForObjectDetection were not initialized from the model checkpoint at PekingU/rtdetr_v2_r50vd and are newly initialized because the shapes did not match: … - model.enc_score_head.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated - model.enc_score_head.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated</p>
<p>You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</p>
</blockquote>
<p>This is because our model has a different number of target classes (7 in total) comapred to the original model (91 in total, from the COCO dataset).</p>
<p>So in order to get this pretrained model to work on our dataset, we’ll need to <strong>fine-tune</strong> it.</p>
<p>You might also notice that if you set <code>ignore_mismatched_sizes=False</code>, you’ll get an error:</p>
<blockquote class="blockquote">
<p>RuntimeError: Error(s) in loading state_dict for Linear: size mismatch for bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([7]).</p>
</blockquote>
<p>This is a similar warning to the one above.</p>
<p>Keep this is mind for when you’re working with pretrained models.</p>
<p>If you are using data slightly different to what the model was trained on, you may need to alter the setup hyperparameters as well as fine-tune it on your own data.</p>
</section>
<section id="inspecting-our-models-layers" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="inspecting-our-models-layers"><span class="header-section-number">6.3</span> Inspecting our model’s layers</h3>
<p>We can inspect the full model architecture by running <code>print(model)</code> (I’ve commented this out for brevity).</p>
<p>And if you do so, you’ll see a large list of layers which combine to contribute to make the overall model.</p>
<p>The following subset of layers has been truncated for brevity.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Shortened version of the model architecture, print the full model to see all layers</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>RTDetrV2ForObjectDetection(</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>  (model): RTDetrV2Model(</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    (backbone): RTDetrV2ConvEncoder(</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>      (model): RTDetrResNetBackbone(</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>        (embedder): RTDetrResNetEmbeddings(</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>          (embedder): Sequential(</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>            (<span class="dv">0</span>): RTDetrResNetConvLayer(</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>              (convolution): Conv2d(<span class="dv">3</span>, <span class="dv">32</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>              (normalization): RTDetrV2FrozenBatchNorm2d()</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>              (activation): ReLU()</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>            ... [Many layers here] ...</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>  (class_embed): ModuleList(</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">0</span><span class="op">-</span><span class="dv">5</span>): <span class="dv">6</span> x Linear(in_features<span class="op">=</span><span class="dv">256</span>, out_features<span class="op">=</span><span class="dv">7</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>  (bbox_embed): ModuleList(</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">0</span><span class="op">-</span><span class="dv">5</span>): <span class="dv">6</span> x RTDetrV2MLPPredictionHead(</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>      (layers): ModuleList(</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>        (<span class="dv">0</span><span class="op">-</span><span class="dv">1</span>): <span class="dv">2</span> x Linear(in_features<span class="op">=</span><span class="dv">256</span>, out_features<span class="op">=</span><span class="dv">256</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>        (<span class="dv">2</span>): Linear(in_features<span class="op">=</span><span class="dv">256</span>, out_features<span class="op">=</span><span class="dv">4</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>      )</span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If we check out a few of our model’s layers, we can see that it is a combination of <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html">convolutional</a>, <a href="https://github.com/huggingface/transformers/blob/b311a3f50697c9602cc5d13a5faf7f6059c392ca/src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py#L225">attention</a>, <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">MLP (multi-layer perceptron)</a> and <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">linear</a> layers.</p>
<p>I’ll leave exploring each of these layer types for extra-curriculum, you can see the source code for the model in the <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py"><code>modeling_rt_detr_v2.py</code></a> file on the <code>transformers</code> GitHub.</p>
<p>For now, think of them as progressively pattern extractors.</p>
<p>We’ll feed our input image into our model and layer by layer it will manipulate the pixel values to try and extract patterns in a way so that its internal parameters matches the image to its input annotations.</p>
<p>More specifically, if we dive into the final two layer sections:</p>
<ol type="1">
<li><code>class_embed</code> = classification head with <code>out_features=7</code> (one for each of our labels, <code>'bin', 'hand', 'not_bin', 'not_hand', 'not_trash', 'trash', 'trash_arm']</code>).</li>
<li><code>bbox_embed</code> = regression head with <code>out_features=4</code> (one for each of our bbox coordinates, e.g.&nbsp;<code>[center_x, center_y, width, height]</code>).</li>
</ol>
<div id="cell-47" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Final classification layer: </span><span class="sc">{</span>model<span class="sc">.</span>class_embed<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Final box regression layer: </span><span class="sc">{</span>model<span class="sc">.</span>bbox_embed<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Final classification layer: ModuleList(
  (0-5): 6 x Linear(in_features=256, out_features=7, bias=True)
)

[INFO] Final box regression layer: ModuleList(
  (0-5): 6 x RTDetrV2MLPPredictionHead(
    (layers): ModuleList(
      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=4, bias=True)
    )
  )
)</code></pre>
</div>
</div>
<p>These two layers are what are going to output the final predictions of our model in structure similar to our annotations.</p>
<p>The <code>class_embed</code> will output the predicted class label of a given bounding box output from <code>bbox_predictor</code>.</p>
<p>In essence, we are trying to get all of the pretrained patterns (also called <strong>parameters</strong>/<strong>weights &amp; biases</strong>) of the previous layers to conform to the ideal outputs we’d like at the end.</p>
</section>
<section id="counting-the-number-of-parameters-in-our-model" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="counting-the-number-of-parameters-in-our-model"><span class="header-section-number">6.4</span> Counting the number of parameters in our model</h3>
<p><strong>Parameters</strong> are individual values which contribute to a model’s final output.</p>
<p>Parameters are also referred to as weights and biases.</p>
<p>You can think of these individual weights as small pushes and pulls on the input data to get it to match the input annotations.</p>
<p>If our weights were perfect, we could input an image and always get back the correct bounding boxes and class labels.</p>
<p>It’s very unlikely to ever have perfect weights (unless your dataset is very small) but we can make them quite good (and useful).</p>
<p>When you have a good set of weights, this is known as a good <a href="https://en.wikipedia.org/wiki/Feature_learning"><strong>representation</strong></a>.</p>
<p>Right now, our weights have been trained on COCO, a collection of 91 different common objects.</p>
<p>So they have a fairly good representation of detecting general common objects, however, we’d like to <strong>fine-tune</strong> these weights to detect our target objects.</p>
<p>Importantly, our model will not be starting from scratch when it begins to train.</p>
<p>It will instead take off from its existing knowledge of detecting common objects in images and try to adhere to our task.</p>
<p>When it comes to parameters and weights, generally, more is better.</p>
<p>Meaning the more parameters your model has, the better representation it can learn.</p>
<p>For example, <a href="https://huggingface.co/microsoft/resnet-50">ResNet50</a> (our computer vision backbone) has ~25 million parameters, about 100 MB in <code>float32</code> precision or 50MB in <code>float16</code> precision.</p>
<p>Whereas a model such as <a href="https://huggingface.co/meta-llama/Llama-3.1-405B">Llama-3.1-405B</a> has ~405 billion parameters, about 1.45 TB in <code>float32</code> precision or 740 GB in <code>float16</code> precision, about 16,000x more than ResNet50.</p>
<p>However, as we can see having more parameters comes with the tradeoff of size and latency.</p>
<p>For each new parameter requires to be stored and it also adds an extra computation unit to your model.</p>
<p>In the case of Trashify, since we’d like our model to run on-device (e.g.&nbsp;make predictions live on an iPhone), we’d opt for the smallest number of parameters we could get acceptable results from.</p>
<p>If performance is your number 1 criteria and size and latency don’t matter, then you’d likely opt for the model with the largest number of parameters (though always evaluate these models on your own data, larger models are <em>generally</em> better, not <em>always</em> better).</p>
<p>Since our model is built using PyTorch, let’s write a small function to count the number of:</p>
<ul>
<li>Trainable parameters (parameters which will be tweaked during training)</li>
<li>Non-trainable parameters (parameters which will <em>not</em> be tweaked during training)</li>
<li>Total parameters (trainable parameters + non-trainable parameters)</li>
</ul>
<div id="cell-50" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Count the number of parameters in the model</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> count_parameters(model):</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Takes in a PyTorch model and returns the number of parameters."""</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    trainable_parameters <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    non_trainable_parameters <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> <span class="kw">not</span> p.requires_grad)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    total_parameters <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Total parameters: </span><span class="sc">{</span>total_parameters<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Trainable parameters (will be updated): </span><span class="sc">{</span>trainable_parameters<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Non-trainable parameters (will not be updated): </span><span class="sc">{</span>non_trainable_parameters<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>count_parameters(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Total parameters: 42,741,357
Trainable parameters (will be updated): 42,741,357
Non-trainable parameters (will not be updated): 0</code></pre>
</div>
</div>
<p>Cool!</p>
<p>It looks like our model has a total of <code>43,396,813</code> parameters, of which, most of them are trainable.</p>
<p>This means that when we fine-tune our model later on, we’ll be tweaking the majority of the parameters to try and represent our data.</p>
<p>In practice, this is known as <strong>full fine-tuning</strong>, trying to fine-tune a large portion of the model to our data.</p>
<p>There are other methods for fine-tuning, such as <strong>feature extraction</strong> (where you only fine-tune the final layers of the model) and <strong>partial fine-tuning</strong> (where you fine-tune a portion of the model).</p>
<p>And even methods such as <a href="https://huggingface.co/papers/2106.09685"><strong>LoRA</strong> (Low-Rank Adaptation)</a> which fine-tunes an adaptor matrix as a compliment to the model’s parameters.</p>
</section>
<section id="creating-a-function-to-build-our-model" class="level3" data-number="6.5">
<h3 data-number="6.5" class="anchored" data-anchor-id="creating-a-function-to-build-our-model"><span class="header-section-number">6.5</span> Creating a function to build our model</h3>
<p>Since machine learning is very experimental, we may want to create multiple instances of our <code>model</code> to test various things.</p>
<p>So let’s functionize the creation of a new model with parameters for our target model name, <code>id2label</code> and <code>label2id</code> dictionaries.</p>
<div id="cell-53" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForObjectDetection</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup the model</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_model(pretrained_model_name_or_path: <span class="bu">str</span> <span class="op">=</span> MODEL_NAME, </span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>                 label2id: <span class="bu">dict</span> <span class="op">=</span> label2id, </span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>                 id2label: <span class="bu">dict</span> <span class="op">=</span> id2label):</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Creates and returns an instance of AutoModelForObjectDetection.</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Args: </span></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="co">        pretrained_model_name_or_path (str): The name or path of the pretrained model to load. </span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="co">            Defaults to MODEL_NAME.</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a><span class="co">        label2id (dict): A dictionary mapping class labels to IDs. Defaults to label2id.</span></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a><span class="co">        id2label (dict): A dictionary mapping class IDs to labels. Defaults to id2label.</span></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a><span class="co">        AutoModelForObjectDetection: A pretrained model for object detection with number of output</span></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a><span class="co">            classes equivalent to len(label2id).</span></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> AutoModelForObjectDetection.from_pretrained(</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>        pretrained_model_name_or_path<span class="op">=</span>pretrained_model_name_or_path,</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>        label2id<span class="op">=</span>label2id,</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>        id2label<span class="op">=</span>id2label,</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>        ignore_mismatched_sizes<span class="op">=</span><span class="va">True</span>, <span class="co"># default</span></span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Perfect!</p>
<p>And to make sure our function works…</p>
<div id="cell-55" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a new model instance</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_model()</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="co"># model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Some weights of RTDetrV2ForObjectDetection were not initialized from the model checkpoint at PekingU/rtdetr_v2_r50vd and are newly initialized because the shapes did not match:
- model.decoder.class_embed.0.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.0.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.decoder.class_embed.1.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.1.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.decoder.class_embed.2.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.2.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.decoder.class_embed.3.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.3.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.decoder.class_embed.4.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.4.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.decoder.class_embed.5.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.5.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.denoising_class_embed.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([8, 256]) in the model instantiated
- model.enc_score_head.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.enc_score_head.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre>
</div>
</div>
</section>
<section id="trying-to-pass-a-single-sample-through-our-model-part-1" class="level3" data-number="6.6">
<h3 data-number="6.6" class="anchored" data-anchor-id="trying-to-pass-a-single-sample-through-our-model-part-1"><span class="header-section-number">6.6</span> Trying to pass a single sample through our model (part 1)</h3>
<p>Okay, now we’ve got a model, let’s put some data through it!</p>
<p>When we call our <code>model</code>, because it’s a PyTorch Module (<a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module"><code>torch.nn.Module</code></a>) it will by default run the <code>forward</code> method.</p>
<p>In PyTorch, <code>forward</code> overrides the special <code>__call__</code> method on functions.</p>
<p>So we can pass data into our model by running:</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>model(input_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Which is equivalent to running:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>model.forward(input_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>To see what happens when we call our model, let’s inspect the <code>forward</code> method’s docstring with <a href="https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrModel.forward"><code>model.forward?</code></a>.</p>
<div id="cell-57" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># What happens when we call our model?</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: for PyTorch modules, `forward` overrides the __call__ method, </span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="co"># so calling the model is equivalent to calling the forward method.</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>model.forward?</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<details>
<summary>
Output of model.forward?
</summary>
<pre><code>Signature:
model.forward(
    pixel_values: torch.FloatTensor,
    pixel_mask: Optional[torch.LongTensor] = None,
    encoder_outputs: Optional[torch.FloatTensor] = None,
    inputs_embeds: Optional[torch.FloatTensor] = None,
    decoder_inputs_embeds: Optional[torch.FloatTensor] = None,
    labels: Optional[List[dict]] = None,
    output_attentions: Optional[bool] = None,
    output_hidden_states: Optional[bool] = None,
    return_dict: Optional[bool] = None,
    **loss_kwargs,
) -&gt; Union[Tuple[torch.FloatTensor], transformers.models.rt_detr_v2.modeling_rt_detr_v2.RTDetrV2ObjectDetectionOutput]
Docstring:
The [`RTDetrV2ForObjectDetection`] forward method, overrides the `__call__` special method.

&lt;Tip&gt;

Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.

&lt;/Tip&gt;

Args:
    pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):
        The tensors corresponding to the input images. Pixel values can be obtained using
        [`{image_processor_class}`]. See [`{image_processor_class}.__call__`] for details ([`{processor_class}`] uses
        [`{image_processor_class}`] for processing images).
    pixel_mask (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):
        Mask to avoid performing attention on padding pixel values. Mask values selected in `[0, 1]`:

        - 1 for pixels that are real (i.e. **not masked**),
        - 0 for pixels that are padding (i.e. **masked**).

        [What are attention masks?](../glossary#attention-mask)
    encoder_outputs (`torch.FloatTensor`, *optional*):
        Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
        `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of
        hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.
    inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
        Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you
        can choose to directly pass a flattened representation of an image.
    decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):
        Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an
        embedded representation.
    labels (`List[dict]` of len `(batch_size,)`, *optional*):
        Labels for computing the bipartite matching loss. List of dicts, each dictionary containing at least the
        following 2 keys: 'class_labels' and 'boxes' (the class labels and bounding boxes of an image in the batch
        respectively). The class labels themselves should be a `torch.LongTensor` of len `(number of bounding boxes
        in the image,)` and the boxes a `torch.FloatTensor` of shape `(number of bounding boxes in the image, 4)`.
    output_attentions (`bool`, *optional*):
        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
        tensors for more detail.
    output_hidden_states (`bool`, *optional*):
        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
        more detail.
    return_dict (`bool`, *optional*):
        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.

Returns:
    [`transformers.models.rt_detr_v2.modeling_rt_detr_v2.RTDetrV2ObjectDetectionOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.models.rt_detr_v2.modeling_rt_detr_v2.RTDetrV2ObjectDetectionOutput`] or a tuple of
    `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various
    elements depending on the configuration ([`RTDetrV2Config`]) and inputs.

    - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)) -- Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a
      bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized
      scale-invariant IoU loss.
    - **loss_dict** (`Dict`, *optional*) -- A dictionary containing the individual losses. Useful for logging.
    - **logits** (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`) -- Classification logits (including no-object) for all queries.
    - **pred_boxes** (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`) -- Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These
      values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding
      possible padding). You can use [`~RTDetrV2ImageProcessor.post_process_object_detection`] to retrieve the
      unnormalized (absolute) bounding boxes.
    - **auxiliary_outputs** (`list[Dict]`, *optional*) -- Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)
      and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and
      `pred_boxes`) for each decoder layer.
    - **last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`) -- Sequence of hidden-states at the output of the last layer of the decoder of the model.
    - **intermediate_hidden_states** (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`) -- Stacked intermediate hidden states (output of each layer of the decoder).
    - **intermediate_logits** (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, config.num_labels)`) -- Stacked intermediate logits (logits of each layer of the decoder).
    - **intermediate_reference_points** (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`) -- Stacked intermediate reference points (reference points of each layer of the decoder).
    - **intermediate_predicted_corners** (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`) -- Stacked intermediate predicted corners (predicted corners of each layer of the decoder).
    - **initial_reference_points** (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`) -- Stacked initial reference points (initial reference points of each layer of the decoder).
    - **decoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of
      shape `(batch_size, num_queries, hidden_size)`. Hidden-states of the decoder at the output of each layer
      plus the initial embedding outputs.
    - **decoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, num_queries,
      num_queries)`. Attentions weights of the decoder, after the attention softmax, used to compute the weighted
      average in the self-attention heads.
    - **cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.
      Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the
      weighted average in the cross-attention heads.
    - **encoder_last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) -- Sequence of hidden-states at the output of the last layer of the encoder of the model.
    - **encoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of
      shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each
      layer plus the initial embedding outputs.
    - **encoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.
      Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
      self-attention heads.
    - **init_reference_points** (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`) -- Initial reference points sent through the Transformer decoder.
    - **enc_topk_logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`) -- Logits of predicted bounding boxes coordinates in the encoder.
    - **enc_topk_bboxes** (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`) -- Logits of predicted bounding boxes coordinates in the encoder.
    - **enc_outputs_class** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`) -- Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are
      picked as region proposals in the first stage. Output of bounding box binary classification (i.e.
      foreground and background).
    - **enc_outputs_coord_logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`) -- Logits of predicted bounding boxes coordinates in the first stage.
    - **denoising_meta_values** (`dict`) -- Extra dictionary for the denoising related values

Examples:

```python
&gt;&gt;&gt; from transformers import RTDetrV2ImageProcessor, RTDetrV2ForObjectDetection
&gt;&gt;&gt; from PIL import Image
&gt;&gt;&gt; import requests
&gt;&gt;&gt; import torch

&gt;&gt;&gt; url = "http://images.cocodataset.org/val2017/000000039769.jpg"
&gt;&gt;&gt; image = Image.open(requests.get(url, stream=True).raw)

&gt;&gt;&gt; image_processor = RTDetrV2ImageProcessor.from_pretrained("PekingU/RTDetrV2_r50vd")
&gt;&gt;&gt; model = RTDetrV2ForObjectDetection.from_pretrained("PekingU/RTDetrV2_r50vd")

&gt;&gt;&gt; # prepare image for the model
&gt;&gt;&gt; inputs = image_processor(images=image, return_tensors="pt")

&gt;&gt;&gt; # forward pass
&gt;&gt;&gt; outputs = model(**inputs)

&gt;&gt;&gt; logits = outputs.logits
&gt;&gt;&gt; list(logits.shape)
[1, 300, 80]

&gt;&gt;&gt; boxes = outputs.pred_boxes
&gt;&gt;&gt; list(boxes.shape)
[1, 300, 4]

&gt;&gt;&gt; # convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)
&gt;&gt;&gt; target_sizes = torch.tensor([image.size[::-1]])
&gt;&gt;&gt; results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[
...     0
... ]

&gt;&gt;&gt; for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
...     box = [round(i, 2) for i in box.tolist()]
...     print(
...         f"Detected {model.config.id2label[label.item()]} with confidence "
...         f"{round(score.item(), 3)} at location {box}"
...     )
Detected sofa with confidence 0.97 at location [0.14, 0.38, 640.13, 476.21]
Detected cat with confidence 0.96 at location [343.38, 24.28, 640.14, 371.5]
Detected cat with confidence 0.958 at location [13.23, 54.18, 318.98, 472.22]
Detected remote with confidence 0.951 at location [40.11, 73.44, 175.96, 118.48]
Detected remote with confidence 0.924 at location [333.73, 76.58, 369.97, 186.99]</code></pre>
<p>File: ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py Type: method</p>
<pre><code>
&lt;/details&gt;

Running `model.forward?` we can see that our model wants to take in `pixel_values` as well as a `pixel_mask` as arguments.

What happens if we try to pass in a single image from our `random_sample`?

Let's try!

::: {.callout-note }

It's good practice to try and pass a single sample through your model as soon as possible to see what happens.

If we're lucky, it'll work.

If we're *really* lucky, we'll get an error message saying why it *didn't* work (this is usually the case because rarely does raw data flow through a model without being preprocessed first).

:::

We'll do so by setting `pixel_values` to our `random_sample["image"]` and `pixel_mask=None`.

::: {#cell-60 .cell execution_count=20}
``` {.python .cell-code}
# Do a single forward pass with the model
random_sample_outputs = model(pixel_values=random_sample["image"],
                              pixel_mask=None)
random_sample_outputs</code></pre>
<p>:::</p>
<details>
<summary>
Output of random_sample_outputs
</summary>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[21], line 3
      1 #| output: false
      2 # Do a single forward pass with the model
----&gt; 3 random_sample_outputs = model(pixel_values=random_sample["image"],
      4                               pixel_mask=None)
      5 random_sample_outputs

File ~/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:1751, in Module._wrapped_call_impl(self, *args, **kwargs)
   1749     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1750 else:
-&gt; 1751     return self._call_impl(*args, **kwargs)

File ~/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:1762, in Module._call_impl(self, *args, **kwargs)
   1757 # If we don't have any hooks, we want to skip the rest of the logic in
   1758 # this function, and just call forward.
   1759 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1760         or _global_backward_pre_hooks or _global_backward_hooks
   1761         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1762     return forward_call(*args, **kwargs)
   1764 result = None
   1765 called_always_called_hooks = set()

File ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py:1967, in RTDetrV2ForObjectDetection.forward(self, pixel_values, pixel_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **loss_kwargs)
   1962 output_hidden_states = (
   1963     output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
   1964 )
   1965 return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-&gt; 1967 outputs = self.model(
   1968     pixel_values,
   1969     pixel_mask=pixel_mask,
   1970     encoder_outputs=encoder_outputs,
   1971     inputs_embeds=inputs_embeds,
   1972     decoder_inputs_embeds=decoder_inputs_embeds,
   1973     labels=labels,
   1974     output_attentions=output_attentions,
   1975     output_hidden_states=output_hidden_states,
   1976     return_dict=return_dict,
   1977 )
   1979 denoising_meta_values = (
   1980     outputs.denoising_meta_values if return_dict else outputs[-1] if self.training else None
   1981 )
   1983 outputs_class = outputs.intermediate_logits if return_dict else outputs[2]

File ~/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:1751, in Module._wrapped_call_impl(self, *args, **kwargs)
   1749     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
   1750 else:
-&gt; 1751     return self._call_impl(*args, **kwargs)

File ~/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:1762, in Module._call_impl(self, *args, **kwargs)
   1757 # If we don't have any hooks, we want to skip the rest of the logic in
   1758 # this function, and just call forward.
   1759 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1760         or _global_backward_pre_hooks or _global_backward_hooks
   1761         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1762     return forward_call(*args, **kwargs)
   1764 result = None
   1765 called_always_called_hooks = set()

File ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py:1658, in RTDetrV2Model.forward(self, pixel_values, pixel_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)
   1653 output_hidden_states = (
   1654     output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
   1655 )
   1656 return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-&gt; 1658 batch_size, num_channels, height, width = pixel_values.shape
   1659 device = pixel_values.device
   1661 if pixel_mask is None:

AttributeError: 'Image' object has no attribute 'shape'</code></pre>
</details>
<p>Oh no!… I mean… Oh, yes!</p>
<p>We get an error:</p>
<blockquote class="blockquote">
<p>AttributeError: ‘Image’ object has no attribute ‘shape’</p>
</blockquote>
<p>Hmmm… it seems we’ve tried to pass a <code>PIL.Image</code> to our model rather than a <code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>It looks like our input data might require some preprocessing before we can pass it to our model.</p>
</details></section>
</section>
<section id="aside-processor-to-model-pattern" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="aside-processor-to-model-pattern"><span class="header-section-number">7</span> Aside: Processor to Model Pattern</h2>
<p>Many Hugging Face data loading and modelling workflows as well as machine learning workflows in general follow the pattern of:</p>
<ul>
<li>Data -&gt; Preprocessor -&gt; Model</li>
</ul>
<figure style="text-align: center;" class="figure">
<img src="https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-trashify-object-detection/04-data-preprocess-model.png" alt="A diagram illustrating a three-stage object detection pipeline: the 'Data' stage shows an image of a hand discarding trash into a red-lidded bin, with bounding boxes labeling 'bin' (green), 'trash' (blue), and 'hand' (purple), alongside its corresponding JSON-like annotation data; an arrow points to the 'Preprocess' stage, represented by a green rounded rectangle labeled 'RTDetrImageProcessor'; another arrow leads to the 'Model' stage, depicted by a stylized neural network icon and labeled 'RTDetrV2ForObjectDetection'" style="width: 100%; max-width: 900px; height: auto;" class="figure-img">
<figcaption>
Workflow we’ll follow to create our own custom object detection model. We’ll start with images labelled with boxes of trash, bin and hand (and other classes), preprocess the process to be ready for use with a model and then we’ll train the model on our preprocessed custom data.
</figcaption>
</figure>
<p>Meaning, the raw input data gets preprocessed or transformed in some way before being passed to a model.</p>
<p>Preprocessors and models are often loaded with an <a href="https://huggingface.co/docs/transformers/en/model_doc/auto">Auto Class</a>.</p>
<p>An Auto Class pairs a preprocessor and model based on their model name or key.</p>
<p>For example:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoProcessor, AutoModel</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load raw data</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>raw_data <span class="op">=</span> load_data()</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Define target model name</span></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>MODEL_NAME <span class="op">=</span> <span class="st">"..."</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Load preprocessor and model (these two are often paired)</span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>preprocessor <span class="op">=</span> AutoProcessor.from_pretrained(MODEL_NAME)</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModel.from_pretrained(MODEL_NAME)</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocess data</span></span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>preprocessed_data <span class="op">=</span> preprocessor.preprocess(raw_data)</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Pass preprocessed data to model</span></span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> model(preprocessed_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is the same for our Trashify 🚮 project.</p>
<p>We’ve got our raw data (images and bounding boxes), however, they need to be preprocessed in order for our model to be able to handle them.</p>
<p>Previously we tried to pass a sample of raw data to our model and this errored.</p>
<p>We can fix this by first preprocessing our raw data with our model’s pair preprocessor and <em>then</em> passing to our model again.</p>
</section>
<section id="loading-our-models-processor" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="loading-our-models-processor"><span class="header-section-number">8</span> Loading our model’s processor</h2>
<p>Time to get our raw data ready for our model!</p>
<p>To begin, let’s load our model’s processor.</p>
<p>We’ll use this to prepare our input images for the model.</p>
<p>To do so, we’ll use <a href="https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoImageProcessor"><code>transformers.AutoImageProcessor</code></a> and pass our target model name to the <code>from_pretrained</code> method.</p>
<p>We can set <code>use_fast=True</code> so the fast version of the processor is loaded (see more in the docs under <a href="https://huggingface.co/docs/transformers/en/model_doc/rt_detr#transformers.RTDetrImageProcessorFast"><code>transformers.RTDetrImageProcessorFast</code></a>).</p>
<div id="cell-64" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoImageProcessor</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>MODEL_NAME <span class="op">=</span> <span class="st">"PekingU/rtdetr_v2_r50vd"</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="co"># MODEL_NAME = "facebook/detr-resnet-50" # Could also use this model as an another experiment</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the image processor</span></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>image_processor <span class="op">=</span> AutoImageProcessor.from_pretrained(pretrained_model_name_or_path<span class="op">=</span>MODEL_NAME,</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>                                                     use_fast<span class="op">=</span><span class="va">True</span>) <span class="co"># load the fast version of the processor</span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Check out the image processor</span></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>image_processor</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>RTDetrImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": false,
  "device": null,
  "do_center_crop": null,
  "do_convert_annotations": true,
  "do_convert_rgb": null,
  "do_normalize": false,
  "do_pad": false,
  "do_rescale": true,
  "do_resize": true,
  "format": "coco_detection",
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "RTDetrImageProcessorFast",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "input_data_format": null,
  "pad_size": null,
  "resample": 2,
  "rescale_factor": 0.00392156862745098,
  "return_segmentation_masks": null,
  "return_tensors": null,
  "size": {
    "height": 640,
    "width": 640
  }
}</code></pre>
</div>
</div>
<p>Ok, a few things going on here.</p>
<p>These parameters will transform our input images before we pass them to our model.</p>
<p>One of the first things to see is the <code>image_processor</code> is expecting our bounding boxes to be in <a href="https://cocodataset.org/#format-data">COCO (or <code>coco_detection</code>) format</a> (this is the default).</p>
<p>We’ll see what this looks like later on but our processor wants this format because that’s the format our model has been trained on (it’s generally best practice to input data to a model in the same way its been trained on, otherwise you might get poor results).</p>
<p>Another thing to notice is that our input images will be resized to the values of the <code>size</code> parameter.</p>
<p>In our case, it’s currently:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co">"size"</span>: {</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"height"</span>: <span class="dv">640</span>,</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"width"</span>: <span class="dv">640</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Which means that input image will get resized to be <code>640x640</code> pixels (height x width).</p>
<p>We’ll keep these dimensions but we’ll update it to use <code>"longest_edge": 640</code> and <code>"shortest_edge: 640"</code> (this will maintain aspect ratio).</p>
<p>You can read more about what each of these does in the <a href="https://huggingface.co/docs/transformers/en/model_doc/rt_detr#transformers.RTDetrImageProcessor"><code>transformers.RTDetrImageProcessor</code> documentation</a>.</p>
<p>Let’s update our instance of <code>transformers.RTDetrImageProcessor</code> with a few custom parameters:</p>
<ul>
<li><code>do_convert_annotations=True</code> - This is the default and it will convert our boxes to the format <code>CXCYWH</code> or <code>(center_x, center_y, width, height)</code> (see <a href="#tbl-bbox-formats" class="quarto-xref">Table&nbsp;1</a>) in the range <code>[0, 1]</code>.</li>
<li><code>size</code> - We’ll update the <code>size</code> dictionary so all of our images have <code>"longest_edge": 640</code> and <code>"shortest_edge: 640"</code>. We’ll use a value of <code>640</code> which is a common size in world of object detection. But there are also other sizes such as <code>300x300</code>, <code>480x480</code>, <code>512x512</code>, <code>800x800</code> and more.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Depending on what task you’re working on, you might want to tweak the image resolution you’re working with.</p>
<p>For example, I like this quote from <a href="https://lucasb.eyer.be/articles/vit_cnn_speed.html">Lucas Beyer</a>, a former research scientist at DeepMind and engineer at OpenAI:</p>
<blockquote class="blockquote">
<p>My conservative claim is that you can always stretch to a square, and for:</p>
<p>natural images, meaning most photos, 224px² is enough; text in photos, phone screens, diagrams and charts, 448px² is enough; desktop screens and single-page documents, 896px² is enough.</p>
</blockquote>
<p>Typically, in the case of object detection, you’ll want to use a higher value.</p>
<p>But this is another thing that is open to experimentation.</p>
</div>
</div>
<div id="cell-66" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set image size</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>IMAGE_SIZE <span class="op">=</span> <span class="dv">640</span> <span class="co"># we could try other sizes here: 300x300, 480x480, 512x512, 640x640, 800x800 (best to experiment and see which works best)</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a new instance of the image processor with the desired image size</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>image_processor <span class="op">=</span> AutoImageProcessor.from_pretrained(</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>    pretrained_model_name_or_path<span class="op">=</span>MODEL_NAME,</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>    use_fast<span class="op">=</span><span class="va">True</span>, <span class="co"># use the fast preprocessor</span></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">format</span><span class="op">=</span><span class="st">"coco_detection"</span>, <span class="co"># this is the default</span></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>    do_convert_annotations<span class="op">=</span><span class="va">True</span>, <span class="co"># defaults to True, converts boxes to (center_x, center_y, width, height) in range [0, 1]</span></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>    size<span class="op">=</span>{<span class="st">"shortest_edge"</span>: IMAGE_SIZE, </span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>          <span class="st">"longest_edge"</span>: IMAGE_SIZE},</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>    return_segmentation_masks<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>    do_pad<span class="op">=</span><span class="va">True</span> <span class="co"># make sure all images have 640x640 size thanks to padding</span></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Optional: View the docstring of our image_processor.preprocess function</span></span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a><span class="co"># image_processor.preprocess?</span></span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Check out our new image processor size</span></span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>image_processor.size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>{'shortest_edge': 640, 'longest_edge': 640}</code></pre>
</div>
</div>
<p>Beautiful!</p>
<p>Now our images will be resized to a square of size <code>640x640</code> when we pass them to our model.</p>
<p>How about we try to preprocess our <code>random_sample</code>?</p>
<p>To do so, we can pass its <code>"image"</code> key and <code>"annotations"</code> key to our <code>image_processor</code>’s <a href="https://huggingface.co/docs/transformers/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.preprocess"><code>preprocess</code></a> method (we can also just called <code>image_processor</code> directly as it will call <code>preprocess</code> via the <code>__call__</code> method).</p>
<p>Let’s try!</p>
<div id="cell-68" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>image_processor</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>RTDetrImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": false,
  "device": null,
  "do_center_crop": null,
  "do_convert_annotations": true,
  "do_convert_rgb": null,
  "do_normalize": false,
  "do_pad": true,
  "do_rescale": true,
  "do_resize": true,
  "format": "coco_detection",
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "RTDetrImageProcessorFast",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "input_data_format": null,
  "pad_size": null,
  "resample": 2,
  "rescale_factor": 0.00392156862745098,
  "return_segmentation_masks": true,
  "return_tensors": null,
  "size": {
    "longest_edge": 640,
    "shortest_edge": 640
  }
}</code></pre>
</div>
</div>
<div id="cell-69" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Try to process a single image and annotation pair (spoiler: this will error)</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>random_sample_preprocessed <span class="op">=</span> image_processor.preprocess(images<span class="op">=</span>random_sample[<span class="st">"image"</span>],</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>                                                        annotations<span class="op">=</span>random_sample[<span class="st">"annotations"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[24], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Try to process a single image and annotation pair (spoiler: this will error)</span>
<span class="ansi-green-fg">----&gt; 2</span> random_sample_preprocessed <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">image_processor</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">preprocess</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">images</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">random_sample</span><span class="ansi-yellow-bg">[</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">"</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">image</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">"</span><span class="ansi-yellow-bg">]</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span class="ansi-yellow-bg">                                                        </span><span class="ansi-yellow-bg">annotations</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">random_sample</span><span class="ansi-yellow-bg">[</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">"</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">annotations</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">"</span><span class="ansi-yellow-bg">]</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/rt_detr/image_processing_rt_detr_fast.py:403</span>, in <span class="ansi-cyan-fg">RTDetrImageProcessorFast.preprocess</span><span class="ansi-blue-fg">(self, images, annotations, masks_path, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">    380</span> <span style="color:rgb(175,0,255)">@auto_docstring</span>
<span class="ansi-green-fg ansi-bold">    381</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span> <span style="color:rgb(0,0,255)">preprocess</span>(
<span class="ansi-green-fg ansi-bold">    382</span>     <span style="color:rgb(0,135,0)">self</span>,
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">    386</span>     <span style="color:rgb(98,98,98)">*</span><span style="color:rgb(98,98,98)">*</span>kwargs: Unpack[RTDetrFastImageProcessorKwargs],
<span class="ansi-green-fg ansi-bold">    387</span> ) <span style="color:rgb(98,98,98)">-</span><span style="color:rgb(98,98,98)">&gt;</span> BatchFeature:
<span class="ansi-green-fg ansi-bold">    388</span> <span style="color:rgb(188,188,188)">    </span><span style="color:rgb(175,0,0)">r</span><span style="font-style:italic;color:rgb(175,0,0)">"""</span>
<span class="ansi-green-fg ansi-bold">    389</span> <span style="font-style:italic;color:rgb(175,0,0)">    annotations (`AnnotationType` or `List[AnnotationType]`, *optional*):</span>
<span class="ansi-green-fg ansi-bold">    390</span> <span style="font-style:italic;color:rgb(175,0,0)">        List of annotations associated with the image or batch of images. If annotation is for object</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">    401</span> <span style="font-style:italic;color:rgb(175,0,0)">        Path to the directory containing the segmentation masks.</span>
<span class="ansi-green-fg ansi-bold">    402</span> <span style="font-style:italic;color:rgb(175,0,0)">    """</span>
<span class="ansi-green-fg">--&gt; 403</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">super</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">)</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">preprocess</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">images</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">annotations</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">masks_path</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/image_processing_utils_fast.py:609</span>, in <span class="ansi-cyan-fg">BaseImageProcessorFast.preprocess</span><span class="ansi-blue-fg">(self, images, *args, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">    606</span> kwargs<span style="color:rgb(98,98,98)">.</span>pop(<span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">default_to_square</span><span style="color:rgb(175,0,0)">"</span>)
<span class="ansi-green-fg ansi-bold">    607</span> kwargs<span style="color:rgb(98,98,98)">.</span>pop(<span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">data_format</span><span style="color:rgb(175,0,0)">"</span>)
<span class="ansi-green-fg">--&gt; 609</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_preprocess</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">images</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/rt_detr/image_processing_rt_detr_fast.py:440</span>, in <span class="ansi-cyan-fg">RTDetrImageProcessorFast._preprocess</span><span class="ansi-blue-fg">(self, images, annotations, masks_path, return_segmentation_masks, do_resize, size, interpolation, do_rescale, rescale_factor, do_normalize, do_convert_annotations, image_mean, image_std, do_pad, pad_size, format, return_tensors, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">    438</span> <span style="color:rgb(0,135,0)">format</span> <span style="color:rgb(98,98,98)">=</span> AnnotationFormat(<span style="color:rgb(0,135,0)">format</span>)
<span class="ansi-green-fg ansi-bold">    439</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> annotations <span style="font-weight:bold;color:rgb(175,0,255)">is</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>:
<span class="ansi-green-fg">--&gt; 440</span>     <span class="ansi-yellow-bg">validate_annotations</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">format</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">SUPPORTED_ANNOTATION_FORMATS</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">annotations</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    442</span> data <span style="color:rgb(98,98,98)">=</span> {}
<span class="ansi-green-fg ansi-bold">    443</span> processed_images <span style="color:rgb(98,98,98)">=</span> []

File <span class="ansi-green-fg">~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/image_utils.py:919</span>, in <span class="ansi-cyan-fg">validate_annotations</span><span class="ansi-blue-fg">(annotation_format, supported_annotation_formats, annotations)</span>
<span class="ansi-green-fg ansi-bold">    917</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> annotation_format <span style="font-weight:bold;color:rgb(175,0,255)">is</span> AnnotationFormat<span style="color:rgb(98,98,98)">.</span>COCO_DETECTION:
<span class="ansi-green-fg ansi-bold">    918</span>     <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> valid_coco_detection_annotations(annotations):
<span class="ansi-green-fg">--&gt; 919</span>         <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">ValueError</span>(
<span class="ansi-green-fg ansi-bold">    920</span>             <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts </span><span style="color:rgb(175,0,0)">"</span>
<span class="ansi-green-fg ansi-bold">    921</span>             <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">(batch of images) with the following keys: `image_id` and `annotations`, with the latter </span><span style="color:rgb(175,0,0)">"</span>
<span class="ansi-green-fg ansi-bold">    922</span>             <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">being a list of annotations in the COCO format.</span><span style="color:rgb(175,0,0)">"</span>
<span class="ansi-green-fg ansi-bold">    923</span>         )
<span class="ansi-green-fg ansi-bold">    925</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> annotation_format <span style="font-weight:bold;color:rgb(175,0,255)">is</span> AnnotationFormat<span style="color:rgb(98,98,98)">.</span>COCO_PANOPTIC:
<span class="ansi-green-fg ansi-bold">    926</span>     <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> valid_coco_panoptic_annotations(annotations):

<span class="ansi-red-fg">ValueError</span>: Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts (batch of images) with the following keys: `image_id` and `annotations`, with the latter being a list of annotations in the COCO format.</pre>
</div>
</div>
</div>
<p>Oh no!</p>
<p>We get an error:</p>
<blockquote class="blockquote">
<p>ValueError: Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts (batch of images) with the following keys: <code>image_id</code> and <code>annotations</code>, with the latter being a list of annotations in the COCO format.</p>
</blockquote>
<section id="preprocessing-a-single-image" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="preprocessing-a-single-image"><span class="header-section-number">8.1</span> Preprocessing a single image</h3>
<p>Okay so it turns out that our annotations aren’t in the format that the <code>preprocess</code> method was expecting.</p>
<p>Since our pre-trained model was trained on the COCO dataset, the <code>preprocess</code> method expects input data to be in line with the COCO format.</p>
<p>We can fix this later on by adjusting our annotations.</p>
<p>How about we try to preprocess just a single image instead?</p>
<div id="cell-72" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocess our target sample</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>random_sample_preprocessed_image_only <span class="op">=</span> image_processor(images<span class="op">=</span>random_sample[<span class="st">"image"</span>],</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>                                                        annotations<span class="op">=</span><span class="va">None</span>, <span class="co"># no annotations this time </span></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>                                                        masks_path<span class="op">=</span><span class="va">None</span>, <span class="co"># no masks inputs</span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>                                                        return_tensors<span class="op">=</span><span class="st">"pt"</span>) <span class="co"># return as PyTorch tensors</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Uncomment to see the full output</span></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a><span class="co"># print(random_sample_preprocessed_image_only)</span></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Print out the keys of the preprocessed image</span></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(random_sample_preprocessed_image_only.keys())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>dict_keys(['pixel_mask', 'pixel_values'])</code></pre>
</div>
</div>
<p>Nice! It looks like the <code>preprocess</code> method works on a single image.</p>
<p>And it seems like we get a dictionary output with the following keys:</p>
<ul>
<li><code>pixel_values</code> - the processed pixel values of the input image.</li>
<li>(Optional) <code>pixel_mask</code> - a mask multiplier for the pixel values as to whether they should be paid attention to or not (a value of <code>0</code> means the pixel value should be ignored by the model and a value of <code>1</code> means the pixel value should be paid attention to by the model).</li>
</ul>
<p>Beautiful!</p>
<p>Now how about we inspect our processed image’s shape?</p>
<div id="cell-75" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Uncomment to inspect all preprocessed pixel values</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="co"># print(random_sample_preprocessed_image_only["pixel_values"][0])</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Original image shape: </span><span class="sc">{</span>random_sample[<span class="st">'image'</span>]<span class="sc">.</span>size<span class="sc">}</span><span class="ss"> -&gt; [width, height]"</span>)</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Preprocessed image shape: </span><span class="sc">{</span>random_sample_preprocessed_image_only[<span class="st">'pixel_values'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [batch_size, colour_channles, height, width]"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Original image shape: (960, 1280) -&gt; [width, height]
[INFO] Preprocessed image shape: torch.Size([1, 3, 640, 480]) -&gt; [batch_size, colour_channles, height, width]</code></pre>
</div>
</div>
<p>Ok wonderful, it looks like our image has been downsized to <code>[3, 640, 480]</code> (1 item in the batch, 3 colour channels, 640 pixels high, 480 pixels wide).</p>
<p>This is down from its original size of <code>[960, 1280]</code> (1280 pixels high, 960 pixels wide).</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The order of image dimensions can differ between libraries and frameworks.</p>
<p>For example, image tensors in PyTorch typically follow the format <code>[colour_channels, height, width]</code> whereas in TensorFlow they follow <code>[height, width, colour_channels]</code>.</p>
<p>And in PIL, the format is <code>[width, height]</code>.</p>
<p>As you can imagine, this can get confusing.</p>
<p>However, with some practice, you’ll be able to decipher which is which.</p>
<p>And if your images and bounding boxes start looking strange, perhaps checking the image dimension and format can help.</p>
</div>
</div>
</section>
<section id="trying-to-pass-a-single-sample-through-our-model-part-2" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="trying-to-pass-a-single-sample-through-our-model-part-2"><span class="header-section-number">8.2</span> Trying to pass a single sample through our model (part 2)</h3>
<p>This is exciting!</p>
<p>We’ve processed an image into the format our model is expecting.</p>
<p>How about we try another forward by calling <code>model.forward(pixel_values, pixel_mask)</code>?</p>
<p>Which is the same as calling <code>model(pixel_values, pixel_mask)</code>.</p>
<div id="cell-78" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Do a single forward pass with the model</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>random_sample_outputs <span class="op">=</span> model(</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>    pixel_values<span class="op">=</span>random_sample_preprocessed_image_only[<span class="st">"pixel_values"</span>], <span class="co"># model expects input [batch_size, color_channels, height, width]</span></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># pixel_mask=random_sample_preprocessed_image_only["pixel_mask"], # some object detection models expect masks</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect the outputs</span></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a><span class="co"># random_sample_outputs # uncomment to see full outputs</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<details>
<summary>
Full output of <code>random_sample_outputs</code>:
</summary>
<pre><code>RTDetrV2ObjectDetectionOutput(loss=None, loss_dict=None, logits=tensor([[[-2.4103, -3.2165, -2.1090,  ..., -2.4911, -1.6609, -2.6324],
         [-2.5400, -3.6887, -1.6646,  ..., -2.1060, -2.1198, -2.6746],
         [-2.4400, -3.6842, -1.4782,  ..., -2.0494, -2.0977, -2.6446],
         ...,
         [-2.7582, -3.9171, -1.3343,  ..., -2.2933, -2.4012, -2.4486],
         [-2.5491, -3.2022, -1.5356,  ..., -2.5856, -2.1178, -2.7912],
         [-2.6526, -3.0643, -1.8657,  ..., -2.4201, -2.7698, -2.2681]]],
       grad_fn=&lt;SelectBackward0&gt;), pred_boxes=tensor([[[0.5707, 0.2961, 0.2293, 0.0371],
         [0.3631, 0.4809, 0.4610, 0.1908],
         [0.4239, 0.4808, 0.5838, 0.2076],
         ...,
         [0.3488, 0.4558, 0.3377, 0.1503],
         [0.3869, 0.5929, 0.3553, 0.3231],
         [0.6407, 0.7314, 0.1552, 0.1663]]], grad_fn=&lt;SelectBackward0&gt;), auxiliary_outputs=None, last_hidden_state=tensor([[[ 4.8664e-01,  8.1942e-01, -9.4307e-04,  ...,  2.0758e-01,
          -1.8689e-01, -5.1746e-01],
         [ 2.5495e-01,  9.9750e-01,  4.1035e-01,  ..., -2.5949e-02,
          -4.6852e-02, -7.8246e-01],
         [ 1.0380e-01,  8.7084e-01,  4.6921e-01,  ..., -1.8778e-01,
           8.0271e-02, -6.9041e-01],
         ...,
         [ 1.2882e-01,  9.6441e-01,  2.7554e-01,  ..., -4.3895e-01,
           1.3827e-01, -7.4690e-01],
         [ 2.7624e-01,  7.5101e-01,  2.8540e-01,  ...,  1.8248e-01,
           8.2285e-02, -5.2314e-01],
         [ 2.9966e-01,  8.5921e-01, -6.3817e-02,  ...,  2.6486e-01,
           8.5958e-02, -4.2009e-01]]], grad_fn=&lt;NativeLayerNormBackward0&gt;), intermediate_hidden_states=tensor([[[[-5.3139e-02,  5.8580e-01, -9.4774e-02,  ..., -1.0139e-01,
            5.4970e-01,  7.9870e-01],
          [-4.3604e-01,  4.7587e-01,  4.2217e-01,  ...,  2.4720e-01,
            4.7449e-01,  4.3058e-01],
          [-3.8533e-01,  3.1024e-01,  2.9360e-01,  ...,  3.8614e-01,
            2.5190e-01,  5.6520e-01],
          ...,
          [-2.1117e-01,  2.5501e-01,  5.4156e-01,  ...,  4.3788e-01,
            1.0951e-01,  3.3779e-01],
          [-8.4637e-03,  2.5538e-01, -5.3647e-01,  ...,  1.1439e-01,
           -1.5487e-01,  8.4961e-01],
          [-6.5541e-01,  6.7535e-01, -1.4167e-01,  ...,  3.8774e-01,
            1.6148e-01,  9.3250e-01]],

         [[ 1.2810e-01,  8.7219e-01,  3.6511e-01,  ...,  2.4804e-01,
           -5.8754e-01,  1.3489e-01],
          [ 2.8370e-01,  1.5663e+00,  3.4950e-03,  ..., -2.2311e-01,
            2.1730e-01, -3.5775e-01],
          [ 2.2267e-01,  1.1197e+00,  3.1473e-02,  ..., -2.2521e-01,
           -1.8248e-01, -2.1684e-01],
          ...,
          [ 2.1268e-01,  1.3836e+00,  3.6696e-01,  ..., -1.6308e-01,
           -1.3671e-01, -5.9738e-01],
          [ 8.2533e-01,  1.1259e+00, -9.0579e-02,  ..., -3.6533e-01,
           -1.3390e-02, -7.2271e-01],
          [ 5.1040e-01,  9.4933e-01, -1.5047e-01,  ..., -4.1083e-02,
            3.6723e-01,  5.1494e-02]],

         [[ 4.0047e-01,  5.0443e-01,  1.1916e-01,  ...,  2.1427e-01,
           -4.9870e-01, -5.9084e-02],
          [-5.3964e-01,  1.4135e+00,  3.8025e-01,  ..., -1.3144e-01,
           -7.2814e-01, -7.3661e-01],
          [-5.8102e-01,  1.2173e+00,  5.9914e-01,  ...,  1.2107e-01,
           -8.4583e-01, -4.5118e-02],
          ...,
          [-4.3799e-01,  1.8912e+00,  6.2712e-01,  ..., -4.0048e-01,
           -9.9042e-01, -6.5335e-01],
          [-2.6272e-02,  9.8732e-01,  2.4686e-01,  ...,  5.3733e-01,
           -6.8889e-01, -1.8957e-01],
          [ 7.4871e-01,  1.0935e+00,  2.5242e-02,  ...,  2.0705e-01,
           -4.3149e-01,  1.6533e-01]],

         [[-9.6548e-02,  6.1364e-02, -3.5741e-01,  ...,  5.7603e-01,
            1.6279e-01, -7.4688e-02],
          [-2.9921e-01,  5.0191e-01,  3.2028e-01,  ..., -8.5623e-02,
           -1.5414e-02, -8.6969e-01],
          [-4.2423e-01,  2.0131e-01,  3.0605e-01,  ..., -9.9301e-02,
           -9.4032e-03, -3.9313e-01],
          ...,
          [-3.3691e-01,  5.0171e-01,  8.0514e-02,  ..., -6.5877e-02,
           -1.8204e-03, -1.4205e-01],
          [ 3.2399e-01, -5.1599e-03, -1.0354e-01,  ...,  2.7421e-01,
            2.0394e-01, -5.6927e-01],
          [ 8.0265e-01,  1.9461e-01, -4.2067e-01,  ...,  2.3415e-02,
            6.6626e-01, -4.5957e-01]],

         [[ 1.4321e-01,  1.7849e-01, -6.8985e-01,  ...,  6.2630e-01,
            1.2153e-01, -5.3756e-01],
          [-3.5460e-01,  6.1744e-01,  7.0757e-02,  ...,  3.0924e-02,
            7.0767e-02, -1.1299e+00],
          [-3.5211e-01,  5.7647e-01,  5.0576e-01,  ..., -1.6950e-01,
            1.4924e-01, -6.5683e-01],
          ...,
          [ 4.7732e-03,  4.8165e-01, -2.9804e-01,  ...,  1.2332e-01,
            5.9123e-01, -6.4708e-01],
          [ 1.1597e-01,  4.8908e-01, -2.6656e-01,  ...,  1.7284e-01,
            5.8165e-01, -7.6454e-01],
          [ 5.7671e-01,  3.1484e-01, -6.6855e-01,  ...,  3.9596e-01,
            9.3491e-01, -3.5171e-01]],

         [[ 4.8664e-01,  8.1942e-01, -9.4307e-04,  ...,  2.0758e-01,
           -1.8689e-01, -5.1746e-01],
          [ 2.5495e-01,  9.9750e-01,  4.1035e-01,  ..., -2.5949e-02,
           -4.6852e-02, -7.8246e-01],
          [ 1.0380e-01,  8.7084e-01,  4.6921e-01,  ..., -1.8778e-01,
            8.0271e-02, -6.9041e-01],
          ...,
          [ 1.2882e-01,  9.6441e-01,  2.7554e-01,  ..., -4.3895e-01,
            1.3827e-01, -7.4690e-01],
          [ 2.7624e-01,  7.5101e-01,  2.8540e-01,  ...,  1.8248e-01,
            8.2285e-02, -5.2314e-01],
          [ 2.9966e-01,  8.5921e-01, -6.3817e-02,  ...,  2.6486e-01,
            8.5958e-02, -4.2009e-01]]]], grad_fn=&lt;StackBackward0&gt;), intermediate_logits=tensor([[[[-2.7654, -1.9506, -3.2306,  ..., -1.7228, -5.0831, -3.3259],
          [-1.6720, -2.0784, -3.3905,  ..., -1.8552, -4.7686, -2.3647],
          [-1.6152, -1.7790, -3.3754,  ..., -1.7405, -4.9992, -2.6163],
          ...,
          [-1.7987, -1.3256, -3.1915,  ..., -1.8915, -4.8887, -2.5755],
          [-1.8172, -1.1075, -3.1850,  ..., -1.5766, -4.7429, -2.9463],
          [-2.3645, -1.5377, -3.2648,  ..., -0.9642, -3.8302, -2.9143]],

         [[-2.2888, -0.3772, -3.3768,  ..., -2.0233, -1.4014, -2.1638],
          [-2.3195,  0.1030, -2.7420,  ..., -1.5070, -1.5560, -1.8782],
          [-2.1245, -0.0459, -2.9056,  ..., -1.8131, -2.3000, -1.5002],
          ...,
          [-1.6669, -0.5204, -2.4404,  ..., -1.5310, -2.6033, -1.8718],
          [-2.1209, -0.0206, -2.9078,  ..., -2.4905, -1.4664, -1.9780],
          [-2.8389, -0.9289, -1.7524,  ..., -1.9419, -2.0081, -2.5840]],

         [[-1.4762, -1.8236, -2.0330,  ..., -1.9085, -3.0767, -1.0480],
          [-1.3000, -1.9365, -1.8160,  ..., -3.6340, -2.9030,  0.1608],
          [-1.8684, -1.3785, -2.5306,  ..., -3.2591, -3.6757,  0.0854],
          ...,
          [-2.1450, -2.0016, -1.9492,  ..., -3.1256, -2.6620,  0.3906],
          [-1.2368, -1.5141, -2.7120,  ..., -3.1521, -3.3444,  0.1144],
          [-1.2681, -1.5955, -1.4615,  ..., -2.9850, -2.3149,  0.0573]],

         [[ 0.2567, -3.0313, -1.2494,  ..., -0.4495, -1.7004, -2.2419],
          [-0.9745, -1.9402, -1.3785,  ..., -0.7464, -0.7797, -2.6964],
          [-0.8782, -2.3249, -1.3109,  ..., -0.4191, -0.9425, -2.6211],
          ...,
          [-0.0977, -2.1923, -1.3370,  ..., -1.5180, -1.6462, -2.4973],
          [-1.4765, -2.7589, -1.4049,  ..., -0.8494, -0.8584, -2.7061],
          [-1.5082, -2.3043, -1.5386,  ..., -1.7372, -1.5837, -3.2329]],

         [[-2.0384, -2.6995, -1.7593,  ..., -2.6025, -2.1955, -1.0745],
          [-1.6715, -3.3748, -1.4403,  ..., -2.0172, -2.1580, -1.1449],
          [-1.3890, -2.7188, -1.4331,  ..., -2.0791, -2.3328, -1.2735],
          ...,
          [-1.1386, -2.7756, -1.2096,  ..., -1.8302, -3.0670, -1.4466],
          [-1.5671, -2.8469, -1.7781,  ..., -1.9640, -2.5537, -0.8333],
          [-1.6719, -4.0084, -1.9040,  ..., -2.5117, -3.6465, -0.9080]],

         [[-2.4103, -3.2165, -2.1090,  ..., -2.4911, -1.6609, -2.6324],
          [-2.5400, -3.6887, -1.6646,  ..., -2.1060, -2.1198, -2.6746],
          [-2.4400, -3.6842, -1.4782,  ..., -2.0494, -2.0977, -2.6446],
          ...,
          [-2.7582, -3.9171, -1.3343,  ..., -2.2933, -2.4012, -2.4486],
          [-2.5491, -3.2022, -1.5356,  ..., -2.5856, -2.1178, -2.7912],
          [-2.6526, -3.0643, -1.8657,  ..., -2.4201, -2.7698, -2.2681]]]],
       grad_fn=&lt;StackBackward0&gt;), intermediate_reference_points=tensor([[[[0.4246, 0.3450, 0.6868, 0.0952],
          [0.3522, 0.5075, 0.4007, 0.2292],
          [0.4169, 0.5195, 0.5569, 0.2495],
          ...,
          [0.3323, 0.4641, 0.4027, 0.1624],
          [0.3743, 0.5587, 0.4831, 0.2996],
          [0.6440, 0.7286, 0.1657, 0.1577]],

         [[0.4947, 0.3540, 0.8195, 0.0886],
          [0.3510, 0.4977, 0.4081, 0.2191],
          [0.4038, 0.5018, 0.5501, 0.2451],
          ...,
          [0.3365, 0.4615, 0.3876, 0.1573],
          [0.3595, 0.5509, 0.4855, 0.3372],
          [0.6372, 0.7305, 0.1418, 0.1217]],

         [[0.5216, 0.3579, 0.8873, 0.0982],
          [0.3686, 0.4920, 0.4201, 0.2134],
          [0.4142, 0.4943, 0.5637, 0.2140],
          ...,
          [0.3453, 0.4565, 0.3535, 0.1386],
          [0.3782, 0.5932, 0.4974, 0.4147],
          [0.6417, 0.7370, 0.1343, 0.1304]],

         [[0.5531, 0.3710, 0.7984, 0.0757],
          [0.3646, 0.4909, 0.4443, 0.2195],
          [0.4210, 0.4847, 0.5764, 0.2169],
          ...,
          [0.3475, 0.4538, 0.3316, 0.1413],
          [0.3795, 0.6289, 0.3801, 0.4177],
          [0.6398, 0.7362, 0.1482, 0.1549]],

         [[0.5707, 0.2961, 0.2293, 0.0371],
          [0.3631, 0.4809, 0.4610, 0.1908],
          [0.4239, 0.4808, 0.5838, 0.2075],
          ...,
          [0.3488, 0.4558, 0.3377, 0.1503],
          [0.3869, 0.5929, 0.3553, 0.3231],
          [0.6407, 0.7314, 0.1552, 0.1663]],

         [[0.5707, 0.2961, 0.2293, 0.0371],
          [0.3631, 0.4809, 0.4610, 0.1908],
          [0.4239, 0.4808, 0.5838, 0.2076],
          ...,
          [0.3488, 0.4558, 0.3377, 0.1503],
          [0.3869, 0.5929, 0.3553, 0.3231],
          [0.6407, 0.7314, 0.1552, 0.1663]]]], grad_fn=&lt;StackBackward0&gt;), intermediate_predicted_corners=None, initial_reference_points=None, decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=[tensor([[[[ 1.3938e+00,  3.6899e+00,  3.9381e+00,  ...,  2.0755e+00,
            1.8361e+00,  1.3195e+00],
          [ 1.3440e+00,  3.2245e+00,  1.4568e+00,  ...,  1.9746e-01,
            2.1494e-01,  2.6737e-01],
          [ 2.7528e-01,  2.2857e-01,  1.9231e-02,  ...,  1.8635e-01,
           -1.4560e-01, -2.7372e-02],
          ...,
          [-4.3640e-01,  4.9777e-01,  3.4311e-01,  ...,  7.7798e-02,
           -2.6776e-02,  5.2855e-02],
          [-4.1876e-01,  1.3459e+00, -2.7879e-01,  ..., -5.2242e-01,
           -2.5501e-01,  1.2173e-01],
          [-5.0070e-01, -4.2214e-01, -5.4279e-01,  ..., -4.6641e-01,
           -5.0324e-01, -3.3993e-01]],

         [[ 2.4725e+00,  2.0903e-01,  2.6478e+00,  ...,  3.4342e+00,
            9.2424e-01,  5.1411e+00],
          [-2.3204e-01, -5.1897e-02,  1.2316e+00,  ...,  1.6147e+00,
            1.1504e-01,  2.9225e+00],
          [ 1.6521e+00,  1.0391e+00,  7.7571e-01,  ...,  2.1104e+00,
            2.7364e-01,  3.4205e+00],
          ...,
          [ 1.5187e+00,  1.9785e-02,  3.1015e-01,  ...,  1.4698e-02,
           -2.7163e-01,  2.8728e+00],
          [-3.2695e-01, -4.2197e-01, -3.7899e-01,  ..., -4.1741e-01,
           -3.8424e-01,  9.2021e-02],
          [ 1.9287e+00, -3.8777e-02,  9.3081e-01,  ...,  2.9011e-01,
           -4.3217e-01,  1.4951e+00]],

         [[ 1.2350e+00,  4.6030e-01,  2.8556e-01,  ...,  1.9445e-01,
            1.6401e-01,  8.9029e-01],
          [ 3.6415e-01, -2.8295e-01, -2.6529e-01,  ..., -1.7090e-01,
            1.7011e-03, -4.6186e-01],
          [ 7.8289e-01,  1.4218e-01,  7.1477e-01,  ...,  1.3742e-01,
            7.8084e-01,  7.7748e-01],
          ...,
          [ 8.6720e-01,  1.5243e+00, -4.3439e-01,  ...,  5.5073e-01,
            5.3333e-01, -1.0822e-01],
          [-3.4549e-01, -4.2964e-01, -5.3637e-01,  ...,  6.4962e-01,
            5.0146e-01, -2.1609e-01],
          [ 8.9138e-01,  9.1170e-02, -4.0709e-01,  ...,  5.9540e-01,
            3.3999e-01,  1.8298e-01]],

         ...,

         [[ 2.1335e+00,  1.8645e+00,  6.3417e-01,  ...,  3.1492e-01,
            8.0026e-01,  1.7273e+00],
          [ 1.4950e+00,  3.3948e+00,  1.5539e+00,  ...,  2.7506e+00,
            1.6872e+00,  1.6127e+00],
          [ 1.6123e+00,  2.7481e+00,  1.6800e+00,  ...,  1.5145e+00,
            4.7902e-01,  1.3030e+00],
          ...,
          [ 6.9388e-01,  7.5684e-02,  4.9832e-01,  ...,  5.0049e-01,
            3.6597e-01,  6.8521e-01],
          [ 1.3686e+00,  6.8740e-01,  2.7520e-01,  ...,  1.1264e-01,
            9.8072e-01,  1.2247e+00],
          [ 2.0563e+00,  7.3219e-01,  7.6325e-01,  ...,  1.0054e-01,
            1.2625e+00,  1.5637e+00]],

         [[-5.0865e-01, -2.1889e-01, -5.0291e-01,  ..., -4.8922e-01,
            1.0710e-01, -1.3777e-01],
          [-2.1542e-01,  3.0400e-02, -4.8931e-01,  ..., -4.8805e-01,
            6.0389e-01, -2.9731e-01],
          [-4.8478e-01, -5.2478e-01, -5.2429e-01,  ..., -3.8690e-01,
           -3.9214e-01, -5.1093e-01],
          ...,
          [-3.3657e-01, -3.8390e-01, -2.0802e-01,  ...,  6.8628e-02,
           -1.4215e-01, -4.1823e-01],
          [ 6.0165e-01,  4.8848e-01, -2.1377e-01,  ..., -4.1432e-01,
           -4.2646e-02, -4.9816e-01],
          [-2.2064e-01,  1.3190e-01, -3.2296e-01,  ..., -5.4226e-01,
           -4.9149e-01, -5.2242e-01]],

         [[ 6.8631e+00,  2.0115e+00,  1.0507e+00,  ...,  7.6572e-01,
            2.8275e+00,  4.7968e+00],
          [ 6.0376e+00,  2.6104e+00,  1.0852e+00,  ...,  8.7101e-01,
            3.6189e+00,  5.2539e+00],
          [ 4.4559e+00,  1.1176e+00,  6.1536e-01,  ...,  5.0730e-01,
            1.7848e+00,  3.4097e+00],
          ...,
          [ 2.7294e+00,  2.6141e+00,  2.0362e-01,  ..., -4.6068e-02,
            7.6516e-01,  7.5494e-01],
          [ 3.6208e+00,  2.3895e+00,  1.2078e+00,  ..., -1.6173e-01,
            1.1155e+00,  1.2592e+00],
          [ 4.0734e+00,  2.5129e+00,  1.4954e+00,  ..., -1.4671e-01,
            7.0241e-01,  1.2234e+00]]]], grad_fn=&lt;AddBackward0&gt;), tensor([[[[ 0.5185,  0.3819, -0.1690,  ...,  0.1612,  0.3752,  0.8158],
          [-0.3562, -0.1270, -0.2667,  ...,  0.0401,  0.4915, -0.2151],
          [-0.4916, -0.5365, -0.5014,  ..., -0.3831, -0.1949, -0.1640],
          ...,
          [-0.1386,  0.0442, -0.1045,  ..., -0.2343, -0.3740, -0.4187],
          [ 0.3513, -0.0980,  0.0411,  ..., -0.3251, -0.0708,  0.0967],
          [ 0.3068,  0.0513, -0.2880,  ..., -0.5223, -0.3471,  0.1323]],

         [[ 5.2011,  2.3660,  1.2325,  ...,  0.8635,  1.1357,  2.8331],
          [ 0.2637,  0.3067,  0.3858,  ...,  1.0561,  1.7242,  0.2823],
          [ 0.9179,  0.4715,  0.1141,  ...,  1.1161,  1.5353,  0.0766],
          ...,
          [-0.0186, -0.2360, -0.1069,  ...,  0.6026,  1.6307,  0.4062],
          [ 0.2480,  0.2849,  0.9233,  ...,  1.4712,  2.0401,  0.4068],
          [ 0.9946,  0.0223,  0.7812,  ...,  1.2851,  1.5783,  1.0995]],

         [[-0.1503, -0.3740,  0.1545,  ..., -0.1221, -0.3535,  0.2091],
          [ 0.1572, -0.3622, -0.0614,  ..., -0.3433, -0.2081, -0.3249],
          [ 0.3192, -0.1503,  0.6414,  ..., -0.3637, -0.1499, -0.2254],
          ...,
          [ 0.0378,  0.5036,  0.1861,  ..., -0.5171, -0.5046, -0.5475],
          [-0.3393, -0.4130, -0.1570,  ..., -0.3578, -0.3516, -0.4207],
          [-0.4436, -0.1539, -0.3768,  ..., -0.5277, -0.4855, -0.4495]],

         ...,

         [[-0.4967, -0.3191, -0.5172,  ..., -0.3178, -0.0690, -0.5089],
          [-0.2761,  0.0149, -0.4904,  ..., -0.2543,  0.0177, -0.2294],
          [-0.4058, -0.4162, -0.2881,  ...,  0.0443,  0.4478, -0.4462],
          ...,
          [-0.4004, -0.1296,  0.1152,  ...,  0.4313,  0.6645,  0.2798],
          [-0.4441,  0.1218, -0.4305,  ...,  0.4615,  0.6798, -0.1293],
          [-0.5465, -0.3989, -0.5344,  ..., -0.0198,  0.0151, -0.4183]],

         [[-0.3388, -0.5053, -0.5295,  ..., -0.4755, -0.4938, -0.5397],
          [-0.4959, -0.5068, -0.5260,  ..., -0.4077, -0.4669,  0.1614],
          [ 0.7145, -0.1875, -0.1235,  ...,  0.2665,  0.0499,  1.1588],
          ...,
          [-0.4128, -0.3582, -0.5506,  ...,  0.2992, -0.2863,  0.2803],
          [ 1.0126, -0.5243,  0.2794,  ...,  1.5115,  1.1862,  1.6769],
          [-0.5020, -0.5326, -0.5383,  ..., -0.0564,  0.1121,  0.0871]],

         [[-0.5202, -0.1108, -0.1819,  ..., -0.4941, -0.4913, -0.5165],
          [-0.2305,  0.1010, -0.2430,  ...,  0.5093,  0.1895,  0.1037],
          [-0.4895, -0.3958, -0.3056,  ..., -0.2141, -0.0102,  0.5653],
          ...,
          [ 1.0310, -0.5228, -0.1168,  ..., -0.5437, -0.4989,  0.2949],
          [-0.0247, -0.3842, -0.1510,  ..., -0.5504, -0.5146, -0.0977],
          [-0.5410, -0.5339, -0.4973,  ..., -0.5472, -0.4962, -0.3349]]]],
       grad_fn=&lt;AddBackward0&gt;), tensor([[[[ 1.7336e-01, -2.6222e-01, -4.2793e-01,  ..., -1.1853e-01,
           -2.9695e-01, -2.0832e-01],
          [-3.4296e-03, -5.1239e-01, -5.2320e-01,  ..., -4.6905e-01,
           -5.4388e-01, -1.7963e-01],
          [ 6.9119e-02, -4.7162e-01, -4.7037e-01,  ..., -2.3459e-01,
           -3.6831e-01, -2.2590e-02],
          ...,
          [ 8.7253e-01,  7.6361e-01,  5.9890e-01,  ...,  8.0291e-01,
            1.0609e+00,  1.1305e+00],
          [ 6.8713e-01,  6.3124e-01,  6.0094e-01,  ...,  8.2307e-01,
            1.2763e+00,  4.9848e-01],
          [ 3.0797e-01,  1.7016e-01,  8.4849e-01,  ...,  3.9683e-02,
            1.4594e-01, -7.6637e-02]],

         [[-4.7298e-01, -4.7068e-01, -4.7336e-01,  ..., -4.0120e-01,
           -3.2050e-01, -2.8249e-01],
          [-4.7759e-01, -5.4533e-01, -4.9800e-01,  ..., -5.0616e-01,
           -4.6732e-01, -3.0524e-01],
          [-4.9285e-01, -5.3249e-01, -5.3137e-01,  ..., -5.2591e-01,
           -5.4686e-01, -4.9311e-01],
          ...,
          [-2.4863e-02,  5.6615e-01,  1.3635e+00,  ...,  1.5708e+00,
            9.7564e-01,  8.4283e-01],
          [-3.4610e-02,  9.4266e-01,  9.5997e-01,  ...,  1.2739e+00,
            7.4010e-01,  7.9288e-01],
          [-6.2025e-02, -2.2215e-01,  4.5640e-02,  ...,  1.0250e+00,
            1.0097e+00,  3.7875e-01]],

         [[ 8.6356e-01,  9.1551e-01,  8.9298e-01,  ...,  7.6591e-01,
            7.6748e-01,  1.3054e+00],
          [ 5.0886e-01,  4.1404e-01,  2.8045e-01,  ...,  7.0586e-01,
            4.6785e-01,  1.3641e+00],
          [ 2.3436e-01,  1.8401e-01,  6.3883e-01,  ...,  6.3147e-01,
            3.4000e-01,  3.0140e-01],
          ...,
          [-3.2019e-01, -2.9413e-01, -1.6662e-01,  ...,  1.5093e-02,
            2.1418e-01,  3.8254e-01],
          [-2.4847e-01, -2.5539e-01, -3.1744e-01,  ..., -2.7485e-01,
           -3.5164e-01, -3.4260e-02],
          [-2.0574e-01, -1.8172e-01, -1.6656e-01,  ..., -1.1945e-01,
           -3.5600e-02,  1.7422e-02]],

         ...,

         [[ 1.0281e-01, -1.7983e-01, -2.6104e-01,  ...,  2.9685e-01,
            2.9522e-02, -1.5922e-01],
          [ 6.4848e-01,  2.1403e-02, -2.9381e-02,  ...,  3.1224e-01,
            4.1161e-01,  7.7101e-01],
          [ 1.5228e-01,  8.2648e-02, -2.4173e-02,  ...,  8.9092e-01,
            6.4181e-01,  4.0476e-01],
          ...,
          [ 4.2079e-01, -3.6907e-02, -1.4251e-01,  ..., -2.9826e-01,
           -6.7151e-02, -3.6289e-02],
          [ 1.2386e-01, -4.1844e-01, -3.9618e-01,  ..., -5.0743e-01,
           -3.4355e-01, -1.8816e-01],
          [-4.4068e-01, -5.4903e-01, -4.7067e-01,  ..., -6.2028e-02,
           -5.0896e-01, -2.2307e-01]],

         [[ 9.8333e-01, -2.7065e-01, -3.3590e-01,  ..., -3.3124e-01,
           -1.4504e-01,  7.5079e-01],
          [ 4.1522e-01, -3.4168e-01, -3.9592e-01,  ..., -4.3886e-01,
           -3.1252e-01,  2.9204e-01],
          [ 5.3854e-01, -9.4371e-02, -2.8638e-01,  ..., -3.7553e-01,
           -2.0117e-01,  5.2053e-01],
          ...,
          [ 7.5336e-01,  4.2127e-02, -4.7527e-01,  ..., -2.3514e-01,
            3.8577e-01,  1.0483e+00],
          [ 7.9049e-01,  2.1774e-02, -3.5424e-01,  ..., -2.3879e-01,
            1.9274e-01,  7.7723e-01],
          [ 8.9491e-01,  3.2616e-01, -1.3408e-01,  ...,  1.5268e-01,
            5.6745e-01,  1.0702e+00]],

         [[ 1.7084e-01, -3.4757e-02, -1.7290e-01,  ..., -8.5381e-02,
            1.6041e-01,  4.4312e-01],
          [ 3.7726e-01, -2.6368e-02, -3.0844e-01,  ...,  5.2936e-02,
            6.6930e-03,  3.1330e-01],
          [ 5.3114e-01, -2.7307e-02, -4.2941e-01,  ...,  3.0857e-01,
            6.0580e-01,  1.5468e-01],
          ...,
          [ 4.8378e-01, -1.1162e-03, -1.8971e-01,  ..., -3.5753e-01,
           -2.4050e-01, -2.1233e-01],
          [ 6.9566e-01,  6.6009e-01,  7.9304e-02,  ..., -3.6931e-01,
           -1.2620e-01, -4.8648e-02],
          [ 4.7603e-01,  3.2894e-01, -2.1638e-01,  ..., -4.6596e-01,
           -2.9300e-01, -7.0762e-02]]]], grad_fn=&lt;AddBackward0&gt;)], encoder_hidden_states=None, encoder_attentions=None, init_reference_points=tensor([[[-0.4862, -0.7119, -0.1336, -2.2152],
         [-0.6809,  0.0766, -0.6053, -1.3876],
         [-0.6570,  0.1424, -0.2582, -1.2478],
         ...,
         [-0.7187, -0.1965, -0.4079, -1.5115],
         [-0.7981,  0.2656,  0.4589, -1.1378],
         [ 0.6217,  1.1485, -1.5680, -1.7015]]]), enc_topk_logits=tensor([[[-2.3160, -1.3959, -2.2753,  ..., -1.0056, -2.1959, -2.4759],
         [-1.8523, -2.0708, -2.8310,  ..., -0.8968, -2.3933, -2.8444],
         [-1.8766, -1.7784, -2.8721,  ..., -0.8604, -2.3539, -3.0173],
         ...,
         [-2.7151, -2.0619, -2.1254,  ..., -1.1068, -2.5075, -3.0717],
         [-2.3066, -0.9151, -1.8485,  ..., -0.6808, -2.1900, -2.4505],
         [-0.6756, -1.9183, -0.7957,  ..., -0.7819, -1.3664, -1.9971]]],
       grad_fn=&lt;GatherBackward0&gt;), enc_topk_bboxes=tensor([[[0.3808, 0.3292, 0.4666, 0.0984],
         [0.3361, 0.5191, 0.3531, 0.1998],
         [0.3414, 0.5355, 0.4358, 0.2231],
         ...,
         [0.3277, 0.4510, 0.3994, 0.1807],
         [0.3104, 0.5660, 0.6128, 0.2427],
         [0.6506, 0.7592, 0.1725, 0.1543]]], grad_fn=&lt;SigmoidBackward0&gt;), enc_outputs_class=tensor([[[-1.8213, -0.6814, -1.5843,  ..., -1.2911, -1.7899, -3.3334],
         [-1.8213, -0.6814, -1.5843,  ..., -1.2911, -1.7899, -3.3334],
         [-1.8213, -0.6814, -1.5843,  ..., -1.2911, -1.7899, -3.3334],
         ...,
         [-1.4263, -0.7852, -3.2138,  ..., -1.3979, -2.2594, -0.6975],
         [-1.8004, -0.7145, -2.7931,  ..., -1.6721, -2.0172, -0.9661],
         [-1.8367, -0.4701, -3.1283,  ..., -1.8585, -1.4448, -1.6497]]],
       grad_fn=&lt;ViewBackward0&gt;), enc_outputs_coord_logits=tensor([[[ 3.4028e+38,  3.4028e+38,  3.4028e+38,  3.4028e+38],
         [ 3.4028e+38,  3.4028e+38,  3.4028e+38,  3.4028e+38],
         [ 3.4028e+38,  3.4028e+38,  3.4028e+38,  3.4028e+38],
         ...,
         [ 1.3017e+00,  1.7718e+00, -5.1475e-01, -6.0635e-01],
         [ 1.8548e+00,  2.4244e+00, -1.1509e+00, -1.5645e+00],
         [ 2.9150e+00,  2.5202e+00, -2.4418e+00, -1.7639e+00]]],
       grad_fn=&lt;AddBackward0&gt;), denoising_meta_values=None)</code></pre>
</details>
<p>Nice!</p>
<p>It looks like it worked!</p>
<p>Our model processed our <code>random_sample_preprocessed_image_only["pixel_values"]</code> and returned a <a href="https://huggingface.co/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2ForObjectDetection.forward"><code>RTDetrV2ObjectDetectionOutput</code></a> object as output.</p>
<p>Let’s inspect the <code>keys()</code> method of this output and see what they are.</p>
<div id="cell-80" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the keys of the output</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>random_sample_outputs.keys()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>odict_keys(['logits', 'pred_boxes', 'last_hidden_state', 'intermediate_hidden_states', 'intermediate_logits', 'intermediate_reference_points', 'encoder_last_hidden_state', 'init_reference_points', 'enc_topk_logits', 'enc_topk_bboxes', 'enc_outputs_class', 'enc_outputs_coord_logits'])</code></pre>
</div>
</div>
<p>Breaking these down:</p>
<ul>
<li><code>logits</code> - The raw outputs from the model, these are the classification <a href="https://datascience.stackexchange.com/questions/31041/what-does-logits-in-machine-learning-mean">logits</a> we can later apply a <a href="https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html"><strong>softmax function</strong></a>/<a href="https://en.wikipedia.org/wiki/Sigmoid_function"><strong>sigmoid function</strong></a> to to get <strong>prediction probabilties</strong>.</li>
<li><code>pred_boxes</code> - Normalized box coordinates in <code>CXCYWH</code> (<code>(center_x, center_y, width, height)</code>) format.</li>
<li><code>last_hidden_state</code> - Last hidden state of the last decoder layer of the model.</li>
<li><code>encoder_last_hidden_state</code> - Last hidden state of the last encoder layer of the model.</li>
</ul>
<p>How about we inspect the <code>shape</code> attribute of the <code>logits</code>?</p>
<div id="cell-83" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect logits output shape</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>output_logits <span class="op">=</span> random_sample_outputs.logits</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Output logits shape: </span><span class="sc">{</span>output_logits<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [1 image, 300 boxes, 7 classes]"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Output logits shape: torch.Size([1, 300, 7]) -&gt; [1 image, 300 boxes, 7 classes]</code></pre>
</div>
</div>
<p>Nice!</p>
<p>We get an output from our model that coincides with the shape of our data.</p>
<p>The final value of <code>7</code> in the <code>output_logits</code> tensor is equivalent to the number of classes we have.</p>
<p>And the <code>300</code> is the number of boxes our model predicts for each image (this is defined by the <code>num_queries</code> parameter of the <a href="https://huggingface.co/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2Config"><code>transformers.RTDetrV2Config</code></a>, where <code>num_queries=300</code> is the default).</p>
<div id="cell-85" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect predicted boxes output shape</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>output_pred_boxes <span class="op">=</span> random_sample_outputs.pred_boxes</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Output predicted boxes shape: </span><span class="sc">{</span>output_pred_boxes<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [1 image, 300 boxes, 4 coordinates (center_x, center_y, width, height)]"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Output predicted boxes shape: torch.Size([1, 300, 4]) -&gt; [1 image, 300 boxes, 4 coordinates (center_x, center_y, width, height)]</code></pre>
</div>
</div>
<p>Reading the <a href="https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection.forward">documentation for the <code>forward</code> method</a>, we can determine the output format of our models predicted boxes:</p>
<blockquote class="blockquote">
<p>Returns:</p>
<p>pred_boxes (torch.FloatTensor of shape (batch_size, num_queries, 4)) — Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding possible padding). You can use <a href="https://huggingface.co/docs/transformers/main/en/model_doc/rt_detr#transformers.RTDetrImageProcessor.post_process_object_detection"><code>post_process_object_detection()</code></a> to retrieve the unnormalized bounding boxes.</p>
</blockquote>
<p>This is good to know!</p>
<p>It means that the raw output boxes from our model come in normalized <code>CXCYWH</code> format (see <a href="#tbl-bbox-formats" class="quarto-xref">Table&nbsp;1</a> for more).</p>
<p>How about we inspect a single box?</p>
<div id="cell-87" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Single example predicted bounding box coordinates</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Example output box: </span><span class="sc">{</span>output_pred_boxes[:, <span class="dv">0</span>, :][<span class="dv">0</span>]<span class="sc">.</span>detach()<span class="sc">}</span><span class="ss"> -&gt; (center_x, center_y, width, height)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Example output box: tensor([0.5608, 0.4045, 0.0437, 0.0570]) -&gt; (center_x, center_y, width, height)</code></pre>
</div>
</div>
<p>Excellent!</p>
<p>We can process these boxes and logits later on into different formats using the <a href="https://huggingface.co/docs/transformers/en/model_doc/rt_detr#transformers.RTDetrImageProcessor.post_process_object_detection"><code>transformers.RTDetrImageProcessor.post_process_object_detection</code></a> method.</p>
<p>For now, let’s figure out how to preprocess our annotations.</p>
</section>
</section>
<section id="preprocessing-our-annotations" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="preprocessing-our-annotations"><span class="header-section-number">9</span> Preprocessing our annotations</h2>
<p>One of the most tricky parts of any machine learning problem is getting your data in the right format.</p>
<p>We’ve done it for our images.</p>
<p>Now let’s do it for our annotations.</p>
<section id="trying-to-preprocess-a-single-annotation" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="trying-to-preprocess-a-single-annotation"><span class="header-section-number">9.1</span> Trying to preprocess a single annotation</h3>
<p>Recall in a previous section we tried to preprocess a single image and its annotation.</p>
<p>And we got an error.</p>
<p>Let’s make sure we’re not crazy and this is still the case.</p>
<div id="cell-91" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocess a single image and annotation pair</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>image_processor.preprocess(</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>    images<span class="op">=</span>random_sample[<span class="st">"image"</span>], </span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>    annotations<span class="op">=</span>random_sample[<span class="st">"annotations"</span>]</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[32], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Preprocess a single image and annotation pair</span>
<span class="ansi-green-fg">----&gt; 2</span> <span class="ansi-yellow-bg">image_processor</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">preprocess</span><span class="ansi-yellow-bg">(</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">images</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">random_sample</span><span class="ansi-yellow-bg">[</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">"</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">image</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">"</span><span class="ansi-yellow-bg">]</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span>
<span class="ansi-green-fg ansi-bold">      4</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">annotations</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">random_sample</span><span class="ansi-yellow-bg">[</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">"</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">annotations</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">"</span><span class="ansi-yellow-bg">]</span>
<span class="ansi-green-fg ansi-bold">      5</span> <span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/rt_detr/image_processing_rt_detr_fast.py:403</span>, in <span class="ansi-cyan-fg">RTDetrImageProcessorFast.preprocess</span><span class="ansi-blue-fg">(self, images, annotations, masks_path, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">    380</span> <span style="color:rgb(175,0,255)">@auto_docstring</span>
<span class="ansi-green-fg ansi-bold">    381</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span> <span style="color:rgb(0,0,255)">preprocess</span>(
<span class="ansi-green-fg ansi-bold">    382</span>     <span style="color:rgb(0,135,0)">self</span>,
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">    386</span>     <span style="color:rgb(98,98,98)">*</span><span style="color:rgb(98,98,98)">*</span>kwargs: Unpack[RTDetrFastImageProcessorKwargs],
<span class="ansi-green-fg ansi-bold">    387</span> ) <span style="color:rgb(98,98,98)">-</span><span style="color:rgb(98,98,98)">&gt;</span> BatchFeature:
<span class="ansi-green-fg ansi-bold">    388</span> <span style="color:rgb(188,188,188)">    </span><span style="color:rgb(175,0,0)">r</span><span style="font-style:italic;color:rgb(175,0,0)">"""</span>
<span class="ansi-green-fg ansi-bold">    389</span> <span style="font-style:italic;color:rgb(175,0,0)">    annotations (`AnnotationType` or `List[AnnotationType]`, *optional*):</span>
<span class="ansi-green-fg ansi-bold">    390</span> <span style="font-style:italic;color:rgb(175,0,0)">        List of annotations associated with the image or batch of images. If annotation is for object</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">    401</span> <span style="font-style:italic;color:rgb(175,0,0)">        Path to the directory containing the segmentation masks.</span>
<span class="ansi-green-fg ansi-bold">    402</span> <span style="font-style:italic;color:rgb(175,0,0)">    """</span>
<span class="ansi-green-fg">--&gt; 403</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">super</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">)</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">preprocess</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">images</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">annotations</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">masks_path</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/image_processing_utils_fast.py:609</span>, in <span class="ansi-cyan-fg">BaseImageProcessorFast.preprocess</span><span class="ansi-blue-fg">(self, images, *args, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">    606</span> kwargs<span style="color:rgb(98,98,98)">.</span>pop(<span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">default_to_square</span><span style="color:rgb(175,0,0)">"</span>)
<span class="ansi-green-fg ansi-bold">    607</span> kwargs<span style="color:rgb(98,98,98)">.</span>pop(<span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">data_format</span><span style="color:rgb(175,0,0)">"</span>)
<span class="ansi-green-fg">--&gt; 609</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_preprocess</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">images</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/rt_detr/image_processing_rt_detr_fast.py:440</span>, in <span class="ansi-cyan-fg">RTDetrImageProcessorFast._preprocess</span><span class="ansi-blue-fg">(self, images, annotations, masks_path, return_segmentation_masks, do_resize, size, interpolation, do_rescale, rescale_factor, do_normalize, do_convert_annotations, image_mean, image_std, do_pad, pad_size, format, return_tensors, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">    438</span> <span style="color:rgb(0,135,0)">format</span> <span style="color:rgb(98,98,98)">=</span> AnnotationFormat(<span style="color:rgb(0,135,0)">format</span>)
<span class="ansi-green-fg ansi-bold">    439</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> annotations <span style="font-weight:bold;color:rgb(175,0,255)">is</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>:
<span class="ansi-green-fg">--&gt; 440</span>     <span class="ansi-yellow-bg">validate_annotations</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">format</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">SUPPORTED_ANNOTATION_FORMATS</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">annotations</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    442</span> data <span style="color:rgb(98,98,98)">=</span> {}
<span class="ansi-green-fg ansi-bold">    443</span> processed_images <span style="color:rgb(98,98,98)">=</span> []

File <span class="ansi-green-fg">~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/image_utils.py:919</span>, in <span class="ansi-cyan-fg">validate_annotations</span><span class="ansi-blue-fg">(annotation_format, supported_annotation_formats, annotations)</span>
<span class="ansi-green-fg ansi-bold">    917</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> annotation_format <span style="font-weight:bold;color:rgb(175,0,255)">is</span> AnnotationFormat<span style="color:rgb(98,98,98)">.</span>COCO_DETECTION:
<span class="ansi-green-fg ansi-bold">    918</span>     <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> valid_coco_detection_annotations(annotations):
<span class="ansi-green-fg">--&gt; 919</span>         <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">ValueError</span>(
<span class="ansi-green-fg ansi-bold">    920</span>             <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts </span><span style="color:rgb(175,0,0)">"</span>
<span class="ansi-green-fg ansi-bold">    921</span>             <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">(batch of images) with the following keys: `image_id` and `annotations`, with the latter </span><span style="color:rgb(175,0,0)">"</span>
<span class="ansi-green-fg ansi-bold">    922</span>             <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">being a list of annotations in the COCO format.</span><span style="color:rgb(175,0,0)">"</span>
<span class="ansi-green-fg ansi-bold">    923</span>         )
<span class="ansi-green-fg ansi-bold">    925</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> annotation_format <span style="font-weight:bold;color:rgb(175,0,255)">is</span> AnnotationFormat<span style="color:rgb(98,98,98)">.</span>COCO_PANOPTIC:
<span class="ansi-green-fg ansi-bold">    926</span>     <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> valid_coco_panoptic_annotations(annotations):

<span class="ansi-red-fg">ValueError</span>: Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts (batch of images) with the following keys: `image_id` and `annotations`, with the latter being a list of annotations in the COCO format.</pre>
</div>
</div>
</div>
<p>Wonderful!</p>
<p>We’re not crazy…</p>
<p>But we still get an error:</p>
<blockquote class="blockquote">
<p>ValueError: Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts (batch of images) with the following keys: <code>image_id</code> and <code>annotations</code>, with the latter being a list of annotations in the COCO format.</p>
</blockquote>
<p>In this section, we’re going to fix it.</p>
</section>
<section id="discussing-the-format-our-annotations-need-to-be-in" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="discussing-the-format-our-annotations-need-to-be-in"><span class="header-section-number">9.2</span> Discussing the format our annotations need to be in</h3>
<p>According the error we got in the previous segment, the <a href="https://huggingface.co/docs/transformers/en/model_doc/rt_detr#transformers.RTDetrImageProcessor.preprocess"><code>transformers.RTDetrImageProcessor.preprocess</code></a> method expects input annotations in COCO format.</p>
<p>In the documentation we can read that the <code>annotations</code> parameter taks in a list of dictionaries with the following keys:</p>
<ul>
<li><code>"image_id"</code> (<code>int</code>): The image id.</li>
<li><code>"annotations"</code> (<code>List[Dict]</code>): List of annotations for an image. Each annotation should be a dictionary. An image can have no annotations, in which case the list should be empty.</li>
</ul>
<p>As for the <code>"annotations"</code> field, this should be a list of dictionaries containing individual annotations in <a href="https://cocodataset.org/#format-data">COCO format</a>:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># COCO format, see: https://cocodataset.org/#format-data  </span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>[{</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"image_id"</span>: <span class="dv">42</span>,</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"annotations"</span>: [{</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">"id"</span>: <span class="dv">123456</span>,</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">"category_id"</span>: <span class="dv">1</span>,</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">"iscrowd"</span>: <span class="dv">0</span>,</span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">"segmentation"</span>: [</span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a>            [<span class="fl">42.0</span>, <span class="fl">55.6</span>, ... <span class="fl">99.3</span>, <span class="fl">102.3</span>]</span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">"image_id"</span>: <span class="dv">42</span>, <span class="co"># this matches the 'image_id' field above</span></span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">"area"</span>: <span class="fl">135381.07</span>,</span>
<span id="cb66-13"><a href="#cb66-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">"bbox"</span>: [<span class="fl">523.70</span>,</span>
<span id="cb66-14"><a href="#cb66-14" aria-hidden="true" tabindex="-1"></a>                 <span class="fl">545.09</span>,</span>
<span id="cb66-15"><a href="#cb66-15" aria-hidden="true" tabindex="-1"></a>                 <span class="fl">402.79</span>,</span>
<span id="cb66-16"><a href="#cb66-16" aria-hidden="true" tabindex="-1"></a>                 <span class="fl">336.11</span>]</span>
<span id="cb66-17"><a href="#cb66-17" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb66-18"><a href="#cb66-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Next annotation in the same format as the previous one (one annotation per dict).</span></span>
<span id="cb66-19"><a href="#cb66-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For example, if an image had 4 bounding boxes, there would be a list of 4 dictionaries</span></span>
<span id="cb66-20"><a href="#cb66-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># each containing a single annotation.</span></span>
<span id="cb66-21"><a href="#cb66-21" aria-hidden="true" tabindex="-1"></a>    ...]</span>
<span id="cb66-22"><a href="#cb66-22" aria-hidden="true" tabindex="-1"></a>}]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let’s breakdown each of the fields in the COCO annotation:</p>
<div id="tbl-coco-format" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-coco-format-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: COCO data format keys breakdown
</figcaption>
<div aria-describedby="tbl-coco-format-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Field</th>
<th>Requirement</th>
<th>Data Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>image_id</code> (top-level)</td>
<td>Required</td>
<td>Integer</td>
<td>ID of the target image.</td>
</tr>
<tr class="even">
<td><code>annotations</code></td>
<td>Required</td>
<td>List[Dict]</td>
<td>List of dictionaries with one box annotation per dict. Can be empty if there are no boxes.</td>
</tr>
<tr class="odd">
<td><code>id</code></td>
<td>Not required</td>
<td>Integer</td>
<td>ID of the particular annotation.</td>
</tr>
<tr class="even">
<td><code>category_id</code></td>
<td>Required</td>
<td>Integer</td>
<td>ID of the class the box relates to (e.g.&nbsp;<code>{0: 'bin', 1: 'hand', 2: 'not_bin', 3: 'not_hand', 4: 'not_trash', 5: 'trash'}</code>).</td>
</tr>
<tr class="odd">
<td><code>segmentation</code></td>
<td>Not required</td>
<td>List or None</td>
<td>Segmentation mask related to an annotation instance. Focus is on boxes, not segmentation.</td>
</tr>
<tr class="even">
<td><code>image_id</code> (inside <code>annotations</code> field)</td>
<td>Required</td>
<td>Integer</td>
<td>ID of the target image the particular box relates to, should match <code>image_id</code> on the top-level field.</td>
</tr>
<tr class="odd">
<td><code>area</code></td>
<td>Not required</td>
<td>Float</td>
<td>Area of the target bounding box (e.g.&nbsp;box height * width).</td>
</tr>
<tr class="even">
<td><code>bbox</code></td>
<td>Required</td>
<td>List[Float]</td>
<td>Coordinates of the target bounding box in <code>XYWH</code> (<code>[x, y, width, height]</code>) format. <code>(x, y)</code> are the top left corner coordinates, <code>width</code> and <code>height</code> are dimensions.</td>
</tr>
<tr class="odd">
<td><code>is_crowd</code></td>
<td>Not required</td>
<td>Int</td>
<td>Boolean flag (0 or 1) to indicate whether or not an object is multiple (a crowd) of the same thing. For example, a crowd of “people” or a group of “apples” rather than a single apple.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>And now our annotation data comes in the format:</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>{<span class="st">'image'</span>: <span class="op">&lt;</span>PIL.Image.Image image mode<span class="op">=</span>RGB size<span class="op">=</span><span class="dv">960</span><span class="er">x1280</span><span class="op">&gt;</span>,</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a> <span class="st">'image_id'</span>: <span class="dv">292</span>,</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a> <span class="st">'annotations'</span>: {<span class="st">'file_name'</span>: [<span class="st">'00347467-13f1-4cb9-94aa-4e4369457e0c.jpeg'</span>,</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>   <span class="st">'00347467-13f1-4cb9-94aa-4e4369457e0c.jpeg'</span>],</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">'image_id'</span>: [<span class="dv">292</span>, <span class="dv">292</span>],</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>  <span class="st">'category_id'</span>: [<span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>  <span class="st">'bbox'</span>: [[<span class="fl">523.7000122070312</span>,</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>    <span class="fl">545.0999755859375</span>,</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>    <span class="fl">402.79998779296875</span>,</span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>    <span class="fl">336.1000061035156</span>],</span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>   [<span class="fl">10.399999618530273</span>,</span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a>    <span class="fl">163.6999969482422</span>,</span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a>    <span class="fl">943.4000244140625</span>,</span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>    <span class="fl">1101.9000244140625</span>]],</span>
<span id="cb67-15"><a href="#cb67-15" aria-hidden="true" tabindex="-1"></a>  <span class="st">'iscrowd'</span>: [<span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb67-16"><a href="#cb67-16" aria-hidden="true" tabindex="-1"></a>  <span class="st">'area'</span>: [<span class="fl">135381.078125</span>, <span class="fl">1039532.4375</span>]},</span>
<span id="cb67-17"><a href="#cb67-17" aria-hidden="true" tabindex="-1"></a> <span class="st">'label_source'</span>: <span class="st">'manual_prodigy_label'</span>,</span>
<span id="cb67-18"><a href="#cb67-18" aria-hidden="true" tabindex="-1"></a> <span class="st">'image_source'</span>: <span class="st">'manual_taken_photo'</span>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>How about we write some code to convert our current annotation format to COCO format?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>It’s common practice to get a dataset in a certain format and then have to preprocess it into another format before you can use it with a model.</p>
<p>We’re getting hands-on and practicing here so when it comes to working on converting another dataset, you’ve already had some practice.</p>
</div>
</div>
</section>
<section id="creating-dataclasses-to-represent-the-coco-bounding-box-format" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="creating-dataclasses-to-represent-the-coco-bounding-box-format"><span class="header-section-number">9.3</span> Creating dataclasses to represent the COCO bounding box format</h3>
<p>Let’s write some code to transform our existing annotation data into the format required by <code>transformers.RTDetrImageProcessor.preprocess</code>.</p>
<p>We’ll start by creating two <a href="https://docs.python.org/3/library/dataclasses.html#module-dataclasses">Python dataclasses</a> to house our desired COCO annotation format.</p>
<p>To do this we’ll:</p>
<ol type="1">
<li>Create <code>SingleCOCOAnnotation</code> which contains the format structure of a single COCO annotation.</li>
<li>Create <code>ImageCOCOAnnotations</code> which contains all of the annotations for a given image in COCO format. This may be a single instance of <code>SingleCOCOAnnotation</code> or multiple.</li>
</ol>
<p>We’ll decorate both of these with the <a href="https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass"><code>@dataclass</code></a> decorator.</p>
<p>Using a <code>@dataclass</code> gives several benefits:</p>
<ul>
<li>Type hints - we can define the types of objects we want in the class definition, for example, we want <code>image_id</code> to be an <code>int</code>.</li>
<li>Helpful built-in methods - we can use methods such as <a href="https://docs.python.org/3/library/dataclasses.html#dataclasses.asdict"><code>asdict</code></a> to convert our <code>@dataclass</code> into a dictionary (COCO wants lists of dictionaries).</li>
<li>Data validation - we can use methods such as <a href="https://docs.python.org/3/library/dataclasses.html#dataclasses.__post_init__"><code>__post_init__</code></a> to run checks on our <code>@dataclass</code> as it’s initialized, for example, we always want the length of <code>bbox</code> to be 4 (bounding box coordinates in <code>XYWH</code> format).</li>
</ul>
<div id="cell-95" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass, asdict</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List, Tuple</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Create a dataclass for a single COCO annotation</span></span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SingleCOCOAnnotation:</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""An instance of a single COCO annotation. </span></span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Represent a COCO-formatted (see: https://cocodataset.org/#format-data) single instance of an object </span></span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a><span class="co">    in an image. </span></span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Attributes:</span></span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a><span class="co">        image_id: Unique integer identifier for the image which the annotation belongs to.</span></span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a><span class="co">        category_id: Integer identifier for the target object label/category (e.g. "0" for "bin").</span></span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a><span class="co">        bbox: List of floats containing target bounding box coordinates in absolute XYWH format ([x_top_left, y_top_left, width, height]).</span></span>
<span id="cb68-16"><a href="#cb68-16" aria-hidden="true" tabindex="-1"></a><span class="co">        area: Area of the target bounding box. Defaults to 0.0.</span></span>
<span id="cb68-17"><a href="#cb68-17" aria-hidden="true" tabindex="-1"></a><span class="co">        iscrowd: Boolean flag (0 or 1) indicating whether the target is a crowd of objects, for example, a group of </span></span>
<span id="cb68-18"><a href="#cb68-18" aria-hidden="true" tabindex="-1"></a><span class="co">            apples rather than a single apple. Defaults to 0.</span></span>
<span id="cb68-19"><a href="#cb68-19" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb68-20"><a href="#cb68-20" aria-hidden="true" tabindex="-1"></a>    image_id: <span class="bu">int</span></span>
<span id="cb68-21"><a href="#cb68-21" aria-hidden="true" tabindex="-1"></a>    category_id: <span class="bu">int</span></span>
<span id="cb68-22"><a href="#cb68-22" aria-hidden="true" tabindex="-1"></a>    bbox: List[<span class="bu">float</span>] <span class="co"># bboxes in XYWH format ([x_top_left, y_top_left, width, height])</span></span>
<span id="cb68-23"><a href="#cb68-23" aria-hidden="true" tabindex="-1"></a>    area: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb68-24"><a href="#cb68-24" aria-hidden="true" tabindex="-1"></a>    iscrowd: <span class="bu">int</span> <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb68-25"><a href="#cb68-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-26"><a href="#cb68-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make sure the bbox is always a list of 4 values (XYWH format)</span></span>
<span id="cb68-27"><a href="#cb68-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> __post_init__(<span class="va">self</span>):</span>
<span id="cb68-28"><a href="#cb68-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.bbox) <span class="op">!=</span> <span class="dv">4</span>:</span>
<span id="cb68-29"><a href="#cb68-29" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"bbox must contain exactly 4 values, current length: </span><span class="sc">{</span><span class="bu">len</span>(<span class="va">self</span>.bbox)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb68-30"><a href="#cb68-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-31"><a href="#cb68-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-32"><a href="#cb68-32" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Create a dataclass for a collection of COCO annotations for a single image</span></span>
<span id="cb68-33"><a href="#cb68-33" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb68-34"><a href="#cb68-34" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ImageCOCOAnnotations:</span>
<span id="cb68-35"><a href="#cb68-35" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A collection of COCO annotations for a single image_id.</span></span>
<span id="cb68-36"><a href="#cb68-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-37"><a href="#cb68-37" aria-hidden="true" tabindex="-1"></a><span class="co">    Attributes:</span></span>
<span id="cb68-38"><a href="#cb68-38" aria-hidden="true" tabindex="-1"></a><span class="co">        image_id: Unique integer identifier for the image which the annotations belong to.</span></span>
<span id="cb68-39"><a href="#cb68-39" aria-hidden="true" tabindex="-1"></a><span class="co">        annotations: List of SingleCOCOAnnotation instances.</span></span>
<span id="cb68-40"><a href="#cb68-40" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb68-41"><a href="#cb68-41" aria-hidden="true" tabindex="-1"></a>    image_id: <span class="bu">int</span></span>
<span id="cb68-42"><a href="#cb68-42" aria-hidden="true" tabindex="-1"></a>    annotations: List[SingleCOCOAnnotation]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Beautiful!</p>
<p>Let’s now inspect our <code>SingleCOCOAnnotation</code> dataclass.</p>
<p>We can use the <code>SingleCOCOAnnotation?</code> syntax to view the docstring of the class.</p>
<div id="cell-97" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># One of the benefits of using a dataclass is that we can inspect the attributes with the `?` syntax</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>SingleCOCOAnnotation?</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Init signature:
SingleCOCOAnnotation(
    image_id: int,
    category_id: int,
    bbox: List[float],
    area: float = 0.0,
    iscrowd: int = 0,
) -&gt; None
Docstring:     
An instance of a single COCO annotation. 

Represent a COCO-formatted (see: https://cocodataset.org/#format-data) single instance of an object 
in an image. 

Attributes:
    image_id: Unique integer identifier for the image which the annotation belongs to.
    category_id: Integer identifier for the target object label/category (e.g. "0" for "bin").
    bbox: List of floats containing target bounding box coordinates in absolute XYWH format ([x_top_left, y_top_left, width, height]).
    area: Area of the target bounding box. Defaults to 0.0.
    iscrowd: Boolean flag (0 or 1) indicating whether the target is a crowd of objects, for example, a group of 
        apples rather than a single apple. Defaults to 0.
Type:           type
Subclasses:     </code></pre>
</div>
</div>
<p>We can also see the error handling of our <code>__post_init__</code> method in action by trying to create an instance of <code>SingleCOCOAnnotation</code> with an incorrect number of bbox values.</p>
<div id="cell-99" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's try our SingleCOCOAnnotation dataclass (this will error since the bbox doesn't have 4 values)</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>SingleCOCOAnnotation(image_id<span class="op">=</span><span class="dv">42</span>, </span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>                     category_id<span class="op">=</span><span class="dv">0</span>, </span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>                     bbox<span class="op">=</span>[<span class="dv">100</span>, <span class="dv">100</span>, <span class="dv">100</span>]) <span class="co"># missing a 4th value</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[35], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Let's try our SingleCOCOAnnotation dataclass (this will error since the bbox doesn't have 4 values)</span>
<span class="ansi-green-fg">----&gt; 2</span> <span class="ansi-yellow-bg">SingleCOCOAnnotation</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">image_id</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">42</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span>
<span class="ansi-green-fg ansi-bold">      3</span> <span class="ansi-yellow-bg">                     </span><span class="ansi-yellow-bg">category_id</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">0</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span>
<span class="ansi-green-fg ansi-bold">      4</span> <span class="ansi-yellow-bg">                     </span><span class="ansi-yellow-bg">bbox</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">[</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">100</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">100</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">100</span><span class="ansi-yellow-bg">]</span><span class="ansi-yellow-bg">)</span> <span style="font-style:italic;color:rgb(95,135,135)"># missing a 4th value</span>

File <span class="ansi-green-fg">&lt;string&gt;:8</span>, in <span class="ansi-cyan-fg">__init__</span><span class="ansi-blue-fg">(self, image_id, category_id, bbox, area, iscrowd)</span>

Cell <span class="ansi-green-fg">In[33], line 29</span>, in <span class="ansi-cyan-fg">SingleCOCOAnnotation.__post_init__</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-fg ansi-bold">     27</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span> <span style="color:rgb(0,0,255)">__post_init__</span>(<span style="color:rgb(0,135,0)">self</span>):
<span class="ansi-green-fg ansi-bold">     28</span>     <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="color:rgb(0,135,0)">len</span>(<span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>bbox) <span style="color:rgb(98,98,98)">!=</span> <span style="color:rgb(98,98,98)">4</span>:
<span class="ansi-green-fg">---&gt; 29</span>         <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">ValueError</span>(<span style="color:rgb(175,0,0)">f</span><span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">bbox must contain exactly 4 values, current length: </span><span style="font-weight:bold;color:rgb(175,95,135)">{</span><span style="color:rgb(0,135,0)">len</span>(<span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>bbox)<span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="color:rgb(175,0,0)">"</span>)

<span class="ansi-red-fg">ValueError</span>: bbox must contain exactly 4 values, current length: 3</pre>
</div>
</div>
</div>
<p>And now if we pass the correct number of values to our <code>SingleCOCOAnnotation</code>, it should work.</p>
<div id="cell-101" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>SingleCOCOAnnotation(image_id<span class="op">=</span><span class="dv">42</span>, </span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>                     category_id<span class="op">=</span><span class="dv">0</span>, </span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>                     bbox<span class="op">=</span>[<span class="dv">100</span>, <span class="dv">100</span>, <span class="dv">100</span>, <span class="dv">100</span>]) <span class="co"># correct number of values</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>SingleCOCOAnnotation(image_id=42, category_id=0, bbox=[100, 100, 100, 100], area=0.0, iscrowd=0)</code></pre>
</div>
</div>
</section>
<section id="creating-a-function-to-format-our-annotations-as-coco-format" class="level3" data-number="9.4">
<h3 data-number="9.4" class="anchored" data-anchor-id="creating-a-function-to-format-our-annotations-as-coco-format"><span class="header-section-number">9.4</span> Creating a function to format our annotations as COCO format</h3>
<p>Now we’ve got the COCO data format in our <code>SingleCOCOAnnotation</code> and <code>ImageCOCOAnnotation</code> dataclasses, let’s write a function to take our existing image annotations and format them in COCO style.</p>
<p>Our <code>format_image_annotations_as_coco</code> function will:</p>
<ol type="1">
<li>Take in an <code>image_id</code> to represent a unique identifier for the image as well as lists of category integers, area values and bounding box coordinates.</li>
<li>Perform a list comprehension on a zipped version of each category, area and bounding box coordinate value in the input lists creating an instance of <code>SingleCOCOAnnotation</code> as a dictionary (using the <code>asdict</code> method) each time, this will give us a list of <code>SingleCOCOAnnotation</code> formatted dictionaries.</li>
<li>Return a dictionary version of <code>ImageCOCOAnnotations</code> using <code>asdict</code> passing it the <code>image_id</code> as well as list of <code>SingleCOCOAnnotation</code> dictionaries from 2.</li>
</ol>
<p>Why does our function take in lists of categories, areas and bounding boxes?</p>
<p>Because that’s the current format our existing annotations are in (how we downloaded them from Hugging Face in the beginning).</p>
<p>Let’s do it!</p>
<div id="cell-103" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Take in a unique image_id as well as lists of categories, areas, and bounding boxes</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> format_image_annotations_as_coco(</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>        image_id: <span class="bu">int</span>,</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>        categories: List[<span class="bu">int</span>],</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>        areas: List[<span class="bu">float</span>],</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>        bboxes: List[Tuple[<span class="bu">float</span>, <span class="bu">float</span>, <span class="bu">float</span>, <span class="bu">float</span>]] <span class="co"># bboxes in XYWH format ([x_top_left, y_top_left, width, height])</span></span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="bu">dict</span>:</span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Formats lists of image annotations into COCO format.</span></span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Takes in parallel lists of categories, areas, and bounding boxes and</span></span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a><span class="co">    then formats them into a COCO-style dictionary of annotations.</span></span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a><span class="co">        image_id: Unique integer identifier for an image.</span></span>
<span id="cb74-15"><a href="#cb74-15" aria-hidden="true" tabindex="-1"></a><span class="co">        categories: List of integer category IDs for each annotation.</span></span>
<span id="cb74-16"><a href="#cb74-16" aria-hidden="true" tabindex="-1"></a><span class="co">        areas: List of float areas for each annotation.</span></span>
<span id="cb74-17"><a href="#cb74-17" aria-hidden="true" tabindex="-1"></a><span class="co">        bboxes: List of tuples containing bounding box coordinates in XYWH format </span></span>
<span id="cb74-18"><a href="#cb74-18" aria-hidden="true" tabindex="-1"></a><span class="co">            ([x_top_left, y_top_left, width, height]).</span></span>
<span id="cb74-19"><a href="#cb74-19" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb74-20"><a href="#cb74-20" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb74-21"><a href="#cb74-21" aria-hidden="true" tabindex="-1"></a><span class="co">        A dictionary of image annotations in COCO format with the following structure:</span></span>
<span id="cb74-22"><a href="#cb74-22" aria-hidden="true" tabindex="-1"></a><span class="co">        {</span></span>
<span id="cb74-23"><a href="#cb74-23" aria-hidden="true" tabindex="-1"></a><span class="co">            "image_id": int,</span></span>
<span id="cb74-24"><a href="#cb74-24" aria-hidden="true" tabindex="-1"></a><span class="co">            "annotations": [</span></span>
<span id="cb74-25"><a href="#cb74-25" aria-hidden="true" tabindex="-1"></a><span class="co">                {</span></span>
<span id="cb74-26"><a href="#cb74-26" aria-hidden="true" tabindex="-1"></a><span class="co">                    "image_id": int,</span></span>
<span id="cb74-27"><a href="#cb74-27" aria-hidden="true" tabindex="-1"></a><span class="co">                    "category_id": int,</span></span>
<span id="cb74-28"><a href="#cb74-28" aria-hidden="true" tabindex="-1"></a><span class="co">                    "bbox": List[float],</span></span>
<span id="cb74-29"><a href="#cb74-29" aria-hidden="true" tabindex="-1"></a><span class="co">                    "area": float</span></span>
<span id="cb74-30"><a href="#cb74-30" aria-hidden="true" tabindex="-1"></a><span class="co">                },</span></span>
<span id="cb74-31"><a href="#cb74-31" aria-hidden="true" tabindex="-1"></a><span class="co">                ...more annotations here</span></span>
<span id="cb74-32"><a href="#cb74-32" aria-hidden="true" tabindex="-1"></a><span class="co">            ]</span></span>
<span id="cb74-33"><a href="#cb74-33" aria-hidden="true" tabindex="-1"></a><span class="co">        }</span></span>
<span id="cb74-34"><a href="#cb74-34" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb74-35"><a href="#cb74-35" aria-hidden="true" tabindex="-1"></a><span class="co">    Note:</span></span>
<span id="cb74-36"><a href="#cb74-36" aria-hidden="true" tabindex="-1"></a><span class="co">        All input lists much be the same length and in the same order.</span></span>
<span id="cb74-37"><a href="#cb74-37" aria-hidden="true" tabindex="-1"></a><span class="co">        Otherwise, there will be mismatched annotations.</span></span>
<span id="cb74-38"><a href="#cb74-38" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb74-39"><a href="#cb74-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb74-40"><a href="#cb74-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Turn input lists into a list of dicts in SingleCOCOAnnotation format</span></span>
<span id="cb74-41"><a href="#cb74-41" aria-hidden="true" tabindex="-1"></a>    coco_format_annotations <span class="op">=</span> [</span>
<span id="cb74-42"><a href="#cb74-42" aria-hidden="true" tabindex="-1"></a>        asdict(SingleCOCOAnnotation(</span>
<span id="cb74-43"><a href="#cb74-43" aria-hidden="true" tabindex="-1"></a>            image_id<span class="op">=</span>image_id,</span>
<span id="cb74-44"><a href="#cb74-44" aria-hidden="true" tabindex="-1"></a>            category_id<span class="op">=</span>category,</span>
<span id="cb74-45"><a href="#cb74-45" aria-hidden="true" tabindex="-1"></a>            bbox<span class="op">=</span><span class="bu">list</span>(bbox),</span>
<span id="cb74-46"><a href="#cb74-46" aria-hidden="true" tabindex="-1"></a>            area<span class="op">=</span>area,</span>
<span id="cb74-47"><a href="#cb74-47" aria-hidden="true" tabindex="-1"></a>        ))</span>
<span id="cb74-48"><a href="#cb74-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> category, area, bbox <span class="kw">in</span> <span class="bu">zip</span>(categories, areas, bboxes)</span>
<span id="cb74-49"><a href="#cb74-49" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb74-50"><a href="#cb74-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-51"><a href="#cb74-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Return a of annotations with format {"image_id": ..., "annotations": [...]} (required COCO format)</span></span>
<span id="cb74-52"><a href="#cb74-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> asdict(ImageCOCOAnnotations(image_id<span class="op">=</span>image_id,</span>
<span id="cb74-53"><a href="#cb74-53" aria-hidden="true" tabindex="-1"></a>                                       annotations<span class="op">=</span>coco_format_annotations))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Nice!</p>
<p>Having those pre-built dataclasses makes everything else fall into place.</p>
<p>Now let’s try our <code>format_image_annotations_as_coco</code> function on a new <em>not so</em> <code>random_sample</code> (we’ll make a <code>random_sample</code> with a known index for reproducibility).</p>
<p>First, we’ll remind ourselves what our <code>random_sample</code> looks like.</p>
<div id="cell-105" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a not so random sample and inspect it </span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>random_sample <span class="op">=</span> dataset[<span class="st">"train"</span>][<span class="dv">77</span>]</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>random_sample</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>{'image': &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,
 'image_id': 558,
 'annotations': {'file_name': ['13df7e4a-1a5c-4da9-a5d3-204b6119670c.jpeg',
   '13df7e4a-1a5c-4da9-a5d3-204b6119670c.jpeg',
   '13df7e4a-1a5c-4da9-a5d3-204b6119670c.jpeg'],
  'image_id': [558, 558, 558],
  'category_id': [5, 0, 1],
  'bbox': [[261.8999938964844, 734.5, 181.8000030517578, 216.3000030517578],
   [99.80000305175781, 215.1999969482422, 730.0, 685.7999877929688],
   [0.0, 769.2999877929688, 367.8999938964844, 508.70001220703125]],
  'iscrowd': [0, 0, 0],
  'area': [39323.33984375, 500634.0, 187150.734375]},
 'label_source': 'manual_prodigy_label',
 'image_source': 'manual_taken_photo'}</code></pre>
</div>
</div>
<p>Ok wonderful, looks like we can extract the <code>image_id</code>, <code>category_id</code> <code>bbox</code> and <code>area</code> fields from our <code>random_sample</code> to get the required inputs to our <code>format_image_annotations_as_coco</code> function.</p>
<p>Let’s try it out.</p>
<div id="cell-107" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract image_id, categories, areas, and bboxes from the random sample</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>random_sample_image_id <span class="op">=</span> random_sample[<span class="st">"image_id"</span>]</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>random_sample_categories <span class="op">=</span> random_sample[<span class="st">"annotations"</span>][<span class="st">"category_id"</span>]</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>random_sample_areas <span class="op">=</span> random_sample[<span class="st">"annotations"</span>][<span class="st">"area"</span>]</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>random_sample_bboxes <span class="op">=</span> random_sample[<span class="st">"annotations"</span>][<span class="st">"bbox"</span>]</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Format the random sample annotations as COCO format</span></span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>random_sample_coco_annotations <span class="op">=</span> format_image_annotations_as_coco(image_id<span class="op">=</span>random_sample_image_id,</span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>                                                                  categories<span class="op">=</span>random_sample_categories,</span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a>                                                                  areas<span class="op">=</span>random_sample_areas,</span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a>                                                                  bboxes<span class="op">=</span>random_sample_bboxes)</span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a>random_sample_coco_annotations</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>{'image_id': 558,
 'annotations': [{'image_id': 558,
   'category_id': 5,
   'bbox': [261.8999938964844, 734.5, 181.8000030517578, 216.3000030517578],
   'area': 39323.33984375,
   'iscrowd': 0},
  {'image_id': 558,
   'category_id': 0,
   'bbox': [99.80000305175781, 215.1999969482422, 730.0, 685.7999877929688],
   'area': 500634.0,
   'iscrowd': 0},
  {'image_id': 558,
   'category_id': 1,
   'bbox': [0.0, 769.2999877929688, 367.8999938964844, 508.70001220703125],
   'area': 187150.734375,
   'iscrowd': 0}]}</code></pre>
</div>
</div>
<p>Woohoo!</p>
<p>Looks like we may have just fixed our <code>ValueError</code> from before:</p>
<blockquote class="blockquote">
<p>ValueError: Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts (batch of images) with the following keys: <code>image_id</code> and <code>annotations</code>, with the latter being a list of annotations in the COCO format.</p>
</blockquote>
<p>Our COCO formatted annotations have the <code>image_id</code> and <code>annotations</code> keys and our <code>annotations</code> are a list of annotations in COCO format.</p>
<p>Perfect!</p>
</section>
<section id="preprocess-a-single-image-and-set-of-coco-format-annotations" class="level3" data-number="9.5">
<h3 data-number="9.5" class="anchored" data-anchor-id="preprocess-a-single-image-and-set-of-coco-format-annotations"><span class="header-section-number">9.5</span> Preprocess a single image and set of COCO format annotations</h3>
<p>Now we’ve preprocessed our annotations to be in COCO format, we can use them with <a href="https://huggingface.co/docs/transformers/en/model_doc/rt_detr#transformers.RTDetrImageProcessor.preprocess"><code>transformers.RTDetrImageProcessor.preprocess</code></a>.</p>
<p>Let’s pass our <code>random_sample</code> image and COCO formatted annotations to the <code>preprocess</code> method.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The default value for the <a href="https://huggingface.co/docs/transformers/en/model_doc/rt_detr#transformers.RTDetrImageProcessorFast.preprocess.do_convert_annotations">parameter <code>do_convert_annotations</code></a> of the <code>preprocess</code> method is <code>True</code>.</p>
<p>This means our boxes will go into the <code>preprocess</code> method in absolute <code>XYWH</code> format (the format we downloaded them in) and will be returned in normalized <code>CXCYWH</code> (or <code>(center_x, center_y, width, height)</code>) format.</p>
<p>Whenever you perform adjustments or preprocessing steps on your annotations, it’s always good to keep track of the format that they are in, otherwise it can lead to unexpected bugs later on.</p>
</div>
</div>
<div id="cell-110" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocess random sample image and assosciated annotations</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>random_sample_preprocessed <span class="op">=</span> image_processor.preprocess(images<span class="op">=</span>random_sample[<span class="st">"image"</span>],</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>                                                        annotations<span class="op">=</span>random_sample_coco_annotations,</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>                                                        do_convert_annotations<span class="op">=</span><span class="va">True</span>, <span class="co"># defaults to True, this will convert our annotations to normalized CXCYWH format</span></span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>                                                        return_tensors<span class="op">=</span><span class="st">"pt"</span> <span class="co"># can return as tensors or not, "pt" returns as PyTorch tensors</span></span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>                                                        ) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>When processing our single image and annotation, you may see a warning similar to the following:</p>
<blockquote class="blockquote">
<p>The <code>max_size</code> parameter is deprecated and will be removed in v4.26. Please specify in <code>size['longest_edge'] instead</code>.</p>
</blockquote>
<p>If you are not using the <code>max_size</code> parameter and are using a version of <code>transformers</code> &gt; 4.26, you can ignore this or disable it (as shown below).</p>
</div>
</div>
<div id="cell-112" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Optional: Disable warnings about `max_size` parameter being deprecated</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>, message<span class="op">=</span><span class="st">"The `max_size` parameter is deprecated*"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Excellent!</p>
<p>It looks like the <code>preprocess</code> method worked on our single sample.</p>
<p>Let’s inspect the <code>keys()</code> method of our <code>random_sample_preprocessed</code>.</p>
<div id="cell-114" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the keys of our preprocessed example</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>random_sample_preprocessed.keys()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="42">
<pre><code>dict_keys(['pixel_mask', 'pixel_values', 'labels'])</code></pre>
</div>
</div>
<p>Wonderful, we get a preprocessed image and labels:</p>
<ul>
<li><code>pixel_values</code> = preprocessed pixels (the preprocessed image).</li>
<li>(Optional) <code>pixel_mask</code> = whether or not to mask the pixels (e.g.&nbsp;0 = mask, 1 = no mask, in our case, all values will be <code>1</code> since we want the model to see all pixels).</li>
<li><code>labels</code> = preprocessed labels (the preprocessed annotations).</li>
</ul>
<div id="cell-116" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect preprocessed image shape</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Preprocessed image shape: </span><span class="sc">{</span>random_sample_preprocessed[<span class="st">'pixel_values'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [batch_size, colour_channels, height, width]"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Preprocessed image shape: torch.Size([1, 3, 640, 480]) -&gt; [batch_size, colour_channels, height, width]</code></pre>
</div>
</div>
<p>Since we only passed a single sample to <code>preprocess</code>, we get back a batch size of 1.</p>
<p>Now how do our labels look?</p>
<div id="cell-118" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect the preprocessed labels (our boxes and other metadata)</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>pprint(random_sample_preprocessed[<span class="st">"labels"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[{'area': tensor([  9830.8350, 125158.5000,  46787.6836]),
  'boxes': tensor([[0.3675, 0.6583, 0.1894, 0.1690],
        [0.4842, 0.4360, 0.7604, 0.5358],
        [0.1916, 0.7997, 0.3832, 0.3974]]),
  'class_labels': tensor([5, 0, 1]),
  'image_id': tensor([558]),
  'iscrowd': tensor([0, 0, 0]),
  'orig_size': tensor([1280,  960]),
  'size': tensor([640, 480])}]</code></pre>
</div>
</div>
<p>Let’s break this down:</p>
<ul>
<li><code>area</code> - An array/tensor of floats containing the area (<code>box_width * box_height</code>) of our boxes.</li>
<li><code>boxes</code> - An array/tensor containing all of the bounding boxes for our image in normalized <code>CXCYWH</code> (<code>(center_x, center_y, width, height)</code>) format.</li>
<li><code>class_labels</code> - An array/tensor of integer labels assosciated with each box (e.g.&nbsp;<code>tensor([5, 1, 0, 0, 4])</code> -&gt; <code>['trash', 'hand', 'bin', 'bin', 'not_trash']</code>).</li>
<li><code>image_id</code> - A unique integer identifier for our target image.</li>
<li><code>is_crowd</code> - An array/tensor of a boolean value (0 or 1) for whether an annotation is a group or not.</li>
<li><code>orig_size</code> - An array/tensor containing the original size in <code>(height, width)</code> format (this is important for drawing conversion factors when using originally sized images).</li>
<li><code>size</code> - An array/tensor with the current size in <code>(height, width)</code> format of the processed image tensor contained within <code>random_sample_preprocessed["pixel_values"]</code>.</li>
</ul>
<p>Woohoo!</p>
<p>We’ve done it!</p>
<p>We’ve officially preprocessed a single sample of our own data, both the image and its annotation pair.</p>
<p>We’ll write some code later on to scale this up to our whole dataset.</p>
<p>For now, let’s see what it looks like postprocessing a single output.</p>
</section>
</section>
<section id="postprocessing-a-single-output" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="postprocessing-a-single-output"><span class="header-section-number">10</span> Postprocessing a single output</h2>
<p>We’ve got our inputs processed and successfully passed them through our model.</p>
<p>How about we postprocess the outputs of our model?</p>
<p>Doing so will make our model’s outputs far more usable.</p>
</section>
<section id="going-end-to-end-on-a-single-sample" class="level2 {callout-tip}" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="going-end-to-end-on-a-single-sample"><span class="header-section-number">11</span> Going end-to-end on a single sample</h2>
<p>When working on a new problem or with a custom dataset and an existing model, it’s good practice to go end-to-end on a single sample.</p>
<p>For example, preprocess one of your samples, pass it through the model and then postprocess it (just like we’re in the middle of doing here).</p>
<p>Being able to go end-to-end on a single sample will help you see the overall process and discover any bugs that may hinder you later on.</p>
</section>
<p>To postprocess the outputs of our model we can use the <a href="https://huggingface.co/docs/transformers/en/model_doc/rt_detr#transformers.RTDetrImageProcessor.post_process_object_detection"><code>transformers.RTDetrImageProcessor.post_process_object_detection()</code></a> method (see the <a href="https://github.com/huggingface/transformers/blob/237c7c356cce5f8a6befc0a5f71bc952bb5adc9a/src/transformers/models/rt_detr/image_processing_rt_detr.py#L1029">source code on GitHub</a>, this is what we’ll reproduce by hand).</p>
<p>Let’s frist recompute the model’s outputs for our preprocessed single sample.</p>
<div id="cell-121" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Recompute the random sample outputs with our preprocessed sample</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>random_sample_outputs <span class="op">=</span> model(</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>    pixel_values<span class="op">=</span>random_sample_preprocessed[<span class="st">"pixel_values"</span>], <span class="co"># model expects input [batch_size, color_channels, height, width]</span></span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># pixel_mask=random_sample_preprocessed["pixel_mask"], # optional: some models expect pixel_mask inputs</span></span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect the output type</span></span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a><span class="bu">type</span>(random_sample_outputs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>transformers.models.rt_detr_v2.modeling_rt_detr_v2.RTDetrV2ObjectDetectionOutput</code></pre>
</div>
</div>
<p>Wonderful!</p>
<p>We get the exact output our <code>post_process_object_detection()</code> method is looking for.</p>
<p>Now we can fill in the following parameters:</p>
<ul>
<li><code>outputs</code> - Raw outputs of the model (for us, this is <code>random_sample_outputs</code>).</li>
<li><code>threshold</code> - A float score value to keep or discard boxes (e.g.&nbsp;<code>threshold=0.3</code> means all boxes under <code>0.3</code> will be discarded). This value can be adjusted as needed. A higher value means only the boxes the model is most confident on will be kept. A lower value means more boxes will be kept, however, these may be over lower quality. Best to be experimented with.</li>
<li><code>target_sizes</code> - Size of target image in <code>(height, width)</code> format for bounding boxes. For example, if our image is 960 pixels wide by 1280 high, we could pass in <code>[1280, 960]</code>. Number of <code>target_sizes</code> must match number of <code>outputs</code>. For example, if pass in 1 set of outputs, only 1 <code>target_sizes</code> is needed. If we pass in a batch of 32 <code>outputs</code>, 32 <code>target_sizes</code> are required, else it will error. If <code>None</code>, postprocessed outputs won’t be resized (this can be lead to poor looking boxes as the coordinates don’t match your image).</li>
<li><code>top_k</code> - Integer defining the number of boxes you’d like to prepare for postprocessing before thresholding. Defaults to <code>100</code>. For example, <code>top_k=100</code> and <code>threshold=0.3</code> means sample 100 boxes and then of those 100 boxes, only keep those with a score over 0.3.</li>
</ul>
<p>You can see what happens behind the scenes of <code>post_process_object_detection</code> in the <a href="https://github.com/huggingface/transformers/blob/5f4ecf2d9f867a1255131d2461d75793c0cf1db2/src/transformers/models/rt_detr/image_processing_rt_detr_fast.py#L533">source code</a>.</p>
<div id="cell-123" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the score threshold for postprocessing</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>THRESHOLD <span class="op">=</span> <span class="fl">0.4</span> <span class="co"># adjust this where necessary to get a handful of outputs below (note: if it's too high, e.g. 0.5+, you might not see any outputs, try lowering to 0.3</span></span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Post process a single output from our model</span></span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>random_sample_outputs_post_processed <span class="op">=</span> image_processor.post_process_object_detection(</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>random_sample_outputs,</span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>    threshold<span class="op">=</span>THRESHOLD, <span class="co"># all boxes with scores under this value will be discarded (best to experiment with it)</span></span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a>    target_sizes<span class="op">=</span>random_sample_preprocessed[<span class="st">"labels"</span>][<span class="dv">0</span>][<span class="st">"orig_size"</span>].unsqueeze(<span class="dv">0</span>) <span class="co"># original input image size (or whichever target size you'd like), required to be same number of input items in a list</span></span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a>random_sample_outputs_post_processed</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>[{'scores': tensor([0.6736, 0.5951, 0.5918, 0.5854, 0.5757, 0.5403, 0.5364, 0.5363, 0.5268,
          0.5200, 0.5069, 0.5045, 0.5005, 0.4951, 0.4890, 0.4884, 0.4857, 0.4854,
          0.4828, 0.4814, 0.4808, 0.4788, 0.4780, 0.4746, 0.4729, 0.4649, 0.4629,
          0.4608, 0.4608, 0.4606, 0.4598, 0.4580, 0.4577, 0.4506, 0.4504, 0.4500,
          0.4498, 0.4478, 0.4476, 0.4469, 0.4465, 0.4448, 0.4432, 0.4431, 0.4418,
          0.4417, 0.4393, 0.4378, 0.4377, 0.4374, 0.4372, 0.4365, 0.4348, 0.4332,
          0.4316, 0.4316, 0.4290, 0.4287, 0.4270, 0.4257, 0.4256, 0.4249, 0.4239,
          0.4238, 0.4232, 0.4229, 0.4227, 0.4226, 0.4224, 0.4202, 0.4192, 0.4178,
          0.4178, 0.4160, 0.4154, 0.4151, 0.4147, 0.4145, 0.4122, 0.4107, 0.4101,
          0.4099, 0.4098, 0.4088, 0.4087, 0.4087, 0.4081, 0.4066, 0.4056, 0.4045,
          0.4041, 0.4037, 0.4016, 0.4015, 0.4013, 0.4003],
         grad_fn=&lt;IndexBackward0&gt;),
  'labels': tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
          3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
          3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
          3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]),
  'boxes': tensor([[ 5.0105e+02,  4.5203e-01,  7.3708e+02,  2.4250e+02],
          [ 5.0412e+02,  2.0059e+00,  7.3278e+02,  1.8306e+02],
          [ 6.1310e+02,  2.3096e+02,  6.6116e+02,  3.0177e+02],
          [ 5.0306e+02,  1.2722e+00,  9.5733e+02,  2.2517e+02],
          [ 2.4945e+01,  3.0436e+02,  5.1925e+01,  3.4591e+02],
          [ 5.1260e+02,  6.8830e-01,  9.4725e+02,  6.6806e+01],
          [ 6.1350e+02,  2.2984e+02,  7.0224e+02,  3.0797e+02],
          [ 2.1152e+01, -3.2943e-01,  9.5351e+02,  1.4764e+02],
          [ 1.2980e+01, -3.1096e+00,  9.5033e+02,  3.1315e+02],
          [ 4.9968e+02,  2.2245e+02,  6.2810e+02,  2.4664e+02],
          [ 5.0749e+02,  2.2809e+02,  6.5957e+02,  3.1164e+02],
          [ 1.2229e+02,  6.6094e+02,  2.4567e+02,  7.9839e+02],
          [ 5.0148e+02,  1.3410e+01,  6.8456e+02,  2.0459e+02],
          [-7.3895e-01,  1.1855e+03,  9.6912e+01,  1.2788e+03],
          [ 1.4761e+01,  2.3338e+02,  5.6067e+01,  3.4622e+02],
          [ 5.1296e+02,  8.8423e-01,  7.2169e+02,  4.5847e+01],
          [ 5.0691e+02,  2.2909e+02,  6.6413e+02,  3.0541e+02],
          [ 4.6397e+02,  2.8281e+02,  5.1125e+02,  3.8849e+02],
          [ 4.8981e+02,  4.0450e+02,  6.8503e+02,  4.3418e+02],
          [ 5.1942e+01,  2.6697e+02,  8.3528e+01,  3.4009e+02],
          [ 7.2787e+02,  7.8234e+00,  8.1279e+02,  8.5145e+01],
          [ 4.7700e+02,  4.0306e+02,  6.1262e+02,  4.3070e+02],
          [ 6.1664e+02,  2.3159e+02,  6.5600e+02,  2.9366e+02],
          [ 4.1879e+02,  3.8814e+02,  5.8851e+02,  4.2226e+02],
          [ 8.2972e+02,  1.2125e+03,  8.8315e+02,  1.2791e+03],
          [ 5.2999e+02,  5.5491e+00,  6.4846e+02,  7.5497e+01],
          [ 6.1557e+02,  2.1974e+02,  6.7133e+02,  3.2138e+02],
          [ 8.0869e+02,  3.6362e+02,  8.6213e+02,  4.3602e+02],
          [ 5.3635e+02,  1.1489e+02,  9.3082e+02,  1.7906e+02],
          [ 3.0467e+02,  1.2376e+00,  5.0685e+02,  2.4923e+02],
          [-1.6477e+02, -2.1853e+02,  2.1610e+02,  3.0970e+02],
          [ 2.2749e+00,  4.8322e+02,  1.0236e+02,  6.5523e+02],
          [ 7.5257e+02,  1.2061e+03,  8.4215e+02,  1.2773e+03],
          [ 6.1513e+02,  2.4225e+02,  6.6253e+02,  3.0529e+02],
          [ 4.8960e+02,  2.9996e+02,  5.3337e+02,  3.8242e+02],
          [ 8.0622e+02,  3.5765e+02,  8.8220e+02,  4.3886e+02],
          [ 5.7967e+02,  3.7081e+02,  6.8958e+02,  3.8849e+02],
          [ 4.9100e+02,  2.6876e+02,  5.3557e+02,  3.6860e+02],
          [ 1.4141e+02,  7.6316e+02,  3.2067e+02,  8.1022e+02],
          [ 2.2179e+02,  5.6167e+02,  3.8142e+02,  8.0643e+02],
          [ 1.9144e+02,  8.6899e+02,  3.5675e+02,  9.4008e+02],
          [ 8.4778e-01,  1.7512e+02,  2.7257e+01,  2.4508e+02],
          [ 6.2010e+02,  3.6776e+02,  7.0309e+02,  3.8403e+02],
          [ 5.1821e+02,  3.7700e+02,  7.3989e+02,  4.1093e+02],
          [ 7.0627e+02,  1.1470e+03,  8.4302e+02,  1.3910e+03],
          [ 1.9853e+02,  7.7512e+02,  3.1439e+02,  8.1557e+02],
          [ 7.1665e+02,  2.5629e+02,  8.4298e+02,  4.0160e+02],
          [ 4.7036e+02,  2.9744e+02,  5.1162e+02,  3.8941e+02],
          [ 5.7459e+02,  3.6899e+02,  6.8022e+02,  3.9062e+02],
          [-1.7946e-01,  3.5152e+02,  4.8474e+01,  4.9633e+02],
          [ 7.4111e+02,  1.1022e+03,  7.7271e+02,  1.1402e+03],
          [ 7.1797e+02,  1.2153e+02,  9.5488e+02,  2.0340e+02],
          [ 6.6594e+02,  1.1978e+03,  8.0158e+02,  1.2783e+03],
          [ 5.0715e+02,  3.8645e+02,  6.7002e+02,  4.1483e+02],
          [ 4.7147e+02,  3.8817e+02,  5.8598e+02,  4.1538e+02],
          [ 7.0402e+02,  8.5427e+02,  9.5875e+02,  9.4564e+02],
          [ 5.9653e+02,  2.4633e+02,  6.7358e+02,  3.8572e+02],
          [ 7.5354e+02,  1.1418e+03,  8.3715e+02,  1.2626e+03],
          [ 5.5475e+02,  1.0665e+03,  6.2582e+02,  1.1426e+03],
          [ 8.9434e+02,  1.1260e+03,  9.5993e+02,  1.2288e+03],
          [ 5.4238e+02,  3.0073e+02,  5.8088e+02,  3.4423e+02],
          [ 2.2624e+02,  5.8841e+02,  4.5869e+02,  1.5534e+03],
          [ 7.3856e+00,  8.5226e+02,  2.9402e+02,  9.4297e+02],
          [ 5.6725e+02,  2.9518e+02,  6.1998e+02,  3.8170e+02],
          [ 8.7589e+02,  6.6297e+02,  9.2702e+02,  7.1771e+02],
          [ 7.8245e+02,  8.5805e+02,  9.6093e+02,  9.4616e+02],
          [ 6.8153e+02,  8.6475e+02,  9.5740e+02,  9.4735e+02],
          [ 7.8880e+02,  1.5691e+01,  8.3922e+02,  4.9492e+01],
          [ 6.0934e+02,  3.5620e+02,  8.1543e+02,  4.0694e+02],
          [ 7.0563e+02,  2.5933e+02,  7.6463e+02,  3.2948e+02],
          [ 4.3971e+02,  8.7440e+02,  1.3491e+03,  1.6718e+03],
          [ 5.3532e+02,  3.2163e+02,  5.7972e+02,  3.8141e+02],
          [ 5.7291e+02,  1.1093e+03,  6.7722e+02,  1.2731e+03],
          [ 8.5207e+02,  1.8280e+02,  9.6195e+02,  2.4336e+02],
          [ 1.9446e+02,  8.8931e+02,  2.8334e+02,  9.3941e+02],
          [ 2.8443e+02,  1.1862e+03,  3.9714e+02,  1.2774e+03],
          [ 7.0251e+02,  1.1984e+03,  8.0035e+02,  1.2771e+03],
          [ 5.8677e+02,  1.0741e+03,  6.3615e+02,  1.1441e+03],
          [ 6.3576e+02,  2.3939e+02,  7.0466e+02,  3.4483e+02],
          [ 7.1716e+02,  1.2478e+03,  7.9340e+02,  1.2787e+03],
          [ 7.4763e+02,  1.1095e+03,  7.6297e+02,  1.1288e+03],
          [ 7.0495e+02,  2.7158e+02,  7.4033e+02,  3.2471e+02],
          [ 4.8756e+02,  2.1540e+02,  6.6859e+02,  2.6164e+02],
          [ 7.7167e+02,  6.8096e+02,  8.3375e+02,  7.4444e+02],
          [ 8.7495e+02,  6.4769e+02,  9.5068e+02,  7.0857e+02],
          [ 6.8632e+02,  8.8790e+02,  8.3252e+02,  9.2216e+02],
          [ 7.7111e+02,  9.0154e+02,  8.9993e+02,  9.3323e+02],
          [ 3.5437e+02,  1.1929e+03,  4.3603e+02,  1.2787e+03],
          [ 7.3863e+02,  8.9676e+02,  8.5553e+02,  9.5241e+02],
          [ 5.7965e+02,  1.1198e+03,  6.7414e+02,  1.2746e+03],
          [-3.3766e-01,  4.4367e+02,  4.3007e+01,  5.3567e+02],
          [ 8.3095e+02,  8.6167e+02,  9.5656e+02,  9.3178e+02],
          [ 8.1176e+02,  3.7788e+02,  8.6068e+02,  4.3659e+02],
          [ 4.0108e+02,  8.6327e+02,  6.7953e+02,  9.2438e+02],
          [ 7.9203e+02,  7.7519e+02,  9.6118e+02,  9.3436e+02],
          [ 3.6319e+00,  4.7697e+02,  7.5515e+01,  5.7260e+02]],
         grad_fn=&lt;IndexBackward0&gt;)}]</code></pre>
</div>
</div>
<p>Perfect!</p>
<p>This looks like something we can use.</p>
<p>Let’s break down each of the keys in <code>random_sample_outputs_post_processed</code>.</p>
<p>We get three equal length tensors:</p>
<ul>
<li><code>scores</code> - The prediction probabilities for each box, higher means the model is more confident in this prediction (though it doesn’t mean the prediction is correct). Notice how all the values in this tensor are over our <code>threshold</code> value. This value is acquired by applying <a href="https://pytorch.org/docs/main/generated/torch.sigmoid.html"><code>torch.sigmoid()</code></a> to the models raw output logits.</li>
<li><code>labels</code> - The predicted classification label values for each box. These will be random as our model hasn’t been trained for our dataset. We can turn these into class names by mapping them to the <code>id2label</code> dictionary.</li>
<li><code>boxes</code> - The predicted bounding boxes whose scores are above the <code>threshold</code> parameter. These are normalized and in the format <code>XYXY</code> or <code>(x_top_left, y_top_left, x_bottom_right, y_bottom_right)</code>.</li>
</ul>
<section id="reproducing-our-postprocessed-box-scores-by-hand" class="level3" data-number="11.1">
<h3 data-number="11.1" class="anchored" data-anchor-id="reproducing-our-postprocessed-box-scores-by-hand"><span class="header-section-number">11.1</span> Reproducing our postprocessed box scores by hand</h3>
<p>When a raw prediction output from our model goes through the <code>post_process_object_detection</code> method, a few steps happen.</p>
<p>One of them is that the raw logits from our model get converted into prediction probabilities.</p>
<p>This happens by:</p>
<ol type="1">
<li>Applying the <a href="https://pytorch.org/docs/main/generated/torch.sigmoid.html"><code>torch.sigmoid()</code></a> function to the logits to turn them into prediction probabilities. We’ll also flatten them with <a href="https://docs.pytorch.org/docs/stable/generated/torch.flatten.html"><code>torch.flatten(start_dim=1)</code></a> to turn them into a single dimension tensor (e.g.&nbsp;<code>torch.Size([1, 300, 7])</code> -&gt; <code>torch.Size([1, 2100])</code>).</li>
<li>Getting the top 100 (<code>post_process_object_detection</code> returns the top 100 values by default) prediction scores using <a href="https://pytorch.org/docs/main/generated/torch.topk.html"><code>torch.topk()</code></a>.</li>
<li>Find the values above the target threshold by creating a mask.</li>
<li>Filter the top 100 scores which are above the threshold (using our mask) and sort them in descending order and get the indices with <a href="https://pytorch.org/docs/stable/generated/torch.sort.html"><code>torch.sort()</code></a> (so the predictions with the highest prediction probability come first).</li>
</ol>
<p><code>torch.topk()</code> and <code>torch.sort()</code> will return both raw tensor values and the indices of where they occur in a tuple <code>(values, indices)</code>.</p>
<p>These index values are the predicted label ID.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>When a prediction probability is assigned to a prediction, it usually falls between 0 and 1.</p>
<p>With 1 being the highest possible score.</p>
<p>The value with the highest prediction probability is the value the model is predicting to the be the <em>most likely</em> value.</p>
<p>However, just because a prediction has being assigned a high prediction probability does not mean it is correct.</p>
<p>A high prediction probability is essentially the model saying, “based on the training data I have and the patterns I’ve learned, this is the <em>most likely</em> outcome”.</p>
<p>A workflow using prediction probabilities could be to automatically send samples with low prediction probabilities for manual review.</p>
<p>For now, our model will likely assign close to random prediction probabilities as it has not been trained on our data.</p>
</div>
</div>
<p>To see this happen, let’s reproduce the <code>"scores"</code> key in <code>random_sample_outputs_post_processed</code> by hand.</p>
<div id="cell-126" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the output scores from our post processed single output</span></span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>output_scores <span class="op">=</span> random_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"scores"</span>]</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(output_scores), output_scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<pre><code>(96,
 tensor([0.6736, 0.5951, 0.5918, 0.5854, 0.5757, 0.5403, 0.5364, 0.5363, 0.5268,
         0.5200, 0.5069, 0.5045, 0.5005, 0.4951, 0.4890, 0.4884, 0.4857, 0.4854,
         0.4828, 0.4814, 0.4808, 0.4788, 0.4780, 0.4746, 0.4729, 0.4649, 0.4629,
         0.4608, 0.4608, 0.4606, 0.4598, 0.4580, 0.4577, 0.4506, 0.4504, 0.4500,
         0.4498, 0.4478, 0.4476, 0.4469, 0.4465, 0.4448, 0.4432, 0.4431, 0.4418,
         0.4417, 0.4393, 0.4378, 0.4377, 0.4374, 0.4372, 0.4365, 0.4348, 0.4332,
         0.4316, 0.4316, 0.4290, 0.4287, 0.4270, 0.4257, 0.4256, 0.4249, 0.4239,
         0.4238, 0.4232, 0.4229, 0.4227, 0.4226, 0.4224, 0.4202, 0.4192, 0.4178,
         0.4178, 0.4160, 0.4154, 0.4151, 0.4147, 0.4145, 0.4122, 0.4107, 0.4101,
         0.4099, 0.4098, 0.4088, 0.4087, 0.4087, 0.4081, 0.4066, 0.4056, 0.4045,
         0.4041, 0.4037, 0.4016, 0.4015, 0.4013, 0.4003],
        grad_fn=&lt;IndexBackward0&gt;))</code></pre>
</div>
</div>
<p>And we can reproduce these scores by following the steps outlined above.</p>
<div id="cell-128" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Original input logits shape: </span><span class="sc">{</span>random_sample_outputs<span class="sc">.</span>logits<span class="sc">.</span>shape<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>) </span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Perform sigmoid on the logits to get prediction probabilities and take the max value for each prediction</span></span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a>output_scores_manual <span class="op">=</span> random_sample_outputs.logits.sigmoid().flatten(start_dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Manual output scores shape: </span><span class="sc">{</span>output_scores_manual<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] First 10 scores (these will be in random order):</span><span class="ch">\n</span><span class="sc">{</span>output_scores_manual[<span class="dv">0</span>][:<span class="dv">10</span>]<span class="sc">.</span>detach()<span class="sc">.</span>cpu()<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Get the top 100 scores (this is the default setting in post_process_object_detection)</span></span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a>output_scores_manual_top_100, output_scores_manual_top_100_indices <span class="op">=</span> torch.topk(<span class="bu">input</span><span class="op">=</span>output_scores_manual,</span>
<span id="cb93-10"><a href="#cb93-10" aria-hidden="true" tabindex="-1"></a>                                                                                k<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb93-11"><a href="#cb93-11" aria-hidden="true" tabindex="-1"></a>                                                                                dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb93-12"><a href="#cb93-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Top 100 scores shape: </span><span class="sc">{</span>output_scores_manual_top_100<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb93-13"><a href="#cb93-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] First top 100 score:</span><span class="ch">\n</span><span class="sc">{</span>output_scores_manual_top_100[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb93-14"><a href="#cb93-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-15"><a href="#cb93-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Find the values above the threshold and create a mask</span></span>
<span id="cb93-16"><a href="#cb93-16" aria-hidden="true" tabindex="-1"></a>output_scores_manual_mask <span class="op">=</span> output_scores_manual_top_100 <span class="op">&gt;</span> THRESHOLD</span>
<span id="cb93-17"><a href="#cb93-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-18"><a href="#cb93-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Sort the top 100 scores which are above the threshold and sort them in descending order and get the indices</span></span>
<span id="cb93-19"><a href="#cb93-19" aria-hidden="true" tabindex="-1"></a>output_scores_manual_filtered, output_scores_manual_filtered_indices <span class="op">=</span> torch.sort(<span class="bu">input</span><span class="op">=</span>output_scores_manual_top_100[output_scores_manual_mask], </span>
<span id="cb93-20"><a href="#cb93-20" aria-hidden="true" tabindex="-1"></a>                                                                                  descending<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb93-21"><a href="#cb93-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-22"><a href="#cb93-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Filtered scores shape: </span><span class="sc">{</span>output_scores_manual_filtered<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb93-23"><a href="#cb93-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] First filtered scores:</span><span class="ch">\n</span><span class="sc">{</span>output_scores_manual_filtered[<span class="dv">0</span>]<span class="sc">.</span>detach()<span class="sc">.</span>cpu()<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Original input logits shape: torch.Size([1, 300, 7])

[INFO] Manual output scores shape: torch.Size([1, 2100])
[INFO] First 10 scores (these will be in random order):
tensor([0.0792, 0.2543, 0.1580, 0.5757, 0.2654, 0.1171, 0.0330, 0.0410, 0.1540,
        0.0951])

[INFO] Top 100 scores shape: torch.Size([1, 100])
[INFO] First top 100 score:
0.6736

[INFO] Filtered scores shape: torch.Size([96])
[INFO] First filtered scores:
0.6736</code></pre>
</div>
</div>
<p>Now we’ve got our own <code>output_scores_manual_filtered</code>, how about we compare it to <code>output_scores</code>?</p>
<p>We can see if they’re close to each other with <a href="https://pytorch.org/docs/stable/generated/torch.isclose.html"><code>torch.isclose()</code></a>.</p>
<div id="cell-130" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare the original output scores to our own manual version</span></span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>torch.isclose(<span class="bu">input</span><span class="op">=</span>output_scores[:<span class="bu">len</span>(output_scores_manual_filtered)], </span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>              other<span class="op">=</span>output_scores_manual_filtered, </span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>              atol<span class="op">=</span><span class="fl">1e-2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre><code>tensor([True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True])</code></pre>
</div>
</div>
<p>Nice!</p>
<p>We managed to reproduce our postprocessed output scores values by hand.</p>
<p>How about the labels?</p>
</section>
<section id="reproducing-our-postprocessed-box-labels-by-hand" class="level3" data-number="11.2">
<h3 data-number="11.2" class="anchored" data-anchor-id="reproducing-our-postprocessed-box-labels-by-hand"><span class="header-section-number">11.2</span> Reproducing our postprocessed box labels by hand</h3>
<p>We’ve reproduce our postprocessed model prediction scores by hand.</p>
<p>Now let’s do the same with the labels.</p>
<p>First, we’ll get the output labels from our postprocessed object.</p>
<div id="cell-133" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the model's predicted labels </span></span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>output_labels <span class="op">=</span> random_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"labels"</span>]</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Output labels shape: </span><span class="sc">{</span><span class="bu">len</span>(output_labels)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Output labels:</span><span class="ch">\n</span><span class="sc">{</span>output_labels<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Output labels shape: 96
[INFO] Output labels:
tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])</code></pre>
</div>
</div>
<p>Wonderful!</p>
<p>Now to reproduce these values, we can:</p>
<ol type="1">
<li>Get the number of classes we have using <code>random_sample_outputs.logits.shape[2]</code>, this way we can normalize the flattened indices we created in the previous section.</li>
<li>Modulo the <code>output_scores_manual_top_100_indices</code> by the number of classes to get the predicted labels (e.g.&nbsp;index % num_classes = predicted label -&gt; 747 % 7 = 5).</li>
<li>Filter the remaining labels for predictions which pass the threshold using <code>output_scores_manual_filtered_indices</code>.</li>
</ol>
<div id="cell-135" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Get the number of classes</span></span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>num_classes <span class="op">=</span> random_sample_outputs.logits.shape[<span class="dv">2</span>]</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Found total number of classes: </span><span class="sc">{</span>num_classes<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Modulo the output_scores_manual_top_100_indices by the number of classes to get the predicted class (this is because we flattened our outputs above with .flatten(1))</span></span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a>output_labels_manual <span class="op">=</span> output_scores_manual_top_100_indices <span class="op">%</span> num_classes</span>
<span id="cb99-7"><a href="#cb99-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-8"><a href="#cb99-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Find the top labels which pass our score threshold</span></span>
<span id="cb99-9"><a href="#cb99-9" aria-hidden="true" tabindex="-1"></a>output_labels_manual_filtered <span class="op">=</span> output_labels_manual[<span class="dv">0</span>][output_scores_manual_filtered_indices]</span>
<span id="cb99-10"><a href="#cb99-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-11"><a href="#cb99-11" aria-hidden="true" tabindex="-1"></a>output_labels.shape, output_labels_manual_filtered.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Found total number of classes: 7</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>(torch.Size([96]), torch.Size([96]))</code></pre>
</div>
</div>
<p>Excellent, now let’s make sure these labels are equivalent to the postprocessed labels.</p>
<div id="cell-137" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>output_labels[:<span class="bu">len</span>(output_labels_manual_filtered)] <span class="op">==</span> output_labels_manual_filtered</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="52">
<pre><code>tensor([True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True])</code></pre>
</div>
</div>
<p>Perfect!</p>
<p>How about we repeat the same for our model’s postprocessed predicted boxes?</p>
</section>
<section id="reproducing-our-postprocessed-box-coordinates-by-hand" class="level3" data-number="11.3">
<h3 data-number="11.3" class="anchored" data-anchor-id="reproducing-our-postprocessed-box-coordinates-by-hand"><span class="header-section-number">11.3</span> Reproducing our postprocessed box coordinates by hand</h3>
<p>Our postprocessed boxes are in absolute <code>XYXY</code> format.</p>
<p>And the raw boxes out of the model are in normalized <code>CXCYWH</code> format.</p>
<p>First, let’s get the postprocessed predicted boxes we’re trying to reproduce.</p>
<div id="cell-140" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="co"># These are in absolute XYXY (x_top_left, y_top_left, x_bottom_right, y_bottom_right) format</span></span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a>output_boxes <span class="op">=</span> random_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"boxes"</span>]</span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Output boxes shape: </span><span class="sc">{</span>output_boxes<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Output boxes (absolute XYXY format), first 10:</span><span class="ch">\n</span><span class="sc">{</span>output_boxes[:<span class="dv">10</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Output boxes shape: torch.Size([96, 4])
[INFO] Output boxes (absolute XYXY format), first 10:
tensor([[ 5.0105e+02,  4.5203e-01,  7.3708e+02,  2.4250e+02],
        [ 5.0412e+02,  2.0059e+00,  7.3278e+02,  1.8306e+02],
        [ 6.1310e+02,  2.3096e+02,  6.6116e+02,  3.0177e+02],
        [ 5.0306e+02,  1.2722e+00,  9.5733e+02,  2.2517e+02],
        [ 2.4945e+01,  3.0436e+02,  5.1925e+01,  3.4591e+02],
        [ 5.1260e+02,  6.8830e-01,  9.4725e+02,  6.6806e+01],
        [ 6.1350e+02,  2.2984e+02,  7.0224e+02,  3.0797e+02],
        [ 2.1152e+01, -3.2943e-01,  9.5351e+02,  1.4764e+02],
        [ 1.2980e+01, -3.1096e+00,  9.5033e+02,  3.1315e+02],
        [ 4.9968e+02,  2.2245e+02,  6.2810e+02,  2.4664e+02]],
       grad_fn=&lt;SliceBackward0&gt;)</code></pre>
</div>
</div>
<p>Beautiful!</p>
<p>These are the boxes we’d like to reproduce.</p>
<p>Let’s now get the raw predicted box coordinates from our model.</p>
<div id="cell-142" class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get model output raw boxes</span></span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a><span class="co"># These are in format: normalized CXCYWH (center_x, center_y, width, height) format</span></span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a>output_boxes_manual_cxcywh <span class="op">=</span> random_sample_outputs.pred_boxes[<span class="dv">0</span>]</span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Output boxes manual shape: </span><span class="sc">{</span>output_boxes_manual_cxcywh<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Output boxes manual (normalized CXCYWH format), first 10:</span><span class="ch">\n</span><span class="sc">{</span>output_boxes_manual_cxcywh[:<span class="dv">10</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Output boxes manual shape: torch.Size([300, 4])
[INFO] Output boxes manual (normalized CXCYWH format), first 10:
tensor([[0.0400, 0.2540, 0.0281, 0.0325],
        [0.9611, 0.0529, 0.0756, 0.0779],
        [0.4853, 0.4791, 0.1735, 0.0516],
        [0.3670, 0.6512, 0.1584, 0.1370],
        [0.4984, 0.6331, 0.4169, 0.1247],
        [0.3549, 0.5278, 0.0505, 0.0488],
        [0.7606, 0.0885, 0.4732, 0.1749],
        [0.3979, 0.6548, 0.0929, 0.0897],
        [0.9758, 0.7925, 0.0431, 0.0480],
        [0.5076, 0.0575, 0.9712, 0.1156]], grad_fn=&lt;SliceBackward0&gt;)</code></pre>
</div>
</div>
<p>Ok, so we’ve got 300 boxes here, let’s filter these down to only the boxes which have a score above the target <code>threshold</code> using our <code>output_scores_manual_mask</code> tensor.</p>
<p>If we want to go from raw boxes out of the model to the same format as our postprocessed boxes or from normalized <code>CXCYWH</code> to absolute <code>XYXY</code>, we’ll have to:</p>
<ol type="1">
<li>Normalize the <code>output_scores_manual_top_100_indices</code> by dividing them by the number of classes, for example, <code>top_100_index // num_classes = top_box_index</code> -&gt; <code>763 // 7 = 763</code>.</li>
<li>Filter for the top 100 scoring boxes coordinates which make it over the <code>threshold</code> using the <code>output_scores_manual_mask</code>.</li>
<li>Convert the filtered box coordinates from normalized <code>CXCYWH</code> to normalized <code>XYXY</code> using <a href="https://pytorch.org/vision/main/generated/torchvision.ops.box_convert.html"><code>torchvision.ops.box_convert</code></a>.</li>
<li>Get the original input image size (required for box conversion).</li>
<li>Convert the normalized <code>XYXY</code> coordinates to absolute <code>XYXY</code> coordinates by multiplying the <code>x</code> coordinates by the desired width and the <code>y</code> coordinates by the desired height. For example, if we want to plot our boxes on the original image, we’d use the original image dimensions of <code>(1280, 960)</code> (height, width).</li>
<li>Sort the bounding box coordinates in the same order as the scores (descending).</li>
<li>Check for equivalence between original postprocessed boxes and our manually processed boxes</li>
</ol>
<div id="cell-144" class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Normalize the indices by dividing by the number of classes (this is because we flattened our logits tensor in a previous step) </span></span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>output_scores_manual_top_100_indicies_normalized <span class="op">=</span> output_scores_manual_top_100_indices[<span class="dv">0</span>] <span class="op">//</span> num_classes</span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a>output_scores_manual_top_100_indicies_normalized</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="55">
<pre><code>tensor([292,  56, 118,   6,   0,  50, 167,   9,  46, 111, 270,  63, 122, 182,
         87, 243,  82, 103, 229, 208, 161, 170,  91, 179, 198, 149, 130, 160,
        197, 259, 260, 254, 183, 195,  52, 136, 172,  69,  25, 285, 216, 156,
        296, 215, 146,  30, 101, 154, 203, 181, 209,  14,  88, 107,  74,  83,
        213, 106,  68,  18, 294, 237, 178, 248, 224, 109,  92,  23, 115, 284,
        258, 175, 277,  62,  65, 212,  24, 247, 235, 249, 124, 255, 272, 273,
        231, 276, 256,  95,  51,  38,  70, 286, 133, 265, 142, 287, 244, 217,
         89, 139])</code></pre>
</div>
</div>
<div id="cell-145" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Filter boxes for top 100 above the target threshold</span></span>
<span id="cb110-2"><a href="#cb110-2" aria-hidden="true" tabindex="-1"></a>output_boxes_manual_above_threshold_cxcywh <span class="op">=</span> output_boxes_manual_cxcywh[output_scores_manual_top_100_indicies_normalized]</span>
<span id="cb110-3"><a href="#cb110-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-4"><a href="#cb110-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Output boxes manual above threshold shape: </span><span class="sc">{</span>output_boxes_manual_above_threshold_cxcywh<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb110-5"><a href="#cb110-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Output boxes manual above threshold (normalized CXCYWH format), </span><span class="ch">\</span></span>
<span id="cb110-6"><a href="#cb110-6" aria-hidden="true" tabindex="-1"></a><span class="ss">showing first 10:</span><span class="ch">\n</span><span class="sc">{</span>output_boxes_manual_above_threshold_cxcywh[:<span class="dv">10</span>, :]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Output boxes manual above threshold shape: torch.Size([100, 4])
[INFO] Output boxes manual above threshold (normalized CXCYWH format), showing first 10:
tensor([[0.6449, 0.0949, 0.2459, 0.1891],
        [0.6442, 0.0723, 0.2382, 0.1414],
        [0.6637, 0.2081, 0.0501, 0.0553],
        [0.7606, 0.0885, 0.4732, 0.1749],
        [0.0400, 0.2540, 0.0281, 0.0325],
        [0.7603, 0.0264, 0.4528, 0.0517],
        [0.6853, 0.2101, 0.0924, 0.0610],
        [0.5076, 0.0575, 0.9712, 0.1156],
        [0.5017, 0.1211, 0.9764, 0.2471],
        [0.5874, 0.1832, 0.1338, 0.0189]], grad_fn=&lt;SliceBackward0&gt;)</code></pre>
</div>
</div>
<p>Nice!</p>
<p>Okay, now let’s convert the boxes from normalized <code>CXCYWH</code> to normalized <code>XYXY</code> format using <code>torchvision.ops.box_convert</code>.</p>
<div id="cell-147" class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.ops <span class="im">import</span> box_convert</span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Convert the model's predicted boxes from CXCYWH to XYXY format</span></span>
<span id="cb112-4"><a href="#cb112-4" aria-hidden="true" tabindex="-1"></a>output_boxes_manual_above_threshold_xyxy <span class="op">=</span> box_convert(boxes<span class="op">=</span>output_boxes_manual_above_threshold_cxcywh,</span>
<span id="cb112-5"><a href="#cb112-5" aria-hidden="true" tabindex="-1"></a>                                                       in_fmt<span class="op">=</span><span class="st">"cxcywh"</span>,</span>
<span id="cb112-6"><a href="#cb112-6" aria-hidden="true" tabindex="-1"></a>                                                       out_fmt<span class="op">=</span><span class="st">"xyxy"</span>)</span>
<span id="cb112-7"><a href="#cb112-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Output boxes manual above threshold (absolute XYXY format):</span><span class="ch">\n</span><span class="sc">{</span>output_boxes_manual_above_threshold_xyxy[:<span class="dv">10</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Output boxes manual above threshold (absolute XYXY format):
tensor([[ 5.2193e-01,  3.5315e-04,  7.6779e-01,  1.8945e-01],
        [ 5.2513e-01,  1.5671e-03,  7.6331e-01,  1.4302e-01],
        [ 6.3864e-01,  1.8044e-01,  6.8871e-01,  2.3576e-01],
        [ 5.2402e-01,  9.9394e-04,  9.9722e-01,  1.7591e-01],
        [ 2.5984e-02,  2.3778e-01,  5.4088e-02,  2.7024e-01],
        [ 5.3396e-01,  5.3773e-04,  9.8672e-01,  5.2192e-02],
        [ 6.3906e-01,  1.7956e-01,  7.3150e-01,  2.4060e-01],
        [ 2.2033e-02, -2.5737e-04,  9.9324e-01,  1.1535e-01],
        [ 1.3521e-02, -2.4294e-03,  9.8993e-01,  2.4465e-01],
        [ 5.2050e-01,  1.7379e-01,  6.5427e-01,  1.9268e-01]],
       grad_fn=&lt;SliceBackward0&gt;)</code></pre>
</div>
</div>
<p>Excellent, we’ve got our box coordinates in normalized <code>XYXY</code> format.</p>
<p>We could keep them here.</p>
<p>But to fully replicate the outputs of our postprocessed boxes, we’ll convert them to absolute format.</p>
<p>Absolute format conversion will depend on the target size of image we’d like to use.</p>
<p>For example, if we’d like to convert our boxes to the original dimensions of our input image so we can plot them on that image, we can use the image’s original dimensions.</p>
<p>To get the original dimensions of our image we can access the <code>orig_size</code> attribute of our preprocessed sample.</p>
<div id="cell-149" class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Get the original input image size (required for box conversion)</span></span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>random_sample_image_original_size<span class="op">=</span> random_sample_preprocessed[<span class="st">"labels"</span>][<span class="dv">0</span>][<span class="st">"orig_size"</span>]</span>
<span id="cb114-3"><a href="#cb114-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Image original size: </span><span class="sc">{</span>random_sample_image_original_size<span class="sc">}</span><span class="ss"> (height, width)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Image original size: tensor([1280,  960]) (height, width)</code></pre>
</div>
</div>
<p>Now to convert our normalized coordinates to absolute coordinates we can multiply <code>x</code> coordinates by the target width and <code>y</code> coordinates by the target height.</p>
<div id="cell-151" class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb116"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Convert normalized box coordinates to absolute pixel values</span></span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-3"><a href="#cb116-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Get image original height and width</span></span>
<span id="cb116-4"><a href="#cb116-4" aria-hidden="true" tabindex="-1"></a>original_height, original_width <span class="op">=</span> random_sample_image_original_size</span>
<span id="cb116-5"><a href="#cb116-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-6"><a href="#cb116-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an XYXY tensor to multiply by</span></span>
<span id="cb116-7"><a href="#cb116-7" aria-hidden="true" tabindex="-1"></a>original_dimensions <span class="op">=</span> torch.tensor([original_width,   <span class="co"># x1</span></span>
<span id="cb116-8"><a href="#cb116-8" aria-hidden="true" tabindex="-1"></a>                                    original_height,  <span class="co"># y1 </span></span>
<span id="cb116-9"><a href="#cb116-9" aria-hidden="true" tabindex="-1"></a>                                    original_width,   <span class="co"># x2</span></span>
<span id="cb116-10"><a href="#cb116-10" aria-hidden="true" tabindex="-1"></a>                                    original_height]) <span class="co"># y2</span></span>
<span id="cb116-11"><a href="#cb116-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-12"><a href="#cb116-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the boxes to absolute pixel values</span></span>
<span id="cb116-13"><a href="#cb116-13" aria-hidden="true" tabindex="-1"></a>output_boxes_manual_above_threshold_xyxy_absolute <span class="op">=</span> output_boxes_manual_above_threshold_xyxy <span class="op">*</span> original_dimensions</span>
<span id="cb116-14"><a href="#cb116-14" aria-hidden="true" tabindex="-1"></a>output_boxes_manual_above_threshold_xyxy_absolute[:<span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="59">
<pre><code>tensor([[ 5.0105e+02,  4.5203e-01,  7.3708e+02,  2.4250e+02],
        [ 5.0412e+02,  2.0059e+00,  7.3278e+02,  1.8306e+02],
        [ 6.1310e+02,  2.3096e+02,  6.6116e+02,  3.0177e+02],
        [ 5.0306e+02,  1.2722e+00,  9.5733e+02,  2.2517e+02],
        [ 2.4945e+01,  3.0436e+02,  5.1925e+01,  3.4591e+02],
        [ 5.1260e+02,  6.8830e-01,  9.4725e+02,  6.6806e+01],
        [ 6.1350e+02,  2.2984e+02,  7.0224e+02,  3.0797e+02],
        [ 2.1152e+01, -3.2943e-01,  9.5351e+02,  1.4764e+02],
        [ 1.2980e+01, -3.1096e+00,  9.5033e+02,  3.1315e+02],
        [ 4.9968e+02,  2.2245e+02,  6.2810e+02,  2.4664e+02]],
       grad_fn=&lt;SliceBackward0&gt;)</code></pre>
</div>
</div>
<p>Absolute <code>XYXY</code> coordinates acquired!</p>
<p>Time to order them in the same order as our descending scores.</p>
<div id="cell-153" class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb118"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 6. Order boxes in same order as labels and scores (descending based on score)</span></span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a>output_boxes_manual_sorted <span class="op">=</span> output_boxes_manual_above_threshold_xyxy_absolute[output_scores_manual_filtered_indices]</span>
<span id="cb118-3"><a href="#cb118-3" aria-hidden="true" tabindex="-1"></a>output_boxes_manual_sorted[:<span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="60">
<pre><code>tensor([[ 5.0105e+02,  4.5203e-01,  7.3708e+02,  2.4250e+02],
        [ 5.0412e+02,  2.0059e+00,  7.3278e+02,  1.8306e+02],
        [ 6.1310e+02,  2.3096e+02,  6.6116e+02,  3.0177e+02],
        [ 5.0306e+02,  1.2722e+00,  9.5733e+02,  2.2517e+02],
        [ 2.4945e+01,  3.0436e+02,  5.1925e+01,  3.4591e+02],
        [ 5.1260e+02,  6.8830e-01,  9.4725e+02,  6.6806e+01],
        [ 6.1350e+02,  2.2984e+02,  7.0224e+02,  3.0797e+02],
        [ 2.1152e+01, -3.2943e-01,  9.5351e+02,  1.4764e+02],
        [ 1.2980e+01, -3.1096e+00,  9.5033e+02,  3.1315e+02],
        [ 4.9968e+02,  2.2245e+02,  6.2810e+02,  2.4664e+02]],
       grad_fn=&lt;SliceBackward0&gt;)</code></pre>
</div>
</div>
<p>Finally, we can check to see if our manually postprocessed boxes are equivalent to original post processed boxes.</p>
<p>Because we have 100 boxes, we’ll use <a href="https://docs.pytorch.org/docs/stable/generated/torch.all.html"><code>torch.all()</code></a> which checks if all elements in an <code>input</code> evaluate to <code>True</code> to make sure they’re all the same.</p>
<div id="cell-155" class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb120"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 7. Check for equivalence between original postprocessed boxes and our manually processed boxes</span></span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a>torch.<span class="bu">all</span>(<span class="bu">input</span><span class="op">=</span>output_boxes[:<span class="dv">100</span>] <span class="op">==</span> output_boxes_manual_sorted)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="61">
<pre><code>tensor(True)</code></pre>
</div>
</div>
<p>Excellent!</p>
<p>We’ve now successfully converted our model’s raw outputs to postprocessed usable outputs.</p>
<p>Taking the time to do steps like this helps us understand the steps taken behind the scenes for in-built postprocessing methods.</p>
<p>Knowing how to do these conversion steps can also help use troubleshoot errors we may come across in the future.</p>
</section>
<section id="plotting-our-models-first-box-predictions-on-an-image" class="level3" data-number="11.4">
<h3 data-number="11.4" class="anchored" data-anchor-id="plotting-our-models-first-box-predictions-on-an-image"><span class="header-section-number">11.4</span> Plotting our model’s first box predictions on an image</h3>
<p>We’ve got some predictions, time to follow the data explorer’s motto and <em>visualize, visualize, visualize!</em></p>
<p>To do so we’ll:</p>
<ol type="1">
<li>Extract the scores, labels and boxes from our <code>random_sample_outputs_post_processed</code>.</li>
<li>Create a list of label names to plot by mapping label IDs to class names as well as a list of colours to colour our boxes with in accordance to our <code>colour_palette</code>.</li>
<li>Draw boxes on the image with a combination of <code>torchvision</code>’s <a href="https://pytorch.org/vision/main/generated/torchvision.transforms.functional.pil_to_tensor.html"><code>pil_to_tensor</code></a>, <a href="https://pytorch.org/vision/main/generated/torchvision.utils.draw_bounding_boxes.html"><code>draw_bounding_boxes</code></a> and <a href="https://pytorch.org/vision/main/generated/torchvision.transforms.functional.to_pil_image.html"><code>to_pil_image</code></a>.</li>
</ol>
<p>We’ll halve the image as well as the box coordinates using <code>half_image</code> and <code>half_boxes</code> to save space in our notebook (this is not 100% necessary, just for convenience).</p>
<div id="cell-158" class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb122"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Extract scores, labels and boxes</span></span>
<span id="cb122-2"><a href="#cb122-2" aria-hidden="true" tabindex="-1"></a>random_sample_pred_scores <span class="op">=</span> random_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"scores"</span>]</span>
<span id="cb122-3"><a href="#cb122-3" aria-hidden="true" tabindex="-1"></a>random_sample_pred_labels <span class="op">=</span> random_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"labels"</span>]</span>
<span id="cb122-4"><a href="#cb122-4" aria-hidden="true" tabindex="-1"></a>random_sample_pred_boxes <span class="op">=</span> half_boxes(random_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"boxes"</span>])</span>
<span id="cb122-5"><a href="#cb122-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-6"><a href="#cb122-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Create a list of labels and colours to plot on the image/boxes</span></span>
<span id="cb122-7"><a href="#cb122-7" aria-hidden="true" tabindex="-1"></a>random_sample_pred_labels_to_plot <span class="op">=</span> [<span class="ss">f"Pred: </span><span class="sc">{</span>id2label[label_pred.item()]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="bu">round</span>(score_pred.item(), <span class="dv">4</span>)<span class="sc">}</span><span class="ss">)"</span> </span>
<span id="cb122-8"><a href="#cb122-8" aria-hidden="true" tabindex="-1"></a>                  <span class="cf">for</span> label_pred, score_pred <span class="kw">in</span> <span class="bu">zip</span>(random_sample_pred_labels, random_sample_pred_scores)]</span>
<span id="cb122-9"><a href="#cb122-9" aria-hidden="true" tabindex="-1"></a>random_sample_pred_colours <span class="op">=</span> [colour_palette[id2label[label_pred.item()]] <span class="cf">for</span> label_pred <span class="kw">in</span> random_sample_pred_labels]</span>
<span id="cb122-10"><a href="#cb122-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-11"><a href="#cb122-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Labels with scores: </span><span class="sc">{</span>random_sample_pred_labels_to_plot[:<span class="dv">3</span>]<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb122-12"><a href="#cb122-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-13"><a href="#cb122-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Plot the random sample image with randomly predicted boxes </span></span>
<span id="cb122-14"><a href="#cb122-14" aria-hidden="true" tabindex="-1"></a><span class="co"># (these will be very poor since the model is not trained on our data yet)</span></span>
<span id="cb122-15"><a href="#cb122-15" aria-hidden="true" tabindex="-1"></a>to_pil_image(</span>
<span id="cb122-16"><a href="#cb122-16" aria-hidden="true" tabindex="-1"></a>    pic<span class="op">=</span>draw_bounding_boxes(</span>
<span id="cb122-17"><a href="#cb122-17" aria-hidden="true" tabindex="-1"></a>        image<span class="op">=</span>pil_to_tensor(pic<span class="op">=</span>half_image(random_sample[<span class="st">"image"</span>])),</span>
<span id="cb122-18"><a href="#cb122-18" aria-hidden="true" tabindex="-1"></a>        boxes<span class="op">=</span>random_sample_pred_boxes, <span class="co"># boxes are in XYXY format, which is required for draw_bounding_boxes</span></span>
<span id="cb122-19"><a href="#cb122-19" aria-hidden="true" tabindex="-1"></a>        labels<span class="op">=</span>random_sample_pred_labels_to_plot,</span>
<span id="cb122-20"><a href="#cb122-20" aria-hidden="true" tabindex="-1"></a>        colors<span class="op">=</span>random_sample_pred_colours,</span>
<span id="cb122-21"><a href="#cb122-21" aria-hidden="true" tabindex="-1"></a>        width<span class="op">=</span><span class="dv">3</span></span>
<span id="cb122-22"><a href="#cb122-22" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb122-23"><a href="#cb122-23" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Labels with scores: ['Pred: not_hand (0.6736)', 'Pred: not_hand (0.5951)', 'Pred: not_hand (0.5918)']...</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="62">
<div>
<figure class="figure">
<p><img src="hugging_face_object_detection_tutorial_files/figure-html/cell-62-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Woah! Those boxes don’t look good at all both the label and the coordinates look off.</p>
<p>This should be expected though…</p>
<p>While our model has been pretrained on the COCO dataset, it hasn’t been trained on our specific data.</p>
<p>The good news is we can hopefully (there are no guarantees in machine learning) improve our model’s box predictions by fine-tuning it on our dataset.</p>
</section>
<section id="aside-bounding-box-formats-in-and-out-of-our-model" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="aside-bounding-box-formats-in-and-out-of-our-model"><span class="header-section-number">12</span> Aside: Bounding box formats in and out of our model</h2>
<p>We’ve done a fair bit of data transformation to get our data ready to go into our model and we’ve also taken a fair few steps to postprocess it into a usable format.</p>
<p>This is often a standard practice in many machine learning workflows.</p>
<p>Much of the work before ever training a model is preparing the data for the model.</p>
<p>And much of the work after training a model is preparing the data for your use case.</p>
<p>The following table highlights the different states our bounding boxes go in and out of.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><strong>Step</strong></th>
<th><strong>Box format</strong></th>
<th><strong>Scale</strong></th>
<th><strong>Goes into</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Starting data (default downloaded from our Hugging Face dataset, note: not all boxes start in this format)</td>
<td><code>XYWH</code> or <code>[x1, y1, width, height]</code></td>
<td>Absolute</td>
<td><code>preprocess()</code> method</td>
</tr>
<tr class="even">
<td>Out of <a href="https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.preprocess"><code>preprocess()</code></a></td>
<td><code>CXCYWH</code> or <code>[center_x, center_y, width, height]</code></td>
<td>Normalized</td>
<td><code>model.forward()</code></td>
</tr>
<tr class="odd">
<td>Out of <a href="https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection.forward"><code>model.forward()</code></a></td>
<td><code>CXCYWH</code> or <code>[center_x, center_y, width, height]</code></td>
<td>Normalized</td>
<td><code>post_process_object_detection()</code></td>
</tr>
<tr class="even">
<td>Out of <a href="https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.post_process_object_detection"><code>post_process_object_detection()</code></a></td>
<td><code>XYXY</code> or <code>[x_top_left, y_top_left, x_bottom_right, y_bottom_right]</code></td>
<td>Absolute (in relation to the <code>target_sizes</code> parameter).</td>
<td>Plotting or display function.</td>
</tr>
</tbody>
</table>
<figure style="text-align: center;" class="figure">
<img src="https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-trashify-object-detection/08-bounding-box-formats.png" alt="A diagram illustrating a four-stage object detection pipeline: 'Original Data' shows an image of a hand, trash, and bin with bounding boxes, having 'Format: XYWH' and 'Scale: Absolute'; an arrow points to the '.preprocess()' stage, represented by a green 'RTDetrImageProcessor' block, with 'Format: CXCYWH' and 'Scale: Normalized'; another arrow leads to the '.forward()' stage, depicted by a neural network icon labeled 'RTDetrV2ForObjectDetection', with 'Format: CXCYWH' and 'Scale: Normalized'; a final arrow points to the '.post_process_object_detection()' stage, another green 'RTDetrImageProcessor' block, with 'Format: XYXY' and 'Scale: Absolute'." style="width: 100%; max-width: 900px; height: auto;" class="figure-img">
<figcaption>
Our bounding boxes go through a series of format changes from input to final output. Keeping track of what format our boduning boxes are in is important for both training models and visualizing boxes on images. If we use the wrong format for plotting boxes on images, we may falsely assume our model is performing better or worse than it actually is.
</figcaption>
</figure>
<p>Keeping track of these input and output formats is helpful for knowing the state of your data.</p>
<p>But remember, just because our current workflow is like this, doesn’t mean all future workflows you work on will have the same transformation steps.</p>
<p>For more on different bounding box formats, see the <a href="https://www.learnml.io/posts/a-guide-to-bounding-box-formats/">bounding box formats guide</a>.</p>
</section>
<section id="preparing-data-at-scale" class="level2" data-number="13">
<h2 data-number="13" class="anchored" data-anchor-id="preparing-data-at-scale"><span class="header-section-number">13</span> Preparing data at scale</h2>
<p>We’ve performed preprocessing and postprocessing steps on a single data sample.</p>
<p>However, in practice, we’ll likely want to work with many more samples.</p>
<p>Our model is hungry for more data.</p>
<p>So let’s step it up a notch and write some code that’s capable of preprocessing <em>many</em> samples to pass to our model.</p>
<p>We’ll break it down into three subsections:</p>
<ol type="1">
<li><strong>Splitting the data</strong> into training, validation and test sets. We’ll train our model on the training set and check its performance on the validation and test sets (our model won’t see any of these samples during training). We perform these splits before preprocessing the samples in them in case we’d like to perform different preprocessing steps depending on the split. For example, we may want to use <a href="https://pytorch.org/vision/main/transforms.html"><strong>data augmentation</strong></a> on the training set and not use it on the testing set.</li>
<li><strong>Preprocessing multiple samples at a time</strong> by iterating over groups of samples. Rather than preprocess a single sample at a time, we’ll write code capable of processing lists of examples simultaneously.</li>
<li><strong>Collate samples into batches</strong> so our model can view multiple samples simultaneously. Rather than performing a forward pass on a single sample at a time, we’ll pass <strong>batches</strong> of data to the model. For example, we may pass 32 samples (image and label pairs) at a time to our model for it to try and learn the patterns between them. We use batches of data rather than the whole dataset as it’s often much more memory efficient. If you have a <em>really</em> large dataset, all of your samples may not fit into memory at once, so in practice, you break it up into smaller batches of samples.</li>
</ol>
<p>Let’s start by splitting the data into different sets.</p>
<section id="splitting-the-data-into-training-and-test-sets" class="level3" data-number="13.1">
<h3 data-number="13.1" class="anchored" data-anchor-id="splitting-the-data-into-training-and-test-sets"><span class="header-section-number">13.1</span> Splitting the data into training and test sets</h3>
<p>Right now our data is all in one big group.</p>
<p>However, it’s best practice to split our data into two (or three) different sets:</p>
<ol type="1">
<li><strong>Training set (~70-80% of data)</strong> - This is the data the model will learn from, all samples in this set are seen by the model during training.</li>
<li><strong>Validation set (~5-20% of data)</strong> - This is the data we can fine-tune our model’s hyperparameters on, all samples in this set are <em>not</em> seen by the model during training.</li>
<li><strong>Test set (~5-20% of data)</strong> - This is the data we will evaluate what our model has learned after going through the training set, all samples in this set are <em>not</em> seen by the model during training.</li>
</ol>
<p>Using the analogy of a student at univeristy, the <strong>training set</strong> would be the course materials throughout the semester, the <strong>validation set</strong> would be the practice exam and the <strong>test set</strong> would be the final exam.</p>
<p>If a student doesn’t perform well on the final exam, then we would usually say perhaps the course materials weren’t of the highest quality.</p>
<p>This is similar to our machine learning workflow.</p>
<p>In an ideal world, the samples in the training set are sufficiently representative of those in the test set and in turn, sufficiently representative of samples in the wild.</p>
<p>Before we split our dataset into different sets, let’s remind ourselves of what it looks like.</p>
<div id="cell-163" class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Original dataset (only a "train" split)</span></span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(path<span class="op">=</span><span class="st">"mrdbourke/trashify_manual_labelled_images"</span>)</span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a>original_dataset_length <span class="op">=</span> <span class="bu">len</span>(dataset[<span class="st">"train"</span>])</span>
<span id="cb124-4"><a href="#cb124-4" aria-hidden="true" tabindex="-1"></a>dataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="63">
<pre><code>DatasetDict({
    train: Dataset({
        features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],
        num_rows: 1128
    })
})</code></pre>
</div>
</div>
<p>Wonderful! Right now, we’ve only got one split, <code>"train"</code>.</p>
<p>To make our required splits, we can call the <a href="https://huggingface.co/docs/datasets/en/package_reference/main_classes#datasets.Dataset.train_test_split"><code>train_test_split()</code></a> method on our dataset and pass in the size of the split we’d like via the <code>test_size</code> parameter.</p>
<p>For example, <code>test_size=0.3</code> means 30% of the data will go to the test set and 70% will go to the training set.</p>
<p>We’ll make the following splits:</p>
<ul>
<li>70% of data to training set.</li>
<li>~10% of data to validation set.</li>
<li>~20% of data to testing set.</li>
</ul>
<p>To do so, we’ll call <code>train_test_split()</code> twice with different amounts:</p>
<ol type="1">
<li>First on <code>dataset["train"]</code> with <code>test_size=0.3</code> to make the 70/30 training/test split, we’ll save this split to the variable <code>dataset_split</code>.</li>
<li>Next on <code>dataset_split["test"]</code> with <code>test_size=0.66</code> to make the 66/33 test/validation split, we’ll set this variable to <code>dataset_test_val_split</code>.</li>
</ol>
<figure style="text-align: center;" class="figure">
<img src="https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-trashify-object-detection/05-dataset-splits.png" alt="A diagram illustrating two methods of splitting a dataset, shown as three vertical bars: the leftmost pink bar represents the 'Whole Dataset'; an arrow points to the middle bar, which is split into a blue 'Testing Split (30%)' on top and a pink 'Training Split (70%)' on the bottom; a second arrow points to the rightmost bar, which is divided into a blue 'Testing Split (20%)' at the top, a yellow 'Validation Split (10%)' in the middle, and a pink 'Training Split (70%)' at the bottom." style="width: 100%; max-width: 900px; height: auto;" class="figure-img">
<figcaption>
An approximate breakdown of the different dataset splits we’re going to create. We’ll start with the whole dataset and then break it into training and test splits before breaking the subsequent test split into test and validation splits. Our model will train on the training data and be evaluated on the validation and testing data.
</figcaption>
</figure>
<p>Once we’ve done this, we’ll reassign all of the splits back to our original <code>dataset</code>.</p>
<p>We’ll also set <code>seed=42</code> for reproducibility.</p>
<p>Let’s do it!</p>
<div id="cell-165" class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb126"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: Be careful of running this cell multiple times, if you do, the dataset size will get smaller. </span></span>
<span id="cb126-2"><a href="#cb126-2" aria-hidden="true" tabindex="-1"></a><span class="co"># If this happens, just reload the whole `dataset` as above.</span></span>
<span id="cb126-3"><a href="#cb126-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-4"><a href="#cb126-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Split the data into "train" and "test" splits</span></span>
<span id="cb126-5"><a href="#cb126-5" aria-hidden="true" tabindex="-1"></a>dataset_split <span class="op">=</span> dataset[<span class="st">"train"</span>].train_test_split(test_size<span class="op">=</span><span class="fl">0.3</span>, seed<span class="op">=</span><span class="dv">42</span>) <span class="co"># split the dataset into 70/30 train/test</span></span>
<span id="cb126-6"><a href="#cb126-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-7"><a href="#cb126-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Split the test split into "test" and "validation" splits</span></span>
<span id="cb126-8"><a href="#cb126-8" aria-hidden="true" tabindex="-1"></a>dataset_test_val_split <span class="op">=</span> dataset_split[<span class="st">"test"</span>].train_test_split(test_size<span class="op">=</span><span class="fl">0.66</span>, seed<span class="op">=</span><span class="dv">42</span>) <span class="co"># split the test set into 40/60 validation/test</span></span>
<span id="cb126-9"><a href="#cb126-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-10"><a href="#cb126-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create "train" split from 1.</span></span>
<span id="cb126-11"><a href="#cb126-11" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">"train"</span>] <span class="op">=</span> dataset_split[<span class="st">"train"</span>]</span>
<span id="cb126-12"><a href="#cb126-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-13"><a href="#cb126-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a "validation" and "test" split from 2.</span></span>
<span id="cb126-14"><a href="#cb126-14" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">"validation"</span>] <span class="op">=</span> dataset_test_val_split[<span class="st">"train"</span>]</span>
<span id="cb126-15"><a href="#cb126-15" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">"test"</span>] <span class="op">=</span> dataset_test_val_split[<span class="st">"test"</span>]</span>
<span id="cb126-16"><a href="#cb126-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-17"><a href="#cb126-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure splits lengths add to equal original dataset length (otherwise there's a mistmatch somewhere)</span></span>
<span id="cb126-18"><a href="#cb126-18" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> original_dataset_length <span class="op">==</span> <span class="bu">len</span>(dataset[<span class="st">"train"</span>]) <span class="op">+</span> <span class="bu">len</span>(dataset[<span class="st">"validation"</span>]) <span class="op">+</span> <span class="bu">len</span>(dataset[<span class="st">"test"</span>]), <span class="st">"Total dataset split lengths don't equal original dataset length, is there a mismatch? Perhaps try reloading the original dataset and re-running this cell."</span></span>
<span id="cb126-19"><a href="#cb126-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-20"><a href="#cb126-20" aria-hidden="true" tabindex="-1"></a><span class="co"># View the dataset (now with splits)</span></span>
<span id="cb126-21"><a href="#cb126-21" aria-hidden="true" tabindex="-1"></a>dataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="64">
<pre><code>DatasetDict({
    train: Dataset({
        features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],
        num_rows: 789
    })
    validation: Dataset({
        features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],
        num_rows: 115
    })
    test: Dataset({
        features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],
        num_rows: 224
    })
})</code></pre>
</div>
</div>
<p>Perfect!</p>
<p>Now we’ve got three splits of our dataset to work with.</p>
<p>We’ll make sure our model never sees the <code>validation</code> and <code>test</code> splits during training, so when evaluate it we know that it’s only seeing new samples.</p>
</section>
<section id="writing-a-function-for-preprocessing-multiple-samples-at-a-time" class="level3" data-number="13.2">
<h3 data-number="13.2" class="anchored" data-anchor-id="writing-a-function-for-preprocessing-multiple-samples-at-a-time"><span class="header-section-number">13.2</span> Writing a function for preprocessing multiple samples at a time</h3>
<p>We’ve preprocessed and passed one sample through our model, new let’s do the same for multiple samples.</p>
<p>We’re going to work towards having a function that can go from a group or batch of samples (images and their annotations) and return them in preprocessed form (via <a href="https://huggingface.co/docs/transformers/en/model_doc/rt_detr#transformers.RTDetrImageProcessor.preprocess"><code>transformers.RTDetrImageProcessor.preprocess</code></a>) ready to be used with our model.</p>
<p>Let’s first remind ourselves of what a single unprocessed sample looks like.</p>
<div id="cell-168" class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb128"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get one sample from the training dataset </span></span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a>one_sample <span class="op">=</span> dataset[<span class="st">"train"</span>][<span class="dv">42</span>]</span>
<span id="cb128-3"><a href="#cb128-3" aria-hidden="true" tabindex="-1"></a>one_sample</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="65">
<pre><code>{'image': &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,
 'image_id': 663,
 'annotations': {'file_name': ['1d2ea64a-0296-403d-93cd-31e3f116c995.jpeg',
   '1d2ea64a-0296-403d-93cd-31e3f116c995.jpeg'],
  'image_id': [663, 663],
  'category_id': [1, 5],
  'bbox': [[413.29998779296875,
    529.7000122070312,
    343.6000061035156,
    687.0999755859375],
   [435.8999938964844, 463.0, 77.19999694824219, 99.9000015258789]],
  'iscrowd': [0, 0],
  'area': [236087.5625, 7712.27978515625]},
 'label_source': 'manual_prodigy_label',
 'image_source': 'manual_taken_photo'}</code></pre>
</div>
</div>
<p>Awesome, we get an <code>image</code> in <code>PIL.Image.Image</code> form as well as a single dictionary of <code>annotations</code>.</p>
<p>How about if we were to inspect a group of three samples?</p>
<div id="cell-170" class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb130"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get three samples from the training set</span></span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a>group_of_samples <span class="op">=</span> dataset[<span class="st">"train"</span>][<span class="dv">0</span>:<span class="dv">3</span>]</span>
<span id="cb130-3"><a href="#cb130-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-4"><a href="#cb130-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Uncomment for full output (commented for brevity)</span></span>
<span id="cb130-5"><a href="#cb130-5" aria-hidden="true" tabindex="-1"></a><span class="co"># group_of_samples </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<details>
<summary>
Output of random_samples
</summary>
<pre><code>Signature:
{'image': [&lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,
  &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;,
  &lt;PIL.Image.Image image mode=RGB size=960x1280&gt;],
 'image_id': [69, 1027, 1092],
 'annotations': [{'file_name': ['c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',
    'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',
    'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',
    'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',
    'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',
    'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',
    'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',
    'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg'],
   'image_id': [69, 69, 69, 69, 69, 69, 69, 69],
   'category_id': [5, 0, 1, 4, 4, 4, 4, 4],
   'bbox': [[360.20001220703125, 528.5, 177.1999969482422, 261.79998779296875],
    [298.29998779296875,
     495.1000061035156,
     381.1000061035156,
     505.70001220703125],
    [81.5999984741211,
     592.0999755859375,
     358.79998779296875,
     316.29998779296875],
    [1.2999999523162842,
     776.7000122070312,
     193.8000030517578,
     211.89999389648438],
    [301.1000061035156, 60.79999923706055, 146.89999389648438, 115.0],
    [501.0, 75.9000015258789, 24.200000762939453, 71.19999694824219],
    [546.4000244140625,
     54.70000076293945,
     130.3000030517578,
     115.0999984741211],
    [862.9000244140625,
     41.099998474121094,
     75.69999694824219,
     80.19999694824219]],
   'iscrowd': [0, 0, 0, 0, 0, 0, 0, 0],
   'area': [46390.9609375,
    192722.265625,
    113488.4375,
    41066.21875,
    16893.5,
    1723.0400390625,
    14997.5302734375,
    6071.14013671875]},
  {'file_name': ['b664785b-f8b6-4dd2-9ede-d89c07564812.jpeg',
    'b664785b-f8b6-4dd2-9ede-d89c07564812.jpeg',
    'b664785b-f8b6-4dd2-9ede-d89c07564812.jpeg',
    'b664785b-f8b6-4dd2-9ede-d89c07564812.jpeg',
    'b664785b-f8b6-4dd2-9ede-d89c07564812.jpeg'],
   'image_id': [1027, 1027, 1027, 1027, 1027],
   'category_id': [5, 4, 1, 0, 0],
   'bbox': [[378.29998779296875, 657.5, 139.8000030517578, 165.10000610351562],
    [463.29998779296875, 754.5, 39.400001525878906, 30.299999237060547],
    [451.20001220703125,
     734.7999877929688,
     109.19999694824219,
     163.8000030517578],
    [140.39999389648438, 400.29998779296875, 460.8999938964844, 491.5],
    [2.299999952316284,
     322.29998779296875,
     201.6999969482422,
     429.20001220703125]],
   'iscrowd': [0, 0, 0, 0, 0],
   'area': [23080.98046875,
    1193.8199462890625,
    17886.9609375,
    226532.34375,
    86569.640625]},
  {'file_name': ['d822c383-f53a-4a2e-b2f2-3eac55c0e515.jpeg',
    'd822c383-f53a-4a2e-b2f2-3eac55c0e515.jpeg',
    'd822c383-f53a-4a2e-b2f2-3eac55c0e515.jpeg',
    'd822c383-f53a-4a2e-b2f2-3eac55c0e515.jpeg'],
   'image_id': [1092, 1092, 1092, 1092],
   'category_id': [2, 5, 1, 0],
   'bbox': [[97.80000305175781, 93.30000305175781, 177.5, 101.5999984741211],
    [342.20001220703125, 572.5999755859375, 350.0, 344.20001220703125],
    [185.1999969482422, 803.0, 304.3999938964844, 371.6000061035156],
    [219.39999389648438, 259.1000061035156, 598.7000122070312, 584.5]],
   'iscrowd': [0, 0, 0, 0],
   'area': [18034.0, 120470.0, 113115.0390625, 349940.15625]}],
 'label_source': ['manual_prodigy_label',
  'manual_prodigy_label',
  'manual_prodigy_label'],
 'image_source': ['manual_taken_photo',
  'manual_taken_photo',
  'manual_taken_photo']}</code></pre>
</details>
<p>Okay, now we get a list of <code>image</code> objects as well as a list of <code>annotation</code> dictionaries and more in the format:</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span></span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a>    <span class="dt">"image"</span><span class="fu">:</span> <span class="ot">[</span><span class="er">&lt;PIL.Image.Image&gt;</span><span class="ot">,</span> <span class="er">&lt;PIL.Image.Image&gt;</span><span class="ot">,</span> <span class="er">...</span><span class="ot">]</span><span class="fu">,</span></span>
<span id="cb132-3"><a href="#cb132-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">"image_id"</span><span class="fu">:</span> <span class="ot">[</span><span class="er">int</span><span class="ot">,</span> <span class="er">int</span><span class="ot">,</span> <span class="er">...</span><span class="ot">]</span><span class="fu">,</span></span>
<span id="cb132-4"><a href="#cb132-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">"annotations"</span><span class="fu">:</span> <span class="ot">[</span></span>
<span id="cb132-5"><a href="#cb132-5" aria-hidden="true" tabindex="-1"></a>        <span class="fu">{</span></span>
<span id="cb132-6"><a href="#cb132-6" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"file_name"</span><span class="fu">:</span> <span class="ot">[</span><span class="er">str</span><span class="ot">,</span> <span class="er">str</span><span class="ot">,</span> <span class="er">...</span><span class="ot">]</span><span class="fu">,</span></span>
<span id="cb132-7"><a href="#cb132-7" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"image_id"</span><span class="fu">:</span> <span class="ot">[</span><span class="er">int</span><span class="ot">,</span> <span class="er">int</span><span class="ot">,</span> <span class="er">...</span><span class="ot">]</span><span class="fu">,</span></span>
<span id="cb132-8"><a href="#cb132-8" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"category_id"</span><span class="fu">:</span> <span class="ot">[</span><span class="er">int</span><span class="ot">,</span> <span class="er">int</span><span class="ot">,</span> <span class="er">...</span><span class="ot">]</span><span class="fu">,</span></span>
<span id="cb132-9"><a href="#cb132-9" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"bbox"</span><span class="fu">:</span> <span class="ot">[[</span><span class="er">float</span><span class="ot">,</span> <span class="er">float</span><span class="ot">,</span> <span class="er">float</span><span class="ot">,</span> <span class="er">float</span><span class="ot">],</span> <span class="er">...</span><span class="ot">]</span><span class="fu">,</span></span>
<span id="cb132-10"><a href="#cb132-10" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"iscrowd"</span><span class="fu">:</span> <span class="ot">[</span><span class="er">int</span><span class="ot">,</span> <span class="er">int</span><span class="ot">,</span> <span class="er">...</span><span class="ot">]</span><span class="fu">,</span></span>
<span id="cb132-11"><a href="#cb132-11" aria-hidden="true" tabindex="-1"></a>            <span class="dt">"area"</span><span class="fu">:</span> <span class="ot">[</span><span class="er">float</span><span class="ot">,</span> <span class="er">float</span><span class="ot">,</span> <span class="er">...</span><span class="ot">]</span></span>
<span id="cb132-12"><a href="#cb132-12" aria-hidden="true" tabindex="-1"></a>        <span class="fu">}</span><span class="ot">,</span></span>
<span id="cb132-13"><a href="#cb132-13" aria-hidden="true" tabindex="-1"></a>        <span class="fu">{</span><span class="er">...</span><span class="fu">}</span><span class="ot">,</span></span>
<span id="cb132-14"><a href="#cb132-14" aria-hidden="true" tabindex="-1"></a>        <span class="fu">{</span><span class="er">...</span><span class="fu">}</span></span>
<span id="cb132-15"><a href="#cb132-15" aria-hidden="true" tabindex="-1"></a>    <span class="ot">]</span><span class="fu">,</span></span>
<span id="cb132-16"><a href="#cb132-16" aria-hidden="true" tabindex="-1"></a>    <span class="dt">"label_source"</span><span class="fu">:</span> <span class="ot">[</span><span class="er">str</span><span class="ot">,</span> <span class="er">str</span><span class="ot">,</span> <span class="er">...</span><span class="ot">]</span><span class="fu">,</span></span>
<span id="cb132-17"><a href="#cb132-17" aria-hidden="true" tabindex="-1"></a>    <span class="dt">"image_source"</span><span class="fu">:</span> <span class="ot">[</span><span class="er">str</span><span class="ot">,</span> <span class="er">str</span><span class="ot">,</span> <span class="er">...</span><span class="ot">]</span></span>
<span id="cb132-18"><a href="#cb132-18" aria-hidden="true" tabindex="-1"></a><span class="fu">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Knowing this structure, we’ll want to write a function capable of taking it as input and then preparing it for the <code>preprocess</code> method.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Our data is in this structure (a dictionary of lists, rather than a list of dictionaries) because it is built on Hugging Face Datasets and <a href="https://huggingface.co/docs/datasets/en/about_arrow">Hugging Face Datasets is built on Apache Arrow</a>.</p>
<p>And <a href="https://arrow.apache.org/">Apache Arrow</a> is column-orientated in nature.</p>
<p>So instead of our dataset being represented as many rows (list of dictionaries), it is represented as many columns (dictionary of lists).</p>
</div>
</div>
<p>The <code>preprocess</code> method expects a list of images as well as COCO formatted annotations as input.</p>
<p>So to create our <code>preprocess_batch</code> function we’ll:</p>
<ol type="1">
<li>Take in a list of examples (these will be in the format above), an <code>image_processor</code> and optional <code>transforms</code> (we don’t need to pass these in for now but it’s good to have the option).</li>
<li>Create empty lists of <code>images</code> and <code>coco_annotations</code> we’ll fill throughout the rest of the function.</li>
<li>Extract the <code>image</code>, <code>image_id</code> and <code>annotations_dict</code> from our list of input examples.</li>
<li>Create lists of annotations attributes such as <code>bbox</code>, <code>category_id</code> and <code>area</code> (these are required for our <code>format_image_annotations_as_coco</code> function.</li>
<li>Optionally perform transforms/augmentations on the image and related boxes (because in object detection if you transform an image, should transform the related boxes as well).</li>
<li>Convert the annotations into COCO format using the <code>format_image_annotations_as_coco</code> helper function we created earlier.</li>
<li>Append the images and COCO formatted annotations to the empty lists created in 2.</li>
<li>Pass the list of images and COCO formatted annotations to the <code>image_processor.preprocess</code> method to get the preprocessed batch.</li>
<li>Return the preprocessed batch.</li>
</ol>
<p>Let’s do it!</p>
<div id="cell-172" class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb133"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Take in a list of examples, image processor and optional transforms</span></span>
<span id="cb133-2"><a href="#cb133-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> preprocess_batch(examples, </span>
<span id="cb133-3"><a href="#cb133-3" aria-hidden="true" tabindex="-1"></a>                     image_processor,</span>
<span id="cb133-4"><a href="#cb133-4" aria-hidden="true" tabindex="-1"></a>                     transforms<span class="op">=</span><span class="va">None</span>, <span class="co"># Note: Could optionally add transforms (e.g. data augmentation) here </span></span>
<span id="cb133-5"><a href="#cb133-5" aria-hidden="true" tabindex="-1"></a>                     ):</span>
<span id="cb133-6"><a href="#cb133-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb133-7"><a href="#cb133-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Preprocesses a batch of image data with annotations for object detection models.</span></span>
<span id="cb133-8"><a href="#cb133-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-9"><a href="#cb133-9" aria-hidden="true" tabindex="-1"></a><span class="co">    This function takes a batch of examples in a custom dataset format, extracts images and</span></span>
<span id="cb133-10"><a href="#cb133-10" aria-hidden="true" tabindex="-1"></a><span class="co">    their corresponding annotations, and converts them into a format suitable for model training</span></span>
<span id="cb133-11"><a href="#cb133-11" aria-hidden="true" tabindex="-1"></a><span class="co">    or inference using the provided image processor.</span></span>
<span id="cb133-12"><a href="#cb133-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-13"><a href="#cb133-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb133-14"><a href="#cb133-14" aria-hidden="true" tabindex="-1"></a><span class="co">        examples (dict): A dictionary containing the batch data with the following structure:</span></span>
<span id="cb133-15"><a href="#cb133-15" aria-hidden="true" tabindex="-1"></a><span class="co">            - "image" (List[PIL.Image.Image]): List of PIL Image objects</span></span>
<span id="cb133-16"><a href="#cb133-16" aria-hidden="true" tabindex="-1"></a><span class="co">            - "image_id" (List[int]): List of unique image identifiers</span></span>
<span id="cb133-17"><a href="#cb133-17" aria-hidden="true" tabindex="-1"></a><span class="co">            - "annotations" (List[dict]): List of annotation dictionaries, where each contains:</span></span>
<span id="cb133-18"><a href="#cb133-18" aria-hidden="true" tabindex="-1"></a><span class="co">                - "file_name" (List[str]): List of image filenames</span></span>
<span id="cb133-19"><a href="#cb133-19" aria-hidden="true" tabindex="-1"></a><span class="co">                - "image_id" (List[int]): List of image identifiers</span></span>
<span id="cb133-20"><a href="#cb133-20" aria-hidden="true" tabindex="-1"></a><span class="co">                - "category_id" (List[int]): List of object category IDs</span></span>
<span id="cb133-21"><a href="#cb133-21" aria-hidden="true" tabindex="-1"></a><span class="co">                - "bbox" (List[List[float]]): List of bounding boxes as [x, y, width, height]</span></span>
<span id="cb133-22"><a href="#cb133-22" aria-hidden="true" tabindex="-1"></a><span class="co">                - "iscrowd" (List[int]): List of crowd indicators (0 or 1)</span></span>
<span id="cb133-23"><a href="#cb133-23" aria-hidden="true" tabindex="-1"></a><span class="co">                - "area" (List[float]): List of object areas</span></span>
<span id="cb133-24"><a href="#cb133-24" aria-hidden="true" tabindex="-1"></a><span class="co">            - "label_source" (List[str]): List of label sources</span></span>
<span id="cb133-25"><a href="#cb133-25" aria-hidden="true" tabindex="-1"></a><span class="co">            - "image_source" (List[str]): List of image sources</span></span>
<span id="cb133-26"><a href="#cb133-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-27"><a href="#cb133-27" aria-hidden="true" tabindex="-1"></a><span class="co">        image_processor: An image processor object to preprocess images for model input.</span></span>
<span id="cb133-28"><a href="#cb133-28" aria-hidden="true" tabindex="-1"></a><span class="co">            For example, can be `transformers.RTDetrDetrImageProcessor`.</span></span>
<span id="cb133-29"><a href="#cb133-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-30"><a href="#cb133-30" aria-hidden="true" tabindex="-1"></a><span class="co">        transforms (optional): Image and annotations transforms for data augmentation.</span></span>
<span id="cb133-31"><a href="#cb133-31" aria-hidden="true" tabindex="-1"></a><span class="co">            Defaults to None.</span></span>
<span id="cb133-32"><a href="#cb133-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-33"><a href="#cb133-33" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb133-34"><a href="#cb133-34" aria-hidden="true" tabindex="-1"></a><span class="co">        dict: Preprocessed batch with images and annotations converted to tensors</span></span>
<span id="cb133-35"><a href="#cb133-35" aria-hidden="true" tabindex="-1"></a><span class="co">            in the format required for a `transformers.RTDetrV2ForObjectDetection` model.</span></span>
<span id="cb133-36"><a href="#cb133-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-37"><a href="#cb133-37" aria-hidden="true" tabindex="-1"></a><span class="co">    Note:</span></span>
<span id="cb133-38"><a href="#cb133-38" aria-hidden="true" tabindex="-1"></a><span class="co">        The `format_image_annotations_as_coco` function converts the input annotation format to COCO</span></span>
<span id="cb133-39"><a href="#cb133-39" aria-hidden="true" tabindex="-1"></a><span class="co">        format before applying the image_processor. This is required as the image_processor is designed</span></span>
<span id="cb133-40"><a href="#cb133-40" aria-hidden="true" tabindex="-1"></a><span class="co">        to handle COCO format annotations. </span></span>
<span id="cb133-41"><a href="#cb133-41" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb133-42"><a href="#cb133-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Create empty lists to store images and annotations</span></span>
<span id="cb133-43"><a href="#cb133-43" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> []</span>
<span id="cb133-44"><a href="#cb133-44" aria-hidden="true" tabindex="-1"></a>    coco_annotations <span class="op">=</span> [] </span>
<span id="cb133-45"><a href="#cb133-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-46"><a href="#cb133-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Extract the image, image_id and annotations from the examples</span></span>
<span id="cb133-47"><a href="#cb133-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> image, image_id, annotations_dict <span class="kw">in</span> <span class="bu">zip</span>(examples[<span class="st">"image"</span>], </span>
<span id="cb133-48"><a href="#cb133-48" aria-hidden="true" tabindex="-1"></a>                                                 examples[<span class="st">"image_id"</span>], </span>
<span id="cb133-49"><a href="#cb133-49" aria-hidden="true" tabindex="-1"></a>                                                 examples[<span class="st">"annotations"</span>]):</span>
<span id="cb133-50"><a href="#cb133-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-51"><a href="#cb133-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 4. Create lists of annotation attributes</span></span>
<span id="cb133-52"><a href="#cb133-52" aria-hidden="true" tabindex="-1"></a>        bbox_list <span class="op">=</span> annotations_dict[<span class="st">"bbox"</span>]</span>
<span id="cb133-53"><a href="#cb133-53" aria-hidden="true" tabindex="-1"></a>        category_list <span class="op">=</span> annotations_dict[<span class="st">"category_id"</span>]</span>
<span id="cb133-54"><a href="#cb133-54" aria-hidden="true" tabindex="-1"></a>        area_list <span class="op">=</span> annotations_dict[<span class="st">"area"</span>]</span>
<span id="cb133-55"><a href="#cb133-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-56"><a href="#cb133-56" aria-hidden="true" tabindex="-1"></a>        <span class="co">###</span></span>
<span id="cb133-57"><a href="#cb133-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 5. Note: Could optionally apply a transform/augmentation here.</span></span>
<span id="cb133-58"><a href="#cb133-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> transforms:</span>
<span id="cb133-59"><a href="#cb133-59" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Perform transform on image/boxes</span></span>
<span id="cb133-60"><a href="#cb133-60" aria-hidden="true" tabindex="-1"></a>            <span class="cf">pass</span></span>
<span id="cb133-61"><a href="#cb133-61" aria-hidden="true" tabindex="-1"></a>        <span class="co">###</span></span>
<span id="cb133-62"><a href="#cb133-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-63"><a href="#cb133-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 6. Format the annotations into COCO format</span></span>
<span id="cb133-64"><a href="#cb133-64" aria-hidden="true" tabindex="-1"></a>        cooc_format_annotations <span class="op">=</span> format_image_annotations_as_coco(image_id<span class="op">=</span>image_id,</span>
<span id="cb133-65"><a href="#cb133-65" aria-hidden="true" tabindex="-1"></a>                                                                   categories<span class="op">=</span>category_list,</span>
<span id="cb133-66"><a href="#cb133-66" aria-hidden="true" tabindex="-1"></a>                                                                   areas<span class="op">=</span>area_list,</span>
<span id="cb133-67"><a href="#cb133-67" aria-hidden="true" tabindex="-1"></a>                                                                   bboxes<span class="op">=</span>bbox_list)</span>
<span id="cb133-68"><a href="#cb133-68" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb133-69"><a href="#cb133-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 7. Add images/annotations to their respective lists</span></span>
<span id="cb133-70"><a href="#cb133-70" aria-hidden="true" tabindex="-1"></a>        images.append(image) <span class="co"># Note: may need to open image if it is an image path rather than PIL.Image</span></span>
<span id="cb133-71"><a href="#cb133-71" aria-hidden="true" tabindex="-1"></a>        coco_annotations.append(cooc_format_annotations)</span>
<span id="cb133-72"><a href="#cb133-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-73"><a href="#cb133-73" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb133-74"><a href="#cb133-74" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 8. Apply the image processor to lists of images and annotations</span></span>
<span id="cb133-75"><a href="#cb133-75" aria-hidden="true" tabindex="-1"></a>    preprocessed_batch <span class="op">=</span> image_processor.preprocess(images<span class="op">=</span>images,</span>
<span id="cb133-76"><a href="#cb133-76" aria-hidden="true" tabindex="-1"></a>                                                    annotations<span class="op">=</span>coco_annotations,</span>
<span id="cb133-77"><a href="#cb133-77" aria-hidden="true" tabindex="-1"></a>                                                    return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb133-78"><a href="#cb133-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-79"><a href="#cb133-79" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 9. Return the preprocessed batch</span></span>
<span id="cb133-80"><a href="#cb133-80" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> preprocessed_batch</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Nice!</p>
<p>Now how about we test it out on our <code>group_of_samples</code>?</p>
<div id="cell-174" class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb134"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a>preprocessed_samples <span class="op">=</span> preprocess_batch(examples<span class="op">=</span>group_of_samples,</span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a>                                        image_processor<span class="op">=</span>image_processor)</span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-4"><a href="#cb134-4" aria-hidden="true" tabindex="-1"></a>preprocessed_samples.keys()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="68">
<pre><code>dict_keys(['pixel_mask', 'pixel_values', 'labels'])</code></pre>
</div>
</div>
<p>Perfect, we get the same <code>keys()</code> as with our single sample.</p>
<p>Except this time, we’ve got multiple samples, let’s check the shape.</p>
<div id="cell-176" class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb136"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the shape of our preprocessed samples</span></span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Shape of preprocessed samples: </span><span class="sc">{</span>preprocessed_samples[<span class="st">'pixel_values'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [batch_size, colour_channels, height, width]"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Shape of preprocessed samples: torch.Size([3, 3, 640, 480]) -&gt; [batch_size, colour_channels, height, width]</code></pre>
</div>
</div>
<p>Wonderful, our batch of three samples have been preprocessed and are ready for input to our model.</p>
</section>
<section id="applying-our-preprocessing-function-to-each-data-split" class="level3" data-number="13.3">
<h3 data-number="13.3" class="anchored" data-anchor-id="applying-our-preprocessing-function-to-each-data-split"><span class="header-section-number">13.3</span> Applying our preprocessing function to each data split</h3>
<p>We’ve seen our <code>preprocess_batch</code> function in action on a small group of samples.</p>
<p>Now let’s apply it to our different data splits.</p>
<p>To do so, we can call the <a href="https://huggingface.co/docs/datasets/en/package_reference/main_classes#datasets.Dataset.with_transform"><code>with_transform()</code></a> method on our target dataset split and pass it our desired <code>transform</code>.</p>
<p>Using <code>with_transform()</code> means our transformations will be applied on-the-fly when we call on our split datasets.</p>
<p>Because the <code>with_transform()</code> method expects a callable with a single argument (the input examples), we’ll turn our <code>preprocess_batch</code> into a <a href="https://docs.python.org/3/library/functools.html#functools.partial">Python partial function</a>.</p>
<p>Doing this will mean we can prefill the <code>image_processor</code> and optionally the <code>transforms</code> parameter of our <code>preprocess_batch</code> function meaning it will only take <code>examples</code> as input, this is inline with the <code>with_transform()</code> method.</p>
<div id="cell-179" class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb138"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a partial function for preprocessing</span></span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb138-3"><a href="#cb138-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb138-4"><a href="#cb138-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: Could create separate preprocess functions with different inputs depending on the split </span></span>
<span id="cb138-5"><a href="#cb138-5" aria-hidden="true" tabindex="-1"></a><span class="co"># (e.g. use data augmentation on training but not on validation/test)</span></span>
<span id="cb138-6"><a href="#cb138-6" aria-hidden="true" tabindex="-1"></a>preprocess_batch_partial <span class="op">=</span> partial(preprocess_batch,</span>
<span id="cb138-7"><a href="#cb138-7" aria-hidden="true" tabindex="-1"></a>                                   image_processor<span class="op">=</span>image_processor,</span>
<span id="cb138-8"><a href="#cb138-8" aria-hidden="true" tabindex="-1"></a>                                   transforms<span class="op">=</span><span class="va">None</span>) <span class="co"># could use transforms here if wanted</span></span>
<span id="cb138-9"><a href="#cb138-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb138-10"><a href="#cb138-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect the preprocess_batch_partial function</span></span>
<span id="cb138-11"><a href="#cb138-11" aria-hidden="true" tabindex="-1"></a><span class="co"># preprocess_batch_partial</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Beautiful, now let’s pass the <code>preprocess_batch_partial</code> function to the <code>with_transform()</code> method on each of our data splits.</p>
<div id="cell-181" class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb139"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a copy of the original dataset </span></span>
<span id="cb139-2"><a href="#cb139-2" aria-hidden="true" tabindex="-1"></a><span class="co"># (we don't need to do this, this is just so we can inspect the original dataset later on)</span></span>
<span id="cb139-3"><a href="#cb139-3" aria-hidden="true" tabindex="-1"></a>processed_dataset <span class="op">=</span> dataset.copy()</span>
<span id="cb139-4"><a href="#cb139-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-5"><a href="#cb139-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply the preprocessing function to the datasets (the preprocessing will happen on the fly, e.g. when the dataset is called rather than in-place)</span></span>
<span id="cb139-6"><a href="#cb139-6" aria-hidden="true" tabindex="-1"></a>processed_dataset[<span class="st">"train"</span>] <span class="op">=</span> dataset[<span class="st">"train"</span>].with_transform(transform<span class="op">=</span>preprocess_batch_partial)</span>
<span id="cb139-7"><a href="#cb139-7" aria-hidden="true" tabindex="-1"></a>processed_dataset[<span class="st">"validation"</span>] <span class="op">=</span> dataset[<span class="st">"validation"</span>].with_transform(transform<span class="op">=</span>preprocess_batch_partial)</span>
<span id="cb139-8"><a href="#cb139-8" aria-hidden="true" tabindex="-1"></a>processed_dataset[<span class="st">"test"</span>] <span class="op">=</span> dataset[<span class="st">"test"</span>].with_transform(transform<span class="op">=</span>preprocess_batch_partial)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now when we get (via <code>__getitem__</code>) one of our samples from a <code>processed_dataset</code> split, it will be preprocessed on the fly.</p>
<div id="cell-183" class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb140"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get an item from the dataset (in will be preprocessed as we get it)</span></span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a>processed_dataset[<span class="st">"train"</span>][<span class="dv">42</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="72">
<pre><code>{'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         ...,
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1]]),
 'pixel_values': tensor([[[0.0824, 0.2275, 0.2471,  ..., 0.3255, 0.3059, 0.3804],
          [0.2588, 0.1608, 0.2706,  ..., 0.4000, 0.4588, 0.4667],
          [0.2706, 0.2588, 0.2549,  ..., 0.7059, 0.5686, 0.4431],
          ...,
          [0.4941, 0.3137, 0.2235,  ..., 0.2745, 0.2314, 0.1647],
          [0.4824, 0.5490, 0.2392,  ..., 0.1725, 0.1451, 0.2157],
          [0.3176, 0.5294, 0.3137,  ..., 0.2039, 0.1059, 0.1490]],
 
         [[0.0941, 0.2392, 0.2549,  ..., 0.3176, 0.2941, 0.3765],
          [0.2706, 0.1686, 0.2784,  ..., 0.3922, 0.4471, 0.4588],
          [0.2784, 0.2667, 0.2588,  ..., 0.6980, 0.5569, 0.4353],
          ...,
          [0.4667, 0.2824, 0.1882,  ..., 0.2902, 0.2549, 0.2000],
          [0.4510, 0.5098, 0.2000,  ..., 0.1922, 0.1843, 0.2588],
          [0.2824, 0.4902, 0.2706,  ..., 0.2353, 0.1529, 0.2000]],
 
         [[0.0353, 0.1725, 0.1647,  ..., 0.1686, 0.1373, 0.1804],
          [0.1882, 0.1020, 0.1725,  ..., 0.2353, 0.2824, 0.2667],
          [0.1922, 0.1804, 0.1490,  ..., 0.5412, 0.3804, 0.2471],
          ...,
          [0.3137, 0.1922, 0.1255,  ..., 0.1451, 0.1333, 0.0745],
          [0.2863, 0.3922, 0.1333,  ..., 0.0667, 0.0549, 0.1137],
          [0.1373, 0.3490, 0.2000,  ..., 0.0863, 0.0118, 0.0510]]]),
 'labels': {'size': tensor([640, 480]), 'image_id': tensor([663]), 'class_labels': tensor([1, 5]), 'boxes': tensor([[0.6095, 0.6822, 0.3579, 0.5368],
         [0.4943, 0.4007, 0.0804, 0.0780]]), 'area': tensor([59021.8906,  1928.0699]), 'iscrowd': tensor([0, 0]), 'orig_size': tensor([1280,  960])}}</code></pre>
</div>
</div>
<p>And the same happens when we get multiple (a batch) samples!</p>
<div id="cell-185" class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb142"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb142-1"><a href="#cb142-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Now when we call one or more of our samples, the preprocessing will take place</span></span>
<span id="cb142-2"><a href="#cb142-2" aria-hidden="true" tabindex="-1"></a>batch_size_to_get <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb142-3"><a href="#cb142-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Shape of preprocessed images: </span><span class="sc">{</span>processed_dataset[<span class="st">'train'</span>][:batch_size_to_get][<span class="st">'pixel_values'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; [batch_size, colour_channels, height, width]"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Shape of preprocessed images: torch.Size([32, 3, 640, 480]) -&gt; [batch_size, colour_channels, height, width]</code></pre>
</div>
</div>
<div id="cell-186" class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb144"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We can pass these straight to our model! (note: may take a while if it's on CPU)</span></span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a><span class="co"># model(processed_dataset["train"][:batch_size_to_get]["pixel_values"]) # uncomment to view output</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="creating-a-collation-function" class="level3" data-number="13.4">
<h3 data-number="13.4" class="anchored" data-anchor-id="creating-a-collation-function"><span class="header-section-number">13.4</span> Creating a collation function</h3>
<p>We now preprocess multiple samples at once.</p>
<p>Time to create a collation function which will tell our model trainer how to stack these samples together into batches.</p>
<p>We do this because processing more samples at once (e.g.&nbsp;32 samples in a batch) in a batch is generally more efficient than one sample at a time or trying to process all samples at once.</p>
<p>Our collation function will be used for the <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.data_collator"><code>data_collator</code> parameter</a> in our <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer"><code>transformers.Trainer</code></a> instance later on.</p>
<p>The input to our data collation function will be the output of <code>image_processor.preprocess()</code> (a preprocessed sample).</p>
<p>And the output will be passed as a batch (we’ll define the batch size later on) to our model’s <a href="https://huggingface.co/docs/transformers/en/model_doc/conditional_detr#transformers.ConditionalDetrModel.forward"><code>forward()</code> method</a>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>What batch size should I use?</p>
<p>You should generally use the batch size which uses the maximum amount of GPU memory you have.</p>
<p>For example, if you have 16GB of GPU memory and a batch size of 32 only uses 8GB of that memory, you should try doubling the batch size to 64.</p>
<p>The ideal batch size for a given dataset/model/hardware is often discovered in an iterative process.</p>
</div>
</div>
<div id="cell-188" class="cell" data-execution_count="75">
<div class="sourceCode cell-code" id="cb145"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb145-1"><a href="#cb145-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List, Dict, Any</span>
<span id="cb145-2"><a href="#cb145-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-3"><a href="#cb145-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> data_collate_function(preprocessed_batch: List[Dict[<span class="bu">str</span>, Any]]) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, Any]:</span>
<span id="cb145-4"><a href="#cb145-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Stacks together groups of preprocessed samples into batches for our model.</span></span>
<span id="cb145-5"><a href="#cb145-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-6"><a href="#cb145-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb145-7"><a href="#cb145-7" aria-hidden="true" tabindex="-1"></a><span class="co">        preprocessed_batch: A list of dictionaries where each dictionary represnets a preprocessed sample.</span></span>
<span id="cb145-8"><a href="#cb145-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-9"><a href="#cb145-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb145-10"><a href="#cb145-10" aria-hidden="true" tabindex="-1"></a><span class="co">        collated_data: A dictionary containing the batched data ready in the format our model</span></span>
<span id="cb145-11"><a href="#cb145-11" aria-hidden="true" tabindex="-1"></a><span class="co">            is expecting. The dictionary has the following keys: </span></span>
<span id="cb145-12"><a href="#cb145-12" aria-hidden="true" tabindex="-1"></a><span class="co">                - "pixel_values": A stacked tensor of preprocessed pixel values.</span></span>
<span id="cb145-13"><a href="#cb145-13" aria-hidden="true" tabindex="-1"></a><span class="co">                - "labels": A list of label dictionaries.</span></span>
<span id="cb145-14"><a href="#cb145-14" aria-hidden="true" tabindex="-1"></a><span class="co">                - "pixel_mask": (Optional) A stacked tensor of pixel masks (this will be present </span></span>
<span id="cb145-15"><a href="#cb145-15" aria-hidden="true" tabindex="-1"></a><span class="co">                    only if the input contains a "pixel_mask" key.</span></span>
<span id="cb145-16"><a href="#cb145-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb145-17"><a href="#cb145-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create an empty dictionary (our model wants a dictionary input) </span></span>
<span id="cb145-18"><a href="#cb145-18" aria-hidden="true" tabindex="-1"></a>    collated_data <span class="op">=</span> {} </span>
<span id="cb145-19"><a href="#cb145-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-20"><a href="#cb145-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stack together a collection of pixel_values tensors</span></span>
<span id="cb145-21"><a href="#cb145-21" aria-hidden="true" tabindex="-1"></a>    collated_data[<span class="st">"pixel_values"</span>] <span class="op">=</span> torch.stack([sample[<span class="st">"pixel_values"</span>] <span class="cf">for</span> sample <span class="kw">in</span> preprocessed_batch])</span>
<span id="cb145-22"><a href="#cb145-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-23"><a href="#cb145-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the labels (these are dictionaries so no need to use torch.stack)</span></span>
<span id="cb145-24"><a href="#cb145-24" aria-hidden="true" tabindex="-1"></a>    collated_data[<span class="st">"labels"</span>] <span class="op">=</span> [sample[<span class="st">"labels"</span>] <span class="cf">for</span> sample <span class="kw">in</span> preprocessed_batch]</span>
<span id="cb145-25"><a href="#cb145-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-26"><a href="#cb145-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If there is a pixel_mask key, return the pixel_mask's as well</span></span>
<span id="cb145-27"><a href="#cb145-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"pixel_mask"</span> <span class="kw">in</span> preprocessed_batch[<span class="dv">0</span>]:</span>
<span id="cb145-28"><a href="#cb145-28" aria-hidden="true" tabindex="-1"></a>        collated_data[<span class="st">"pixel_mask"</span>] <span class="op">=</span> torch.stack([sample[<span class="st">"pixel_mask"</span>] <span class="cf">for</span> sample <span class="kw">in</span> preprocessed_batch])</span>
<span id="cb145-29"><a href="#cb145-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-30"><a href="#cb145-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> collated_data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Excellent! Now let’s try out our data collation function.</p>
<div id="cell-190" class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb146"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb146-2"><a href="#cb146-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb146-3"><a href="#cb146-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Try data_collate_function </span></span>
<span id="cb146-4"><a href="#cb146-4" aria-hidden="true" tabindex="-1"></a>example_collated_data_batch <span class="op">=</span> data_collate_function(processed_dataset[<span class="st">"train"</span>].select(<span class="bu">range</span>(<span class="dv">32</span>)))</span>
<span id="cb146-5"><a href="#cb146-5" aria-hidden="true" tabindex="-1"></a>example_collated_data_batch.keys()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 9.67 s, sys: 319 ms, total: 9.99 s
Wall time: 865 ms</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="76">
<pre><code>dict_keys(['pixel_values', 'labels', 'pixel_mask'])</code></pre>
</div>
</div>
<p>Perfect! Looks like it worked. We’ve now got a batch of preprocessed images and label pairs.</p>
<p>Let’s check the shapes.</p>
<div id="cell-192" class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb149"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb149-1"><a href="#cb149-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check shapes of batched preprocessed samples</span></span>
<span id="cb149-2"><a href="#cb149-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Batch of pixel value shapes: </span><span class="sc">{</span>example_collated_data_batch[<span class="st">'pixel_values'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb149-3"><a href="#cb149-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Batch of labels: </span><span class="sc">{</span>example_collated_data_batch[<span class="st">'labels'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb149-4"><a href="#cb149-4" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="st">"pixel_mask"</span> <span class="kw">in</span> example_collated_data_batch:</span>
<span id="cb149-5"><a href="#cb149-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"[INFO] Batch of pixel masks: </span><span class="sc">{</span>example_collated_data_batch[<span class="st">'pixel_mask'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Batch of pixel value shapes: torch.Size([32, 3, 640, 480])
[INFO] Batch of labels: [{'size': tensor([640, 480]), 'image_id': tensor([69]), 'class_labels': tensor([5, 0, 1, 4, 4, 4, 4, 4]), 'boxes': tensor([[0.4675, 0.5152, 0.1846, 0.2045],
        [0.5092, 0.5843, 0.3970, 0.3951],
        [0.2719, 0.5861, 0.3738, 0.2471],
        [0.1023, 0.6896, 0.2019, 0.1655],
        [0.3902, 0.0924, 0.1530, 0.0898],
        [0.5345, 0.0871, 0.0252, 0.0556],
        [0.6370, 0.0877, 0.1357, 0.0899],
        [0.9383, 0.0634, 0.0789, 0.0627]]), 'area': tensor([11597.7402, 48180.5664, 28372.1094, 10266.5547,  4223.3750,   430.7600,
         3749.3826,  1517.7850]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([1027]), 'class_labels': tensor([5, 4, 1, 0, 0]), 'boxes': tensor([[0.4669, 0.5782, 0.1456, 0.1290],
        [0.5031, 0.6013, 0.0410, 0.0237],
        [0.5269, 0.6380, 0.1138, 0.1280],
        [0.3863, 0.5047, 0.4801, 0.3840],
        [0.1074, 0.4195, 0.2101, 0.3353]]), 'area': tensor([ 5770.2451,   298.4550,  4471.7402, 56633.0859, 21642.4102]), 'iscrowd': tensor([0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([1092]), 'class_labels': tensor([2, 5, 1, 0]), 'boxes': tensor([[0.1943, 0.1126, 0.1849, 0.0794],
        [0.5387, 0.5818, 0.3646, 0.2689],
        [0.3515, 0.7725, 0.3171, 0.2903],
        [0.5404, 0.4307, 0.6236, 0.4566]]), 'area': tensor([ 4508.5000, 30117.5000, 28278.7598, 87485.0391]), 'iscrowd': tensor([0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([228]), 'class_labels': tensor([0]), 'boxes': tensor([[0.5187, 0.5418, 0.4982, 0.5698]]), 'area': tensor([87218.0078]), 'iscrowd': tensor([0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([511]), 'class_labels': tensor([5, 1]), 'boxes': tensor([[0.5284, 0.5886, 0.2903, 0.3347],
        [0.7784, 0.7873, 0.4400, 0.4222]]), 'area': tensor([29848.7695, 57066.2383]), 'iscrowd': tensor([0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([338]), 'class_labels': tensor([5, 0, 1]), 'boxes': tensor([[0.4990, 0.5424, 0.2227, 0.1716],
        [0.5455, 0.5335, 0.3754, 0.3595],
        [0.7111, 0.6979, 0.3313, 0.2838]]), 'area': tensor([11742.9648, 41455.0117, 28882.3496]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([405]), 'class_labels': tensor([0, 1, 5]), 'boxes': tensor([[0.4952, 0.6559, 0.6088, 0.4872],
        [0.2074, 0.7760, 0.4117, 0.4459],
        [0.4132, 0.5714, 0.0663, 0.0580]]), 'area': tensor([91107.9609, 56385.1602,  1179.7800]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([3]), 'class_labels': tensor([0, 5, 1, 4, 4, 4]), 'boxes': tensor([[0.5020, 0.4466, 0.6579, 0.5829],
        [0.5148, 0.5684, 0.2288, 0.1367],
        [0.7040, 0.7836, 0.4468, 0.4219],
        [0.3160, 0.8416, 0.3991, 0.2993],
        [0.4095, 0.0661, 0.0888, 0.0666],
        [0.7489, 0.1356, 0.3843, 0.2637]]), 'area': tensor([117809.1875,   9607.5000,  57901.5000,  36691.4023,   1814.7600,
         31125.9375]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([182]), 'class_labels': tensor([0, 1, 5]), 'boxes': tensor([[0.5786, 0.5016, 0.5992, 0.4539],
        [0.6307, 0.7197, 0.4165, 0.3323],
        [0.4415, 0.6429, 0.1546, 0.2070]]), 'area': tensor([83547.7969, 42508.7344,  9827.7900]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([640]), 'class_labels': tensor([5, 1, 0]), 'boxes': tensor([[0.5314, 0.6391, 0.2920, 0.4553],
        [0.7088, 0.7733, 0.5596, 0.4422],
        [0.5282, 0.5060, 0.5678, 0.4612]]), 'area': tensor([40839.7109, 76013.7969, 80443.1328]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([1126]), 'class_labels': tensor([5, 1, 0, 0, 4]), 'boxes': tensor([[0.4897, 0.6114, 0.2720, 0.2612],
        [0.6082, 0.7287, 0.2006, 0.2145],
        [0.4549, 0.5349, 0.4550, 0.3859],
        [0.1698, 0.4514, 0.3276, 0.2998],
        [0.6611, 0.1925, 0.4202, 0.1516]]), 'area': tensor([21821.4316, 13217.1748, 53944.8008, 30168.4121, 19574.9844]), 'iscrowd': tensor([0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([902]), 'class_labels': tensor([5, 1, 0, 4]), 'boxes': tensor([[0.5237, 0.4816, 0.0443, 0.0498],
        [0.6509, 0.3957, 0.2670, 0.1695],
        [0.3200, 0.4485, 0.6094, 0.6062],
        [0.6201, 0.1730, 0.1955, 0.0725]]), 'area': tensor([   676.8125,  13904.2754, 113490.0000,   4354.6401]), 'iscrowd': tensor([0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([166]), 'class_labels': tensor([5, 1, 0, 4, 0]), 'boxes': tensor([[0.4320, 0.5441, 0.2114, 0.1963],
        [0.2735, 0.6612, 0.3580, 0.2412],
        [0.5321, 0.5080, 0.3639, 0.3277],
        [0.1142, 0.7866, 0.2067, 0.1561],
        [0.7246, 0.4182, 0.2477, 0.2401]]), 'area': tensor([12742.1201, 26533.6406, 36624.1055,  9910.0801, 18268.9844]), 'iscrowd': tensor([0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([409]), 'class_labels': tensor([0, 4, 4, 5, 1]), 'boxes': tensor([[0.3715, 0.6465, 0.7429, 0.5014],
        [0.5047, 0.6748, 0.2114, 0.1916],
        [0.1167, 0.7180, 0.2303, 0.1904],
        [0.4180, 0.6086, 0.0883, 0.0780],
        [0.3020, 0.6926, 0.3045, 0.2649]]), 'area': tensor([114432.9375,  12437.7695,  13470.5176,   2117.8799,  24779.7324]), 'iscrowd': tensor([0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([504]), 'class_labels': tensor([1, 0]), 'boxes': tensor([[0.2105, 0.6075, 0.3550, 0.2591],
        [0.4267, 0.5508, 0.5474, 0.3703]]), 'area': tensor([28260.8398, 62271.7500]), 'iscrowd': tensor([0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([1012]), 'class_labels': tensor([0, 2]), 'boxes': tensor([[0.4518, 0.4870, 0.5355, 0.5652],
        [0.9084, 0.5812, 0.1724, 0.4217]]), 'area': tensor([92987.8359, 22334.2246]), 'iscrowd': tensor([0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([769]), 'class_labels': tensor([6, 5, 0, 2]), 'boxes': tensor([[0.7015, 0.4236, 0.5892, 0.0759],
        [0.4368, 0.4307, 0.1043, 0.1327],
        [0.2781, 0.5959, 0.3932, 0.4465],
        [0.6999, 0.3721, 0.5797, 0.7238]]), 'area': tensor([ 13744.0801,   4249.2451,  53935.3125, 128899.3125]), 'iscrowd': tensor([0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([510]), 'class_labels': tensor([5, 1]), 'boxes': tensor([[0.3557, 0.4248, 0.2382, 0.1798],
        [0.6917, 0.7145, 0.6135, 0.5677]]), 'area': tensor([ 13155.9678, 106991.8516]), 'iscrowd': tensor([0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([429]), 'class_labels': tensor([4, 0, 1, 5]), 'boxes': tensor([[0.4661, 0.8003, 0.4432, 0.1715],
        [0.4992, 0.6146, 0.9984, 0.6917],
        [0.2310, 0.6193, 0.3612, 0.2520],
        [0.4227, 0.5342, 0.0790, 0.0650]]), 'area': tensor([ 23349.3125, 212163.9688,  27969.4199,   1576.6400]), 'iscrowd': tensor([0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([714]), 'class_labels': tensor([5, 1, 0]), 'boxes': tensor([[0.3350, 0.6024, 0.2067, 0.2968],
        [0.2292, 0.7662, 0.4445, 0.4472],
        [0.5794, 0.6870, 0.6228, 0.5439]]), 'area': tensor([ 18843.0391,  61060.7695, 104064.4922]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([301]), 'class_labels': tensor([5, 1, 0]), 'boxes': tensor([[0.4706, 0.5429, 0.0994, 0.0970],
        [0.2963, 0.6009, 0.3128, 0.2155],
        [0.4525, 0.4761, 0.8737, 0.6209]]), 'area': tensor([  2959.7849,  20713.1934, 166669.5625]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([816]), 'class_labels': tensor([6, 5, 0]), 'boxes': tensor([[0.7607, 0.7381, 0.4707, 0.3945],
        [0.5418, 0.5427, 0.1593, 0.1055],
        [0.4945, 0.5723, 0.5662, 0.4344]]), 'area': tensor([57052.3750,  5160.3750, 75560.3984]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([392]), 'class_labels': tensor([5, 1, 4, 4, 4]), 'boxes': tensor([[0.4599, 0.6063, 0.0836, 0.0493],
        [0.2533, 0.7866, 0.5063, 0.4221],
        [0.5349, 0.6495, 0.7540, 0.5713],
        [0.8369, 0.9173, 0.3234, 0.1632],
        [0.5333, 0.9232, 0.1924, 0.1514]]), 'area': tensor([  1266.7325,  65646.4531, 132310.6406,  16215.8623,   8948.7148]), 'iscrowd': tensor([0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([439]), 'class_labels': tensor([5, 0, 1]), 'boxes': tensor([[0.4638, 0.6099, 0.2429, 0.3724],
        [0.4283, 0.5034, 0.4528, 0.3891],
        [0.7492, 0.6229, 0.4982, 0.4316]]), 'area': tensor([27791.6094, 54120.1484, 66053.2266]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([319]), 'class_labels': tensor([5, 1, 0, 4, 4]), 'boxes': tensor([[0.4927, 0.4708, 0.1688, 0.0946],
        [0.7135, 0.5453, 0.3644, 0.2980],
        [0.4998, 0.5359, 0.6276, 0.4492],
        [0.5456, 0.8173, 0.1482, 0.1584],
        [0.4667, 0.9237, 0.1009, 0.1277]]), 'area': tensor([ 4904.5498, 33353.4297, 86609.3750,  7214.6099,  3960.7876]), 'iscrowd': tensor([0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([1106]), 'class_labels': tensor([5, 1, 4, 0]), 'boxes': tensor([[0.4597, 0.4787, 0.1184, 0.0961],
        [0.5932, 0.6244, 0.2401, 0.2405],
        [0.6587, 0.7589, 0.2219, 0.1490],
        [0.3902, 0.5373, 0.7309, 0.5996]]), 'area': tensor([  3496.2749,  17742.7383,  10154.7754, 134638.6875]), 'iscrowd': tensor([0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([763]), 'class_labels': tensor([0, 0, 0, 5, 1]), 'boxes': tensor([[0.4510, 0.5231, 0.5637, 0.4548],
        [0.7868, 0.4366, 0.4092, 0.3365],
        [0.2204, 0.4396, 0.3318, 0.3187],
        [0.5497, 0.5397, 0.2101, 0.0714],
        [0.6421, 0.6682, 0.3070, 0.2901]]), 'area': tensor([78758.1328, 42294.7383, 32479.0371,  4608.8452, 27355.5273]), 'iscrowd': tensor([0, 0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([379]), 'class_labels': tensor([5, 1]), 'boxes': tensor([[0.5053, 0.5406, 0.5852, 0.7876],
        [0.7293, 0.6370, 0.5284, 0.4556]]), 'area': tensor([141587.6406,  73964.3438]), 'iscrowd': tensor([0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([394]), 'class_labels': tensor([1, 5, 0]), 'boxes': tensor([[0.2053, 0.7470, 0.4101, 0.4966],
        [0.4299, 0.5713, 0.1728, 0.0933],
        [0.4994, 0.6560, 0.9984, 0.6693]]), 'area': tensor([ 62568.7734,   4952.1152, 205286.7344]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([665]), 'class_labels': tensor([0, 2]), 'boxes': tensor([[0.5282, 0.6071, 0.4164, 0.3630],
        [0.6520, 0.8419, 0.5095, 0.2905]]), 'area': tensor([46425.1562, 45461.8438]), 'iscrowd': tensor([0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([362]), 'class_labels': tensor([5, 1, 0]), 'boxes': tensor([[0.4643, 0.5164, 0.3293, 0.3087],
        [0.6197, 0.7712, 0.7412, 0.4446],
        [0.4982, 0.5305, 0.9742, 0.8731]]), 'area': tensor([ 31222.7773, 101242.8906, 261294.8750]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}, {'size': tensor([640, 480]), 'image_id': tensor([1019]), 'class_labels': tensor([5, 1, 0]), 'boxes': tensor([[0.4699, 0.5841, 0.2358, 0.3263],
        [0.5916, 0.6374, 0.2653, 0.2050],
        [0.4858, 0.5195, 0.6066, 0.5119]]), 'area': tensor([23641.8203, 16708.3203, 95380.7422]), 'iscrowd': tensor([0, 0, 0]), 'orig_size': tensor([1280,  960])}]
[INFO] Batch of pixel masks: torch.Size([32, 640, 480])</code></pre>
</div>
</div>
<p>Now let’s try to pass the <code>"pixel_values"</code> through our model.</p>
<div id="cell-194" class="cell" data-execution_count="78">
<div class="sourceCode cell-code" id="cb151"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb151-1"><a href="#cb151-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time </span>
<span id="cb151-2"><a href="#cb151-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-3"><a href="#cb151-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Try pass a batch through our model (note: this will be relatively slow if our model is on the CPU)</span></span>
<span id="cb151-4"><a href="#cb151-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_model()</span>
<span id="cb151-5"><a href="#cb151-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-6"><a href="#cb151-6" aria-hidden="true" tabindex="-1"></a><span class="co"># example_batch_outputs = model(example_collated_data_batch["pixel_values"])</span></span>
<span id="cb151-7"><a href="#cb151-7" aria-hidden="true" tabindex="-1"></a>example_batch_outputs <span class="op">=</span> model(example_collated_data_batch[<span class="st">"pixel_values"</span>])</span>
<span id="cb151-8"><a href="#cb151-8" aria-hidden="true" tabindex="-1"></a><span class="co"># example_batch_outputs # uncomment for full output</span></span>
<span id="cb151-9"><a href="#cb151-9" aria-hidden="true" tabindex="-1"></a>example_batch_outputs.keys()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Some weights of RTDetrV2ForObjectDetection were not initialized from the model checkpoint at PekingU/rtdetr_v2_r50vd and are newly initialized because the shapes did not match:
- model.decoder.class_embed.0.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.0.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.decoder.class_embed.1.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.1.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.decoder.class_embed.2.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.2.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.decoder.class_embed.3.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.3.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.decoder.class_embed.4.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.4.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.decoder.class_embed.5.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.5.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.denoising_class_embed.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([8, 256]) in the model instantiated
- model.enc_score_head.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.enc_score_head.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 1min 44s, sys: 1min 7s, total: 2min 52s
Wall time: 14.2 s</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="78">
<pre><code>odict_keys(['logits', 'pred_boxes', 'last_hidden_state', 'intermediate_hidden_states', 'intermediate_logits', 'intermediate_reference_points', 'encoder_last_hidden_state', 'init_reference_points', 'enc_topk_logits', 'enc_topk_bboxes', 'enc_outputs_class', 'enc_outputs_coord_logits'])</code></pre>
</div>
</div>
<div id="cell-195" class="cell" data-execution_count="79">
<div class="sourceCode cell-code" id="cb155"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb155-1"><a href="#cb155-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We get 300 predictions per image in our batch, each with a logit value for each of the classes in our dataset </span></span>
<span id="cb155-2"><a href="#cb155-2" aria-hidden="true" tabindex="-1"></a>example_batch_outputs.logits.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="79">
<pre><code>torch.Size([32, 300, 7])</code></pre>
</div>
</div>
<p>This is what will happen during training, our model will continually go over batches (the size of these batches will be defined by us) over data and try to match its own predictions with the ground truth labels.</p>
<p>In summary, we’ve created two major steps:</p>
<ol type="1">
<li><code>preprocess_batch</code> - Preprocesses single or groups of samples into the specific format required by our model.</li>
<li><code>data_collate_function</code> - Stacks together groups/batches of samples to be passed to our model’s <code>forward()</code> method.</li>
</ol>
</section>
</section>
<section id="setting-up-trainingarguments-and-a-trainer-instance-to-train-our-model" class="level2" data-number="14">
<h2 data-number="14" class="anchored" data-anchor-id="setting-up-trainingarguments-and-a-trainer-instance-to-train-our-model"><span class="header-section-number">14</span> Setting up TrainingArguments and a Trainer instance to train our model</h2>
<p>Data ready and prepared, time to train a model!</p>
<p>We’ll use <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments"><code>transformers.TrainingArguments</code></a> to set various hyperparameters for our model (many of these will be set by default, however, we can tweak them to our liking).</p>
<p>We’ll also create an instance of <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer"><code>transformers.Trainer</code></a> which we can pass our preprocessed datasets for it to train/evaluate on.</p>
<p>To train a model, we’ll go through the following steps:</p>
<ol type="1">
<li>Create a fresh instance of our model using the <code>create_model()</code> function.</li>
<li>Make a directory for saving our trained models to.</li>
<li>Define our model’s hyperparameters using <code>transformers.TrainingArguments</code>, we’ll take many of these settings from the assosciated research papers that introduced our model.</li>
<li>Create an evaluation function we can pass to our <code>transformers.Trainer</code> instance as the <code>compute_metrics</code> parameter to evaluate our model.</li>
<li>Create an instance of <code>transformers.Trainer</code> and pass it our training arguments from 2 as well as our preprocessed data.</li>
<li>Call <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.train"><code>transformers.Trainer.train()</code></a> to train the model from 1 on our own data.</li>
</ol>
<p>Let’s do it!</p>
<div id="cell-198" class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb157"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb157-1"><a href="#cb157-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Create a model instance </span></span>
<span id="cb157-2"><a href="#cb157-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> create_model()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Some weights of RTDetrV2ForObjectDetection were not initialized from the model checkpoint at PekingU/rtdetr_v2_r50vd and are newly initialized because the shapes did not match:
- model.decoder.class_embed.0.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.0.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.decoder.class_embed.1.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.1.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.decoder.class_embed.2.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.2.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.decoder.class_embed.3.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.3.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.decoder.class_embed.4.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.4.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.decoder.class_embed.5.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.decoder.class_embed.5.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
- model.denoising_class_embed.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([8, 256]) in the model instantiated
- model.enc_score_head.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([7]) in the model instantiated
- model.enc_score_head.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre>
</div>
</div>
<p>Model ready, let’s now create a folder where we can save our trained models to.</p>
<div id="cell-200" class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb159"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb159-1"><a href="#cb159-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb159-2"><a href="#cb159-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb159-3"><a href="#cb159-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Make a models directory for saving models</span></span>
<span id="cb159-4"><a href="#cb159-4" aria-hidden="true" tabindex="-1"></a>models_dir <span class="op">=</span> Path(<span class="st">"models"</span>)</span>
<span id="cb159-5"><a href="#cb159-5" aria-hidden="true" tabindex="-1"></a>models_dir.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Perfect! Time to setup our model’s hyperparameters with <code>transformers.TrainingArguments</code>.</p>
<section id="setting-up-our-trainingarguments" class="level3" data-number="14.1">
<h3 data-number="14.1" class="anchored" data-anchor-id="setting-up-our-trainingarguments"><span class="header-section-number">14.1</span> Setting up our TrainingArguments</h3>
<p>The <code>transformers.TrainingArguments</code> class holds many of the hyperparameters/settings for training our model.</p>
<p>Many of them are set by default in the <a href="https://huggingface.co/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2Config"><code>transformers.RTDetrV2Config</code></a> class.</p>
<p>However, we can tweak any of them to our own liking.</p>
<p>Where do we get the settings from?</p>
<p>The original <a href="https://arxiv.org/abs/2407.17140"><em>RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer</em></a> paper states that all hyperparameters are the same as the original RT-DETR (see Table A in <a href="https://arxiv.org/abs/2304.08069"><em>DETRs Beat YOLOs on Real-time Object Detection</em></a>).</p>
<p>The main hyperparameters we are going to set are:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><strong>Hyperparameter</strong></th>
<th><strong>Value</strong></th>
<th><strong>What does it do?</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>per_device_train_batch_size</code>, <code>per_device_eval_batch_size</code></td>
<td><code>16</code>, <code>32</code> or larger (hardware dependent)</td>
<td>Defines the number of samples passed to our model at one time. For example, if batch size is 16, our model will see 16 samples at a time. It’s usually best practice to set this value to the highest your hardware can handle.</td>
</tr>
<tr class="even">
<td><code>learning_rate</code></td>
<td><code>0.0001</code> (as per the listed papers)</td>
<td>Defines the multiplier on the size of gradient updates during training. Too high and gradients will explode, too low and gradients won’t update, both lead to poor training results. The papers mention two different learning rates for the backbone and the detection head, I tried these and got poor results (likely because of our smaller dataset), a single learning rate for the whole network turned out to be better.</td>
</tr>
<tr class="odd">
<td><code>weight_decay</code></td>
<td><code>0.0001</code> (as per the listed papers)</td>
<td>Prevents model weights from getting too large by applying a small decay penalty over time. This prevents a single weight providing too much information. In essence, the model is forced to learn smaller, simpler weights to represent the data. A form of regularization (overfitting prevention). See more at <a href="https://paperswithcode.com/method/weight-decay">paperswithcode.com/method/weight-decay</a>.</td>
</tr>
<tr class="even">
<td><code>max_grad_norm</code></td>
<td><code>0.1</code> (as per the listed papers)</td>
<td>Prevents gradients from getting too large during training. This will help to ensure stable training. See more at <a href="https://paperswithcode.com/method/gradient-clipping">paperswithcode.com/method/gradient-clipping</a>.</td>
</tr>
<tr class="odd">
<td><code>num_train_epochs</code></td>
<td><code>25</code> (depends on training data and available time)</td>
<td>Defines how many laps of the data your model will do. For example, setting epochs to 25 means the model will do 25 laps of the training data to learn different patterns. In practice, I’ve found this value to be a good starting point for our dataset and also because we are fine-tuning rather than training from scratch. However, if you had more data you might want to do more epochs (when training from scratch, the papers did 300 epochs).</td>
</tr>
<tr class="even">
<td><code>warmup_ratio</code></td>
<td><code>0.05</code></td>
<td>Percentage of total training steps to take learning rate from <code>0</code> to to the set value (e.g.&nbsp;<code>0.0001</code>). Can help with training stability in the early training steps of the model by not doing too large updates when first starting out. The papers state <code>2000</code> warmup steps, however, in practice I found this to be too many for our smaller dataset.</td>
</tr>
<tr class="odd">
<td><code>dataloader_num_workers</code></td>
<td><code>4</code> (hardware dependent)</td>
<td>Number of workers to load data from the CPU to the GPU. Higher is generally better if it is available, however, it can often cap out. Experimentally I’ve found that <code>0.5 * os.cpu_count()</code> generally works well.</td>
</tr>
</tbody>
</table>
<figure style="text-align: center;" class="figure">
<img src="https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-trashify-object-detection/07-hyperparameters-for-models.png" alt="An image displaying training hyperparameters for RT-DETR, with 'Table A. Main hyperparameters of RT-DETR.' on the left, listing items like 'optimizer' (AdamW), 'base learning rate' (1e-4), 'learning rate of backbone' (1e-5), and various cost and loss weights; on the right, section 'A.4 Training hyperparameters' describes the training process, mentioning the use of 'AdamW', 'gradient clipping', an 'ImageNet pretrained backbone ResNet-50 imported from Torchvision', fine-tuning the backbone with a learning rate of '10^-5', and training the transformer with a learning rate of '10^-4', with key phrases like 'gradient clipping', 'ResNet-50 is imported from Torchvision', 'fine-tune the backbone using learning rate of 10^-5', and 'train the transformer with a learning rate of 10^-4' underlined in green." style="width: 100%; max-width: 900px; height: auto;" class="figure-img">
<figcaption>
Different hyperparameter settings from the official papers for the <a href="https://arxiv.org/pdf/2304.08069">RT-DETR model</a> (left) and the original <a href="https://arxiv.org/pdf/2005.12872">DETR model</a> (right).
</figcaption>
</figure>
<p>It’s important to note that all of these values can be experimented with.</p>
<p>And just because a research paper mentions a specific value, doesn’t mean you have to use.</p>
<p>For example, all the mentioned research papers tend to focus on training a model from scratch on the COCO dataset (330k images, 80 classes).</p>
<p>Which is a much larger dataset with more classes than our dataset (1k images, 7 classes) which we are trying to fine-tune an existing model on rather than train from scratch.</p>
<p>There are many more possible arguments/settings we’ve left out in the above table but if you’d like to explore these, I’d encourage you to check out the documentation for <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments"><code>transformers.TrainingArguments</code></a>.</p>
<div id="cell-203" class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb160"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb160-1"><a href="#cb160-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Create an instance of TrainingArguments to pass to Trainer</span></span>
<span id="cb160-2"><a href="#cb160-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> TrainingArguments</span>
<span id="cb160-3"><a href="#cb160-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb160-4"><a href="#cb160-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Hardware dependent hyperparameters</span></span>
<span id="cb160-5"><a href="#cb160-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the batch size according to the memory you have available on your GPU</span></span>
<span id="cb160-6"><a href="#cb160-6" aria-hidden="true" tabindex="-1"></a><span class="co"># e.g. on my NVIDIA RTX 4090 with 24GB of VRAM, I can use a batch size of 32 </span></span>
<span id="cb160-7"><a href="#cb160-7" aria-hidden="true" tabindex="-1"></a><span class="co"># without running out of memory</span></span>
<span id="cb160-8"><a href="#cb160-8" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb160-9"><a href="#cb160-9" aria-hidden="true" tabindex="-1"></a>DATALOADER_NUM_WORKERS <span class="op">=</span> <span class="dv">4</span> <span class="co"># note: if you're on Google Colab, you may have to lower this to os.cpu_count() or to 0 </span></span>
<span id="cb160-10"><a href="#cb160-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb160-11"><a href="#cb160-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Set number of epochs to how many laps you'd like to do over the data</span></span>
<span id="cb160-12"><a href="#cb160-12" aria-hidden="true" tabindex="-1"></a>NUM_EPOCHS <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb160-13"><a href="#cb160-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb160-14"><a href="#cb160-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup hyperameters for training from the DETR paper(s)</span></span>
<span id="cb160-15"><a href="#cb160-15" aria-hidden="true" tabindex="-1"></a>LEARNING_RATE <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb160-16"><a href="#cb160-16" aria-hidden="true" tabindex="-1"></a>WEIGHT_DECAY <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb160-17"><a href="#cb160-17" aria-hidden="true" tabindex="-1"></a>MAX_GRAD_NORM <span class="op">=</span> <span class="fl">0.1</span> </span>
<span id="cb160-18"><a href="#cb160-18" aria-hidden="true" tabindex="-1"></a>WARMUP_RATIO <span class="op">=</span> <span class="fl">0.05</span> <span class="co"># learning rate warmup from 0 to learning_rate as a ratio of total steps (e.g. 0.05 = 5% of total steps)</span></span>
<span id="cb160-19"><a href="#cb160-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb160-20"><a href="#cb160-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Create directory to save models to </span></span>
<span id="cb160-21"><a href="#cb160-21" aria-hidden="true" tabindex="-1"></a>OUTPUT_DIR <span class="op">=</span> Path(models_dir, <span class="st">"rt_detrv2_finetuned_trashify_box_detector_v1"</span>)</span>
<span id="cb160-22"><a href="#cb160-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Saving model to: </span><span class="sc">{</span>OUTPUT_DIR<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb160-23"><a href="#cb160-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb160-24"><a href="#cb160-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Create TrainingArguments to pass to Trainer</span></span>
<span id="cb160-25"><a href="#cb160-25" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb160-26"><a href="#cb160-26" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span>OUTPUT_DIR,</span>
<span id="cb160-27"><a href="#cb160-27" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb160-28"><a href="#cb160-28" aria-hidden="true" tabindex="-1"></a>    per_device_eval_batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb160-29"><a href="#cb160-29" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span>LEARNING_RATE,</span>
<span id="cb160-30"><a href="#cb160-30" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span>WEIGHT_DECAY,</span>
<span id="cb160-31"><a href="#cb160-31" aria-hidden="true" tabindex="-1"></a>    max_grad_norm<span class="op">=</span>MAX_GRAD_NORM,</span>
<span id="cb160-32"><a href="#cb160-32" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span>NUM_EPOCHS,</span>
<span id="cb160-33"><a href="#cb160-33" aria-hidden="true" tabindex="-1"></a>    lr_scheduler_type<span class="op">=</span><span class="st">"linear"</span>,</span>
<span id="cb160-34"><a href="#cb160-34" aria-hidden="true" tabindex="-1"></a>    warmup_ratio<span class="op">=</span>WARMUP_RATIO, </span>
<span id="cb160-35"><a href="#cb160-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># warmup_steps=2000, # number of warmup steps from 0 to learning_rate (overrides warmup_ratio, found this to be too long for our dataset)</span></span>
<span id="cb160-36"><a href="#cb160-36" aria-hidden="true" tabindex="-1"></a>    logging_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb160-37"><a href="#cb160-37" aria-hidden="true" tabindex="-1"></a>    save_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb160-38"><a href="#cb160-38" aria-hidden="true" tabindex="-1"></a>    save_total_limit<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb160-39"><a href="#cb160-39" aria-hidden="true" tabindex="-1"></a>    remove_unused_columns<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb160-40"><a href="#cb160-40" aria-hidden="true" tabindex="-1"></a>    fp16<span class="op">=</span><span class="va">True</span>, <span class="co"># use mixed precision training</span></span>
<span id="cb160-41"><a href="#cb160-41" aria-hidden="true" tabindex="-1"></a>    dataloader_num_workers<span class="op">=</span>DATALOADER_NUM_WORKERS, <span class="co"># note: if you're on Google Colab, you may have to lower this to os.cpu_count() or to 0</span></span>
<span id="cb160-42"><a href="#cb160-42" aria-hidden="true" tabindex="-1"></a>    eval_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb160-43"><a href="#cb160-43" aria-hidden="true" tabindex="-1"></a>    load_best_model_at_end<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb160-44"><a href="#cb160-44" aria-hidden="true" tabindex="-1"></a>    metric_for_best_model<span class="op">=</span><span class="st">"eval_loss"</span>,</span>
<span id="cb160-45"><a href="#cb160-45" aria-hidden="true" tabindex="-1"></a>    greater_is_better<span class="op">=</span><span class="va">False</span>, <span class="co"># want to minimize eval_loss (e.g. lower is better)</span></span>
<span id="cb160-46"><a href="#cb160-46" aria-hidden="true" tabindex="-1"></a>    report_to<span class="op">=</span><span class="st">"none"</span>, <span class="co"># don't save experiments to a third party service</span></span>
<span id="cb160-47"><a href="#cb160-47" aria-hidden="true" tabindex="-1"></a>    push_to_hub<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb160-48"><a href="#cb160-48" aria-hidden="true" tabindex="-1"></a>    eval_do_concat_batches<span class="op">=</span><span class="va">False</span>, <span class="co"># this defaults to True but we'll set it to False for our evaluation function</span></span>
<span id="cb160-49"><a href="#cb160-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># save_safetensors=False # turn this off to prevent potential checkpoint issues</span></span>
<span id="cb160-50"><a href="#cb160-50" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Saving model to: models/rt_detrv2_finetuned_trashify_box_detector_v1</code></pre>
</div>
</div>
</section>
<section id="optional-setting-up-an-optimizer-for-multiple-learning-rates" class="level3" data-number="14.2">
<h3 data-number="14.2" class="anchored" data-anchor-id="optional-setting-up-an-optimizer-for-multiple-learning-rates"><span class="header-section-number">14.2</span> Optional: Setting up an optimizer for multiple learning rates</h3>
<p>In the papers that mentioned the DETR model we’re using (see Table 1 in the <a href="https://arxiv.org/abs/2407.17140">RT-DETRv2 paper</a>), they state that they used a different learning rate value for the backbone (<code>learning_rate=1e-5</code>) as well as the object detection head (<code>learning_rate=1e-4</code>).</p>
<figure style="text-align: center;" class="figure">
<img src="https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-trashify-object-detection/06-table-1-rt-detrv2-paper.png" alt="A table, titled 'Table 1: The hyperparameters of RT-DETRv2,' lists four model variants: 'RT-DETRv2-S,' 'RT-DETRv2-M,' 'RT-DETRv2-L,' and 'RT-DETRv2-X,' along with their respective 'Backbone' architectures ('ResNet18,' 'ResNet34,' 'ResNet50,' and 'ResNet101'), and their learning rates for the backbone ('lr_backbone': '1e-4,' '5e-5,' '1e-5,' '1e-6') and the detector head ('lr_det': all '1e-4')" style="width: 100%; max-width: 600px; height: auto;" class="figure-img">
<figcaption>
Different learning rates used for different sections of the model from the <a href="https://arxiv.org/pdf/2407.17140">RT-DETRv2 paper</a>. The backbone uses a slightly lower learning rate than the detection head.
</figcaption>
</figure>
<p>To set this up ourselves, we can extract which parameters of our model belong to the <code>backbone</code> as well as which don’t.</p>
<p>To find the backbone parameters, we can loop through our model’s <code>named_parameters()</code> method and filter for any which contain the string <code>"backbone"</code> in their name.</p>
<p>We’ll append these to a list called <code>backbone_parameters</code> and assume any that don’t have <code>"backbone"</code> in their name are not part of the model’s backbone.</p>
<p>We can use these two lists of parameters to pass to <a href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html"><code>torch.optim.AdamW</code></a> with different learning rate values for each.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In my experiments with our smaller dataset size (~1100 images), I found that setting two different learning rates for the backbone and the object detection head led to poorer performance than just setting a single learning rate for the whole model.</p>
<p>The code below is an example of how to create a custom optimizer with different learning rates for different parts of the model.</p>
<p>However, in our actual training code, we’ll use a single learning rate for the whole model.</p>
</div>
</div>
<p>We can then subclass <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer"><code>transformers.Trainer</code></a> and update the method <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.create_optimizer"><code>create_optimizer()</code></a> to use our custom optimizer.</p>
<div id="cell-205" class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb162"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb162-1"><a href="#cb162-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> Trainer</span>
<span id="cb162-2"><a href="#cb162-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb162-3"><a href="#cb162-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create lists for different kinds of parameters</span></span>
<span id="cb162-4"><a href="#cb162-4" aria-hidden="true" tabindex="-1"></a>backbone_parameters <span class="op">=</span> []</span>
<span id="cb162-5"><a href="#cb162-5" aria-hidden="true" tabindex="-1"></a>other_parameters <span class="op">=</span> []</span>
<span id="cb162-6"><a href="#cb162-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb162-7"><a href="#cb162-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Can loop through model parameters and extract different model sections</span></span>
<span id="cb162-8"><a href="#cb162-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.model.named_parameters(): </span>
<span id="cb162-9"><a href="#cb162-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"backbone"</span> <span class="kw">in</span> name:</span>
<span id="cb162-10"><a href="#cb162-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(f"Backbone parameter: {name}")</span></span>
<span id="cb162-11"><a href="#cb162-11" aria-hidden="true" tabindex="-1"></a>        backbone_parameters.append(param)</span>
<span id="cb162-12"><a href="#cb162-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb162-13"><a href="#cb162-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(f"Other parameter: {name}")</span></span>
<span id="cb162-14"><a href="#cb162-14" aria-hidden="true" tabindex="-1"></a>        other_parameters.append(param)</span>
<span id="cb162-15"><a href="#cb162-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb162-16"><a href="#cb162-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Number of backbone parameter modules: </span><span class="sc">{</span><span class="bu">len</span>(backbone_parameters)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb162-17"><a href="#cb162-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Number of other parameter modules: </span><span class="sc">{</span><span class="bu">len</span>(other_parameters)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb162-18"><a href="#cb162-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb162-19"><a href="#cb162-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup a custom subclass of Trainer to use different learning rates for different parts of the model</span></span>
<span id="cb162-20"><a href="#cb162-20" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CustomTrainer(Trainer):</span>
<span id="cb162-21"><a href="#cb162-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> create_optimizer(<span class="va">self</span>):</span>
<span id="cb162-22"><a href="#cb162-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.optimizer <span class="op">=</span> torch.optim.AdamW([</span>
<span id="cb162-23"><a href="#cb162-23" aria-hidden="true" tabindex="-1"></a>            {<span class="st">"params"</span>: backbone_parameters, <span class="st">"lr"</span>: <span class="fl">1e-4</span>},</span>
<span id="cb162-24"><a href="#cb162-24" aria-hidden="true" tabindex="-1"></a>            {<span class="st">"params"</span>: other_parameters, <span class="st">"lr"</span>: <span class="fl">1e-4</span>}</span>
<span id="cb162-25"><a href="#cb162-25" aria-hidden="true" tabindex="-1"></a>        ], weight_decay<span class="op">=</span><span class="fl">0.0001</span>)</span>
<span id="cb162-26"><a href="#cb162-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.optimizer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Number of backbone parameter modules: 55
[INFO] Number of other parameter modules: 363</code></pre>
</div>
</div>
<p>Awesome!</p>
<p>Now if we wanted to use our custom optimizer, we could use <code>CustomTrainer</code> instead of <code>Trainer</code>.</p>
</section>
<section id="creating-an-evaluation-function" class="level3" data-number="14.3">
<h3 data-number="14.3" class="anchored" data-anchor-id="creating-an-evaluation-function"><span class="header-section-number">14.3</span> Creating an evaluation function</h3>
<p>Evaluating a model’s performance is just as important as training a model.</p>
<p>After all, if you don’t know how well your model is performing, how can you be confident in deploying it or using it in the real world?</p>
<p>In this section, let’s create an evaluation function we can pass to <code>transformers.Trainer</code>’s <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.compute_metrics"><code>compute_metrics</code> parameter</a>.</p>
<p>The main goal of an evaluation function is to compare the model’s predictions to the ground truth labels.</p>
<p>For example, how does a model’s box predictions look like compared to the ground truth box predictions?</p>
<p>Once we’ve got a trained model, we can inspect these visually by plotting them on images.</p>
<p>However, during model training, we’ll get our <code>Trainer</code> instance to output evaluation metrics so we can get a snapshot of performance along the way.</p>
<p>Some things to note about the evaluation function we’ll create:</p>
<ul>
<li>Reading the documentation for the <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.compute_metrics"><code>compute_metrics</code> parameter</a>, we can see our evaluation function will be required to take a <a href="https://huggingface.co/docs/transformers/en/internal/trainer_utils#transformers.EvalPrediction"><code>transformers.EvalPrediction</code></a> as input.
<ul>
<li>This contains our model’s predictions and labels as <code>predictions</code> and <code>label_ids</code> attributes respectively.</li>
</ul></li>
<li>We must also return a dictionary with string to metric values for it to be displayed during training. For example, <code>{"metric_value": 42, ...}</code>.</li>
<li>To evaluate our object detection model we’re going to use the mAP metric (Mean Average Precision, a standard metric used amongst object detection models, see the <a href="https://cocodataset.org/#detection-eval">COCO evaluation section</a> for more details). To do so, we’ll use <a href="https://lightning.ai/docs/torchmetrics/stable/"><code>torchmetrics</code> package</a>, specifically <a href="https://lightning.ai/docs/torchmetrics/stable/detection/mean_average_precision.html"><code>torchmetrics.detection.mean_ap.MeanAveragePrecision</code></a>.
<ul>
<li>This method expects boxes in format XYXY absolute format by default.</li>
</ul></li>
<li>Our evaluation function will be an adaptation of the code example in the object detection example <a href="https://github.com/huggingface/transformers/blob/336dc69d63d56f232a183a3e7f52790429b871ef/examples/pytorch/object-detection/run_object_detection.py#L160">on the Hugging Face GitHub</a>.</li>
<li>For an in-depth overview on object detection metrics, see the <a href="https://blog.roboflow.com/object-detection-metrics/">Roboflow Guide to Object Detection Metrics</a>.</li>
</ul>
<p>Phew! A fair bit to take in.</p>
<p>But nothing we can’t handle.</p>
<p>Let’s create our function.</p>
<p>We’ll start by making a small helper function to convert bounding boxes from CXCYWH normalized format to XYXY absolute format.</p>
<div id="cell-208" class="cell" data-execution_count="84">
<div class="sourceCode cell-code" id="cb164"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb164-1"><a href="#cb164-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> convert_bbox_cxcywh_to_xyxy_absolute(boxes, </span>
<span id="cb164-2"><a href="#cb164-2" aria-hidden="true" tabindex="-1"></a>                                         image_size_target):</span>
<span id="cb164-3"><a href="#cb164-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb164-4"><a href="#cb164-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Converts CXCYWH normalized boxes to XYXY absolute boxes.</span></span>
<span id="cb164-5"><a href="#cb164-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb164-6"><a href="#cb164-6" aria-hidden="true" tabindex="-1"></a><span class="co">    The output of our preprocess method puts boxes in CXCYWH format.</span></span>
<span id="cb164-7"><a href="#cb164-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb164-8"><a href="#cb164-8" aria-hidden="true" tabindex="-1"></a><span class="co">    But our evaluation metric torchmetrics.detection.mean_ap.MeanAveragePrecision expects</span></span>
<span id="cb164-9"><a href="#cb164-9" aria-hidden="true" tabindex="-1"></a><span class="co">        boxes in XYXY absolute format.</span></span>
<span id="cb164-10"><a href="#cb164-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb164-11"><a href="#cb164-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb164-12"><a href="#cb164-12" aria-hidden="true" tabindex="-1"></a><span class="co">        boxes (torch.Tensor): A tensor of shape (N, 4) where N is the number of boxes and each box is in CXCYWH format.</span></span>
<span id="cb164-13"><a href="#cb164-13" aria-hidden="true" tabindex="-1"></a><span class="co">        image_size_target (tuple): A tuple containing the target image size as (height, width).</span></span>
<span id="cb164-14"><a href="#cb164-14" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb164-15"><a href="#cb164-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb164-16"><a href="#cb164-16" aria-hidden="true" tabindex="-1"></a><span class="co">        torch.Tensor: A tensor of shape (N, 4) where each box is converted to XYXY absolute format.</span></span>
<span id="cb164-17"><a href="#cb164-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb164-18"><a href="#cb164-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert normalized CXCYWH (output of model) -&gt; absolute XYXY format (required for evaluation)</span></span>
<span id="cb164-19"><a href="#cb164-19" aria-hidden="true" tabindex="-1"></a>    boxes <span class="op">=</span> box_convert(boxes<span class="op">=</span>boxes, in_fmt<span class="op">=</span><span class="st">"cxcywh"</span>, out_fmt<span class="op">=</span><span class="st">"xyxy"</span>)</span>
<span id="cb164-20"><a href="#cb164-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb164-21"><a href="#cb164-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert normalized box coordinates to absolute pixel values based on the target size </span></span>
<span id="cb164-22"><a href="#cb164-22" aria-hidden="true" tabindex="-1"></a>    image_size_target_height <span class="op">=</span> image_size_target[<span class="dv">0</span>]</span>
<span id="cb164-23"><a href="#cb164-23" aria-hidden="true" tabindex="-1"></a>    image_size_target_width <span class="op">=</span> image_size_target[<span class="dv">1</span>]</span>
<span id="cb164-24"><a href="#cb164-24" aria-hidden="true" tabindex="-1"></a>    boxes <span class="op">=</span> boxes <span class="op">*</span> torch.tensor([image_size_target_width, </span>
<span id="cb164-25"><a href="#cb164-25" aria-hidden="true" tabindex="-1"></a>                                  image_size_target_height, </span>
<span id="cb164-26"><a href="#cb164-26" aria-hidden="true" tabindex="-1"></a>                                  image_size_target_width, </span>
<span id="cb164-27"><a href="#cb164-27" aria-hidden="true" tabindex="-1"></a>                                  image_size_target_height]) <span class="co"># Multiply X coordinates by the width and Y coordinates by the height</span></span>
<span id="cb164-28"><a href="#cb164-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb164-29"><a href="#cb164-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> boxes</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Perfect!</p>
<p>Time to craft our <code>compute_metrics</code> function.</p>
<p>The main goal of the function will be to take a <code>transformers.EvalPrediction</code> output from our model and return a dictionary mapping metric names to values, for example, <code>{"metric_name": 42.0 ...}</code>.</p>
<p>To do so, we’ll go through the following steps:</p>
<ol type="1">
<li>Create a Python <code>dataclass</code> to hold our model’s outputs. We could use a dictionary but this will give our code a bit more structure.</li>
<li>Create a <code>compute_metrics</code> function which takes in an <a href="https://huggingface.co/docs/transformers/en/internal/trainer_utils#transformers.EvalPrediction"><code>EvalPrediction</code></a> object as well as other required evaluation parameters such as <code>image_processor</code> (for post processing boxes), <code>id2label</code> (for mapping metrics to class names) and <code>threshold</code> (for assigning a prediction probability threshold to boxes).</li>
<li>Extract predictions and targets from <code>EvalPrediction</code> via <code>EvalPrediction.predictions</code> and <code>EvalPrediction.label_ids</code> respectively.</li>
<li>Create empty lists of <code>image_sizes</code> (for post processing boxes), <code>post_processed_predictions</code> and <code>post_processed_targets</code> (we’ll compare the latter two to each other).</li>
<li>Collect target samples in format required for <a href="https://lightning.ai/docs/torchmetrics/stable/detection/mean_average_precision.html"><code>torchmetrics.detection.mean_ap.MeanAveragePrecision</code></a>, for example, <code>[{"boxes": [...], "labels": [...]}]</code>.</li>
<li>Collect predictions in the required formart for <code>MeanAveragePrecision</code>, our model produces boxes in CXCYWH format, then we use <code>image_processor.post_process_object_detection</code> to convert the predictions to XYXY format, and append them to <code>post_processed_predictions</code> in form <code>[{"boxes": [...], "labels": [...], "scores": [...]}]</code>.</li>
<li>Initialize an instance of <code>torchmetrics.detection.mean_ap.MeanAveragePrecision</code> (see documentation for output of <code>MeanAveragePrecision</code>) and pass it predictions and labels to compute on.</li>
<li>Extract lists of target metrics from the output of <code>MeanAveragePrecision</code>, for example, with <code>metrics.pop("target_item")</code>.</li>
<li>Prepare metrics for output in the form of a dict with metric names -&gt; values, for example, <code>{"metric_name": 42.0, ...}</code>.</li>
<li>Round metric values in output dictionary for visual display during training.</li>
<li>Create a partial function we can pass to <code>transformers.Trainer</code>’s <code>compute_metrics</code> parameter to run as a callable with appropriate parameter inputs to our <code>compute_metrics</code> function.</li>
</ol>
<p>Easy.</p>
<p>We’ve got this.</p>
<div id="cell-210" class="cell" data-execution_count="85">
<div class="sourceCode cell-code" id="cb165"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb165-1"><a href="#cb165-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an evaluation function to test our model's performance</span></span>
<span id="cb165-2"><a href="#cb165-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb165-3"><a href="#cb165-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-4"><a href="#cb165-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Optional, Mapping</span>
<span id="cb165-5"><a href="#cb165-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-6"><a href="#cb165-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> EvalPrediction</span>
<span id="cb165-7"><a href="#cb165-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-8"><a href="#cb165-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.ops <span class="im">import</span> box_convert</span>
<span id="cb165-9"><a href="#cb165-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-10"><a href="#cb165-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchmetrics.detection.mean_ap <span class="im">import</span> MeanAveragePrecision</span>
<span id="cb165-11"><a href="#cb165-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-12"><a href="#cb165-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Create a dataclass to hold our model's outputs</span></span>
<span id="cb165-13"><a href="#cb165-13" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb165-14"><a href="#cb165-14" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ModelOutput:</span>
<span id="cb165-15"><a href="#cb165-15" aria-hidden="true" tabindex="-1"></a>    logits: torch.Tensor</span>
<span id="cb165-16"><a href="#cb165-16" aria-hidden="true" tabindex="-1"></a>    pred_boxes: torch.Tensor</span>
<span id="cb165-17"><a href="#cb165-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-18"><a href="#cb165-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Create a compute_metrics function which takes in EvalPrediction and other required parameters</span></span>
<span id="cb165-19"><a href="#cb165-19" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb165-20"><a href="#cb165-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_metrics(</span>
<span id="cb165-21"><a href="#cb165-21" aria-hidden="true" tabindex="-1"></a>    evaluation_results: EvalPrediction, <span class="co"># these come out of the Trainer.evaluate method, see: https://huggingface.co/docs/transformers/en/internal/trainer_utils#transformers.EvalPrediction </span></span>
<span id="cb165-22"><a href="#cb165-22" aria-hidden="true" tabindex="-1"></a>    image_processor: AutoImageProcessor,</span>
<span id="cb165-23"><a href="#cb165-23" aria-hidden="true" tabindex="-1"></a>    threshold: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.0</span>,</span>
<span id="cb165-24"><a href="#cb165-24" aria-hidden="true" tabindex="-1"></a>    id2label: Optional[Mapping[<span class="bu">int</span>, <span class="bu">str</span>]] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb165-25"><a href="#cb165-25" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Mapping[<span class="bu">str</span>, <span class="bu">float</span>]:</span>
<span id="cb165-26"><a href="#cb165-26" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb165-27"><a href="#cb165-27" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute mean average mAP, mAR and their variants for the object detection task.</span></span>
<span id="cb165-28"><a href="#cb165-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-29"><a href="#cb165-29" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb165-30"><a href="#cb165-30" aria-hidden="true" tabindex="-1"></a><span class="co">        evaluation_results (EvalPrediction): Predictions and targets from evaluation.</span></span>
<span id="cb165-31"><a href="#cb165-31" aria-hidden="true" tabindex="-1"></a><span class="co">        threshold (float, optional): Threshold to filter predicted boxes by confidence. Defaults to 0.0.</span></span>
<span id="cb165-32"><a href="#cb165-32" aria-hidden="true" tabindex="-1"></a><span class="co">        id2label (Optional[dict], optional): Mapping from class id to class name. Defaults to None.</span></span>
<span id="cb165-33"><a href="#cb165-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-34"><a href="#cb165-34" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb165-35"><a href="#cb165-35" aria-hidden="true" tabindex="-1"></a><span class="co">        Mapping[str, float]: Metrics in a form of dictionary {&lt;metric_name&gt;: &lt;metric_value&gt;}</span></span>
<span id="cb165-36"><a href="#cb165-36" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb165-37"><a href="#cb165-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-38"><a href="#cb165-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Extract predictions and targets from EvalPrediction</span></span>
<span id="cb165-39"><a href="#cb165-39" aria-hidden="true" tabindex="-1"></a>    predictions, targets <span class="op">=</span> evaluation_results.predictions, evaluation_results.label_ids</span>
<span id="cb165-40"><a href="#cb165-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-41"><a href="#cb165-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For metric computation we need to provide to MeanAveragePrecision</span></span>
<span id="cb165-42"><a href="#cb165-42" aria-hidden="true" tabindex="-1"></a>    <span class="co">#  - 'targets' in a form of list of dictionaries with keys "boxes", "labels"</span></span>
<span id="cb165-43"><a href="#cb165-43" aria-hidden="true" tabindex="-1"></a>    <span class="co">#  - 'predictions' in a form of list of dictionaries with keys "boxes", "scores", "labels"</span></span>
<span id="cb165-44"><a href="#cb165-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-45"><a href="#cb165-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4. Get a list of image sizes, processed targets and processed predictions</span></span>
<span id="cb165-46"><a href="#cb165-46" aria-hidden="true" tabindex="-1"></a>    image_sizes <span class="op">=</span> []</span>
<span id="cb165-47"><a href="#cb165-47" aria-hidden="true" tabindex="-1"></a>    post_processed_targets <span class="op">=</span> []</span>
<span id="cb165-48"><a href="#cb165-48" aria-hidden="true" tabindex="-1"></a>    post_processed_predictions <span class="op">=</span> []</span>
<span id="cb165-49"><a href="#cb165-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-50"><a href="#cb165-50" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Target collection </span><span class="al">###</span></span>
<span id="cb165-51"><a href="#cb165-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-52"><a href="#cb165-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 5. Collect target attributes in the required format for metric computation</span></span>
<span id="cb165-53"><a href="#cb165-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> targets:</span>
<span id="cb165-54"><a href="#cb165-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Collect ground truth image sizes, we will need them for predictions post processing</span></span>
<span id="cb165-55"><a href="#cb165-55" aria-hidden="true" tabindex="-1"></a>        batch_image_sizes <span class="op">=</span> torch.tensor(np.array([x[<span class="st">"orig_size"</span>] <span class="cf">for</span> x <span class="kw">in</span> batch])) <span class="co"># turn into a list of numpy arrays first, then tensors</span></span>
<span id="cb165-56"><a href="#cb165-56" aria-hidden="true" tabindex="-1"></a>        image_sizes.append(batch_image_sizes)</span>
<span id="cb165-57"><a href="#cb165-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-58"><a href="#cb165-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Collect targets in the required format for metric computation</span></span>
<span id="cb165-59"><a href="#cb165-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># boxes were converted to YOLO format needed for model training</span></span>
<span id="cb165-60"><a href="#cb165-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># here we will convert them to Pascal VOC format (x_min, y_min, x_max, y_max) </span></span>
<span id="cb165-61"><a href="#cb165-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># or XYXY format. We do this because the boxes out of preprocess() are in </span></span>
<span id="cb165-62"><a href="#cb165-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># CXCYWH normalized format.</span></span>
<span id="cb165-63"><a href="#cb165-63" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> image_target <span class="kw">in</span> batch:</span>
<span id="cb165-64"><a href="#cb165-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-65"><a href="#cb165-65" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get boxes and convert from CXCYWH to XYXY</span></span>
<span id="cb165-66"><a href="#cb165-66" aria-hidden="true" tabindex="-1"></a>            boxes <span class="op">=</span> torch.tensor(image_target[<span class="st">"boxes"</span>])</span>
<span id="cb165-67"><a href="#cb165-67" aria-hidden="true" tabindex="-1"></a>            boxes <span class="op">=</span> convert_bbox_cxcywh_to_xyxy_absolute(boxes<span class="op">=</span>boxes, </span>
<span id="cb165-68"><a href="#cb165-68" aria-hidden="true" tabindex="-1"></a>                                                         image_size_target<span class="op">=</span>image_target[<span class="st">"orig_size"</span>])</span>
<span id="cb165-69"><a href="#cb165-69" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb165-70"><a href="#cb165-70" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get labels</span></span>
<span id="cb165-71"><a href="#cb165-71" aria-hidden="true" tabindex="-1"></a>            labels <span class="op">=</span> torch.tensor(image_target[<span class="st">"class_labels"</span>])</span>
<span id="cb165-72"><a href="#cb165-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-73"><a href="#cb165-73" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Append box and label pairs in format requried for MeanAveragePrecision class</span></span>
<span id="cb165-74"><a href="#cb165-74" aria-hidden="true" tabindex="-1"></a>            post_processed_targets.append({<span class="st">"boxes"</span>: boxes, </span>
<span id="cb165-75"><a href="#cb165-75" aria-hidden="true" tabindex="-1"></a>                                           <span class="st">"labels"</span>: labels})</span>
<span id="cb165-76"><a href="#cb165-76" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb165-77"><a href="#cb165-77" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Prediction collection </span><span class="al">###</span></span>
<span id="cb165-78"><a href="#cb165-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-79"><a href="#cb165-79" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 6. Collect predictions in the required format for metric computation,</span></span>
<span id="cb165-80"><a href="#cb165-80" aria-hidden="true" tabindex="-1"></a>    <span class="co"># model produce boxes in YOLO format (CXCYWH), then image_processor.post_process_object_detection to </span></span>
<span id="cb165-81"><a href="#cb165-81" aria-hidden="true" tabindex="-1"></a>    <span class="co"># convert them to Pascal VOC format (XYXY).</span></span>
<span id="cb165-82"><a href="#cb165-82" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch, target_sizes <span class="kw">in</span> <span class="bu">zip</span>(predictions, image_sizes):</span>
<span id="cb165-83"><a href="#cb165-83" aria-hidden="true" tabindex="-1"></a>        batch_logits, batch_boxes <span class="op">=</span> batch[<span class="dv">1</span>], batch[<span class="dv">2</span>]</span>
<span id="cb165-84"><a href="#cb165-84" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> ModelOutput(logits<span class="op">=</span>torch.tensor(batch_logits), </span>
<span id="cb165-85"><a href="#cb165-85" aria-hidden="true" tabindex="-1"></a>                             pred_boxes<span class="op">=</span>torch.tensor(batch_boxes))</span>
<span id="cb165-86"><a href="#cb165-86" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb165-87"><a href="#cb165-87" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Post process the model outputs</span></span>
<span id="cb165-88"><a href="#cb165-88" aria-hidden="true" tabindex="-1"></a>        post_processed_output <span class="op">=</span> image_processor.post_process_object_detection(</span>
<span id="cb165-89"><a href="#cb165-89" aria-hidden="true" tabindex="-1"></a>                                                    outputs<span class="op">=</span>output, </span>
<span id="cb165-90"><a href="#cb165-90" aria-hidden="true" tabindex="-1"></a>                                                    threshold<span class="op">=</span>threshold, </span>
<span id="cb165-91"><a href="#cb165-91" aria-hidden="true" tabindex="-1"></a>                                                    target_sizes<span class="op">=</span>target_sizes) <span class="co"># target sizes required to shape boxes in correct ratio of original image</span></span>
<span id="cb165-92"><a href="#cb165-92" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb165-93"><a href="#cb165-93" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Append post_processed_output in form `[{"boxes": [...], "labels": [...], "scores": [...]}]`</span></span>
<span id="cb165-94"><a href="#cb165-94" aria-hidden="true" tabindex="-1"></a>        post_processed_predictions.extend(post_processed_output)</span>
<span id="cb165-95"><a href="#cb165-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-96"><a href="#cb165-96" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 7. Compute mAP</span></span>
<span id="cb165-97"><a href="#cb165-97" aria-hidden="true" tabindex="-1"></a>    max_detection_thresholds <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>] <span class="co"># 1 = mar@1, mar@10, mar@100 (100 = default max total boxes for post processed predictions out of object detection model)</span></span>
<span id="cb165-98"><a href="#cb165-98" aria-hidden="true" tabindex="-1"></a>    metric <span class="op">=</span> MeanAveragePrecision(box_format<span class="op">=</span><span class="st">"xyxy"</span>, </span>
<span id="cb165-99"><a href="#cb165-99" aria-hidden="true" tabindex="-1"></a>                                  class_metrics<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb165-100"><a href="#cb165-100" aria-hidden="true" tabindex="-1"></a>                                  max_detection_thresholds<span class="op">=</span>max_detection_thresholds) </span>
<span id="cb165-101"><a href="#cb165-101" aria-hidden="true" tabindex="-1"></a>    metric.warn_on_many_detections <span class="op">=</span> <span class="va">False</span> <span class="co"># don't output a warning when large amount of detections come out (the sorting handles this anyway)</span></span>
<span id="cb165-102"><a href="#cb165-102" aria-hidden="true" tabindex="-1"></a>    metric.update(post_processed_predictions, </span>
<span id="cb165-103"><a href="#cb165-103" aria-hidden="true" tabindex="-1"></a>                  post_processed_targets)</span>
<span id="cb165-104"><a href="#cb165-104" aria-hidden="true" tabindex="-1"></a>    metrics <span class="op">=</span> metric.compute()</span>
<span id="cb165-105"><a href="#cb165-105" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb165-106"><a href="#cb165-106" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Optional: print metrics dict for troubleshooting</span></span>
<span id="cb165-107"><a href="#cb165-107" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(metrics)</span></span>
<span id="cb165-108"><a href="#cb165-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-109"><a href="#cb165-109" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 8. Extract list of per class metrics with separate metric for each class</span></span>
<span id="cb165-110"><a href="#cb165-110" aria-hidden="true" tabindex="-1"></a>    classes <span class="op">=</span> metrics.pop(<span class="st">"classes"</span>)</span>
<span id="cb165-111"><a href="#cb165-111" aria-hidden="true" tabindex="-1"></a>    map_per_class <span class="op">=</span> metrics.pop(<span class="st">"map_per_class"</span>)</span>
<span id="cb165-112"><a href="#cb165-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-113"><a href="#cb165-113" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Optional: mAR@N per class (mAR = Mean Average Recall)</span></span>
<span id="cb165-114"><a href="#cb165-114" aria-hidden="true" tabindex="-1"></a>    mar_per_class <span class="op">=</span> metrics.pop(<span class="st">"mar_100_per_class"</span>)</span>
<span id="cb165-115"><a href="#cb165-115" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb165-116"><a href="#cb165-116" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 9. Prepare metrics per class in the form of a dict with metric names -&gt; values, e.g. {"metric_name": 42.0, ...}</span></span>
<span id="cb165-117"><a href="#cb165-117" aria-hidden="true" tabindex="-1"></a>    <span class="co"># for class_id, class_map in zip(classes, map_per_class):</span></span>
<span id="cb165-118"><a href="#cb165-118" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> class_id, class_map, class_mar <span class="kw">in</span> <span class="bu">zip</span>(classes, map_per_class, mar_per_class):</span>
<span id="cb165-119"><a href="#cb165-119" aria-hidden="true" tabindex="-1"></a>        class_name <span class="op">=</span> id2label[class_id.item()] <span class="cf">if</span> id2label <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> class_id.item()</span>
<span id="cb165-120"><a href="#cb165-120" aria-hidden="true" tabindex="-1"></a>        metrics[<span class="ss">f"map_</span><span class="sc">{</span>class_name<span class="sc">}</span><span class="ss">"</span>] <span class="op">=</span> class_map</span>
<span id="cb165-121"><a href="#cb165-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-122"><a href="#cb165-122" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Optional: mAR@100 per class</span></span>
<span id="cb165-123"><a href="#cb165-123" aria-hidden="true" tabindex="-1"></a>        metrics[<span class="ss">f"mar_100_</span><span class="sc">{</span>class_name<span class="sc">}</span><span class="ss">"</span>] <span class="op">=</span> class_mar</span>
<span id="cb165-124"><a href="#cb165-124" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb165-125"><a href="#cb165-125" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 10. Round metrics for suitable visual output</span></span>
<span id="cb165-126"><a href="#cb165-126" aria-hidden="true" tabindex="-1"></a>    metrics <span class="op">=</span> {k: <span class="bu">round</span>(v.item(), <span class="dv">4</span>) <span class="cf">for</span> k, v <span class="kw">in</span> metrics.items()}</span>
<span id="cb165-127"><a href="#cb165-127" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb165-128"><a href="#cb165-128" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Optional: print metrics dict for troubleshooting</span></span>
<span id="cb165-129"><a href="#cb165-129" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(metrics)</span></span>
<span id="cb165-130"><a href="#cb165-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-131"><a href="#cb165-131" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> metrics</span>
<span id="cb165-132"><a href="#cb165-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-133"><a href="#cb165-133" aria-hidden="true" tabindex="-1"></a><span class="co"># 11. Create a partial function for our compute_metrics function (we'll pass this to compute_metrics in Trainer)</span></span>
<span id="cb165-134"><a href="#cb165-134" aria-hidden="true" tabindex="-1"></a>eval_compute_metrics_fn <span class="op">=</span> partial(</span>
<span id="cb165-135"><a href="#cb165-135" aria-hidden="true" tabindex="-1"></a>        compute_metrics, </span>
<span id="cb165-136"><a href="#cb165-136" aria-hidden="true" tabindex="-1"></a>        image_processor<span class="op">=</span>image_processor, </span>
<span id="cb165-137"><a href="#cb165-137" aria-hidden="true" tabindex="-1"></a>        threshold<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb165-138"><a href="#cb165-138" aria-hidden="true" tabindex="-1"></a>        id2label<span class="op">=</span>id2label, </span>
<span id="cb165-139"><a href="#cb165-139" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="training-our-model-with-trainer" class="level3" data-number="14.4">
<h3 data-number="14.4" class="anchored" data-anchor-id="training-our-model-with-trainer"><span class="header-section-number">14.4</span> Training our model with Trainer</h3>
<p>We’ve now got all the ingredients needed to train our model!</p>
<p>The good news is since we’ve put so much effort into preparing our dataset, creating an evaluation function and setting up our training arguments, we can train our model in a few lines of code.</p>
<p>To train our model, we’ll set up an instance of <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer"><code>transformers.Trainer</code></a> and then we’ll pass it the following arguments:</p>
<ul>
<li><code>model</code> - The <code>model</code> we’d like to train. In our case it will be the fresh insteand of <code>model</code> we created using our <code>create_model()</code> function.</li>
<li><code>args</code> - An instance of <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments"><code>transformers.TrainingArguments</code></a> (or <code>training_args</code> in our case) containing various hyperparameter settings to use for our model.</li>
<li><code>data_collator</code> - The function to use which will turn a list of samples from <code>train_dataset</code> into a batch of samples.</li>
<li><code>train_dataset</code> - The dataset we’d like our model to train on, in our case this will be <code>processed_dataset["train"]</code>, the dataset we’ve already preprocessed.</li>
<li><code>eval_dataset</code> - The dataset we’d like our model to be evaluated on, in our case this will be <code>processed_dataset["validation"]</code>, our model will never see these samples during training, it will only test itself on these.</li>
<li><code>compute_metrics</code> - A <code>Callable</code> which takes in [<code>EvalPrediction</code>] and is able to return a string to metric (<code>{"metric_name": value}</code>) dictionary, these will displayed during training.</li>
</ul>
<p>After we’ve done all that, we can start to train our model with by calling <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.train"><code>transformers.Trainer.train()</code></a>.</p>
<div id="cell-212" class="cell" data-execution_count="86">
<div class="sourceCode cell-code" id="cb166"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb166-1"><a href="#cb166-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: Depending on the size/speed of your GPU, this may take a while</span></span>
<span id="cb166-2"><a href="#cb166-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> Trainer</span>
<span id="cb166-3"><a href="#cb166-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb166-4"><a href="#cb166-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Setup instance of Trainer</span></span>
<span id="cb166-5"><a href="#cb166-5" aria-hidden="true" tabindex="-1"></a>model_v1_trainer <span class="op">=</span> Trainer(</span>
<span id="cb166-6"><a href="#cb166-6" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb166-7"><a href="#cb166-7" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb166-8"><a href="#cb166-8" aria-hidden="true" tabindex="-1"></a>    data_collator<span class="op">=</span>data_collate_function,</span>
<span id="cb166-9"><a href="#cb166-9" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>processed_dataset[<span class="st">"train"</span>], <span class="co"># pass in the already preprocessed data</span></span>
<span id="cb166-10"><a href="#cb166-10" aria-hidden="true" tabindex="-1"></a>    eval_dataset<span class="op">=</span>processed_dataset[<span class="st">"validation"</span>],</span>
<span id="cb166-11"><a href="#cb166-11" aria-hidden="true" tabindex="-1"></a>    compute_metrics<span class="op">=</span>eval_compute_metrics_fn,</span>
<span id="cb166-12"><a href="#cb166-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb166-13"><a href="#cb166-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb166-14"><a href="#cb166-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 6. Train the model </span></span>
<span id="cb166-15"><a href="#cb166-15" aria-hidden="true" tabindex="-1"></a>model_v1_results <span class="op">=</span> model_v1_trainer.train(</span>
<span id="cb166-16"><a href="#cb166-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># resume_from_checkpoint=False # you can continue training a model here by passing in the path to a previous checkpoint</span></span>
<span id="cb166-17"><a href="#cb166-17" aria-hidden="true" tabindex="-1"></a>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

    <div>
      
      <progress value="500" max="500" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [500/500 03:29, Epoch 10/10]
    </div>
    
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Epoch</th>
<th data-quarto-table-cell-role="th">Training Loss</th>
<th data-quarto-table-cell-role="th">Validation Loss</th>
<th data-quarto-table-cell-role="th">Map</th>
<th data-quarto-table-cell-role="th">Map 50</th>
<th data-quarto-table-cell-role="th">Map 75</th>
<th data-quarto-table-cell-role="th">Map Small</th>
<th data-quarto-table-cell-role="th">Map Medium</th>
<th data-quarto-table-cell-role="th">Map Large</th>
<th data-quarto-table-cell-role="th">Mar 1</th>
<th data-quarto-table-cell-role="th">Mar 10</th>
<th data-quarto-table-cell-role="th">Mar 100</th>
<th data-quarto-table-cell-role="th">Mar Small</th>
<th data-quarto-table-cell-role="th">Mar Medium</th>
<th data-quarto-table-cell-role="th">Mar Large</th>
<th data-quarto-table-cell-role="th">Map Bin</th>
<th data-quarto-table-cell-role="th">Mar 100 Bin</th>
<th data-quarto-table-cell-role="th">Map Hand</th>
<th data-quarto-table-cell-role="th">Mar 100 Hand</th>
<th data-quarto-table-cell-role="th">Map Not Bin</th>
<th data-quarto-table-cell-role="th">Mar 100 Not Bin</th>
<th data-quarto-table-cell-role="th">Map Not Hand</th>
<th data-quarto-table-cell-role="th">Mar 100 Not Hand</th>
<th data-quarto-table-cell-role="th">Map Not Trash</th>
<th data-quarto-table-cell-role="th">Mar 100 Not Trash</th>
<th data-quarto-table-cell-role="th">Map Trash</th>
<th data-quarto-table-cell-role="th">Mar 100 Trash</th>
<th data-quarto-table-cell-role="th">Map Trash Arm</th>
<th data-quarto-table-cell-role="th">Mar 100 Trash Arm</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>187.744900</td>
<td>76.783653</td>
<td>0.075800</td>
<td>0.142000</td>
<td>0.070300</td>
<td>0.000000</td>
<td>0.014500</td>
<td>0.078600</td>
<td>0.133100</td>
<td>0.287500</td>
<td>0.328500</td>
<td>0.000000</td>
<td>0.185800</td>
<td>0.365700</td>
<td>0.169400</td>
<td>0.644000</td>
<td>0.198200</td>
<td>0.350000</td>
<td>0.010300</td>
<td>0.428600</td>
<td>-1.000000</td>
<td>-1.000000</td>
<td>0.002000</td>
<td>0.170800</td>
<td>0.075000</td>
<td>0.377900</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr class="even">
<td>2</td>
<td>62.347700</td>
<td>24.752775</td>
<td>0.199900</td>
<td>0.306500</td>
<td>0.186000</td>
<td>0.000000</td>
<td>0.056000</td>
<td>0.210900</td>
<td>0.326700</td>
<td>0.533600</td>
<td>0.550300</td>
<td>0.000000</td>
<td>0.175600</td>
<td>0.576500</td>
<td>0.283500</td>
<td>0.752500</td>
<td>0.441200</td>
<td>0.692200</td>
<td>0.006400</td>
<td>0.335700</td>
<td>-1.000000</td>
<td>-1.000000</td>
<td>0.113600</td>
<td>0.304200</td>
<td>0.175600</td>
<td>0.684100</td>
<td>0.178900</td>
<td>0.533300</td>
</tr>
<tr class="odd">
<td>3</td>
<td>25.808900</td>
<td>14.790608</td>
<td>0.255100</td>
<td>0.387700</td>
<td>0.276000</td>
<td>0.000000</td>
<td>0.073300</td>
<td>0.266500</td>
<td>0.381400</td>
<td>0.590300</td>
<td>0.650800</td>
<td>0.000000</td>
<td>0.317600</td>
<td>0.694500</td>
<td>0.207500</td>
<td>0.866000</td>
<td>0.461000</td>
<td>0.689200</td>
<td>0.017200</td>
<td>0.507100</td>
<td>-1.000000</td>
<td>-1.000000</td>
<td>0.098200</td>
<td>0.411100</td>
<td>0.164100</td>
<td>0.664600</td>
<td>0.582700</td>
<td>0.766700</td>
</tr>
<tr class="even">
<td>4</td>
<td>18.084500</td>
<td>11.792915</td>
<td>0.331600</td>
<td>0.491200</td>
<td>0.374800</td>
<td>0.050000</td>
<td>0.091300</td>
<td>0.354400</td>
<td>0.411600</td>
<td>0.608600</td>
<td>0.688000</td>
<td>0.100000</td>
<td>0.509100</td>
<td>0.720100</td>
<td>0.497400</td>
<td>0.867400</td>
<td>0.536700</td>
<td>0.785300</td>
<td>0.099700</td>
<td>0.628600</td>
<td>-1.000000</td>
<td>-1.000000</td>
<td>0.181500</td>
<td>0.465300</td>
<td>0.281600</td>
<td>0.681400</td>
<td>0.392900</td>
<td>0.700000</td>
</tr>
<tr class="odd">
<td>5</td>
<td>15.275100</td>
<td>11.574305</td>
<td>0.369100</td>
<td>0.576400</td>
<td>0.393500</td>
<td>0.150000</td>
<td>0.058200</td>
<td>0.390900</td>
<td>0.432400</td>
<td>0.594200</td>
<td>0.672200</td>
<td>0.150000</td>
<td>0.343200</td>
<td>0.716800</td>
<td>0.563300</td>
<td>0.851800</td>
<td>0.512100</td>
<td>0.795100</td>
<td>0.089900</td>
<td>0.585700</td>
<td>-1.000000</td>
<td>-1.000000</td>
<td>0.165100</td>
<td>0.515300</td>
<td>0.317600</td>
<td>0.652200</td>
<td>0.566500</td>
<td>0.633300</td>
</tr>
<tr class="even">
<td>6</td>
<td>13.623300</td>
<td>10.943520</td>
<td>0.424000</td>
<td>0.611300</td>
<td>0.483300</td>
<td>0.088900</td>
<td>0.062400</td>
<td>0.446400</td>
<td>0.506400</td>
<td>0.658900</td>
<td>0.731000</td>
<td>0.300000</td>
<td>0.356800</td>
<td>0.768600</td>
<td>0.625800</td>
<td>0.861700</td>
<td>0.532900</td>
<td>0.767600</td>
<td>0.106700</td>
<td>0.614300</td>
<td>-1.000000</td>
<td>-1.000000</td>
<td>0.177500</td>
<td>0.588900</td>
<td>0.376900</td>
<td>0.686700</td>
<td>0.724300</td>
<td>0.866700</td>
</tr>
<tr class="odd">
<td>7</td>
<td>12.436100</td>
<td>10.461347</td>
<td>0.442100</td>
<td>0.641400</td>
<td>0.509000</td>
<td>0.066700</td>
<td>0.075100</td>
<td>0.469800</td>
<td>0.495100</td>
<td>0.640100</td>
<td>0.720900</td>
<td>0.200000</td>
<td>0.386900</td>
<td>0.759600</td>
<td>0.641300</td>
<td>0.891500</td>
<td>0.485900</td>
<td>0.767600</td>
<td>0.155600</td>
<td>0.621400</td>
<td>-1.000000</td>
<td>-1.000000</td>
<td>0.221000</td>
<td>0.581900</td>
<td>0.414800</td>
<td>0.696500</td>
<td>0.733800</td>
<td>0.766700</td>
</tr>
<tr class="even">
<td>8</td>
<td>11.657900</td>
<td>10.532933</td>
<td>0.430400</td>
<td>0.610300</td>
<td>0.473200</td>
<td>0.035400</td>
<td>0.061900</td>
<td>0.456400</td>
<td>0.488900</td>
<td>0.642200</td>
<td>0.718300</td>
<td>0.350000</td>
<td>0.243800</td>
<td>0.763500</td>
<td>0.641100</td>
<td>0.858200</td>
<td>0.511300</td>
<td>0.772500</td>
<td>0.157700</td>
<td>0.485700</td>
<td>-1.000000</td>
<td>-1.000000</td>
<td>0.184800</td>
<td>0.588900</td>
<td>0.384100</td>
<td>0.704400</td>
<td>0.703100</td>
<td>0.900000</td>
</tr>
<tr class="odd">
<td>9</td>
<td>11.009100</td>
<td>10.670994</td>
<td>0.455300</td>
<td>0.632500</td>
<td>0.513800</td>
<td>0.066700</td>
<td>0.079400</td>
<td>0.482400</td>
<td>0.503800</td>
<td>0.647500</td>
<td>0.727300</td>
<td>0.400000</td>
<td>0.326700</td>
<td>0.771400</td>
<td>0.664900</td>
<td>0.863800</td>
<td>0.480000</td>
<td>0.761800</td>
<td>0.148100</td>
<td>0.557100</td>
<td>-1.000000</td>
<td>-1.000000</td>
<td>0.213200</td>
<td>0.598600</td>
<td>0.401300</td>
<td>0.715900</td>
<td>0.824300</td>
<td>0.866700</td>
</tr>
<tr class="even">
<td>10</td>
<td>10.597600</td>
<td>10.565559</td>
<td>0.464000</td>
<td>0.641600</td>
<td>0.504400</td>
<td>0.053600</td>
<td>0.079700</td>
<td>0.490200</td>
<td>0.514300</td>
<td>0.666100</td>
<td>0.739800</td>
<td>0.350000</td>
<td>0.480100</td>
<td>0.774500</td>
<td>0.677700</td>
<td>0.859600</td>
<td>0.488800</td>
<td>0.771600</td>
<td>0.153600</td>
<td>0.628600</td>
<td>-1.000000</td>
<td>-1.000000</td>
<td>0.204200</td>
<td>0.605600</td>
<td>0.392100</td>
<td>0.707100</td>
<td>0.867300</td>
<td>0.866700</td>
</tr>
</tbody>
</table>
<p>
</p></div>
<div class="cell-output cell-output-stderr">
<pre><code>There were missing keys in the checkpoint model loaded: ['class_embed.0.weight', 'class_embed.0.bias', 'class_embed.1.weight', 'class_embed.1.bias', 'class_embed.2.weight', 'class_embed.2.bias', 'class_embed.3.weight', 'class_embed.3.bias', 'class_embed.4.weight', 'class_embed.4.bias', 'class_embed.5.weight', 'class_embed.5.bias', 'bbox_embed.0.layers.0.weight', 'bbox_embed.0.layers.0.bias', 'bbox_embed.0.layers.1.weight', 'bbox_embed.0.layers.1.bias', 'bbox_embed.0.layers.2.weight', 'bbox_embed.0.layers.2.bias', 'bbox_embed.1.layers.0.weight', 'bbox_embed.1.layers.0.bias', 'bbox_embed.1.layers.1.weight', 'bbox_embed.1.layers.1.bias', 'bbox_embed.1.layers.2.weight', 'bbox_embed.1.layers.2.bias', 'bbox_embed.2.layers.0.weight', 'bbox_embed.2.layers.0.bias', 'bbox_embed.2.layers.1.weight', 'bbox_embed.2.layers.1.bias', 'bbox_embed.2.layers.2.weight', 'bbox_embed.2.layers.2.bias', 'bbox_embed.3.layers.0.weight', 'bbox_embed.3.layers.0.bias', 'bbox_embed.3.layers.1.weight', 'bbox_embed.3.layers.1.bias', 'bbox_embed.3.layers.2.weight', 'bbox_embed.3.layers.2.bias', 'bbox_embed.4.layers.0.weight', 'bbox_embed.4.layers.0.bias', 'bbox_embed.4.layers.1.weight', 'bbox_embed.4.layers.1.bias', 'bbox_embed.4.layers.2.weight', 'bbox_embed.4.layers.2.bias', 'bbox_embed.5.layers.0.weight', 'bbox_embed.5.layers.0.bias', 'bbox_embed.5.layers.1.weight', 'bbox_embed.5.layers.1.bias', 'bbox_embed.5.layers.2.weight', 'bbox_embed.5.layers.2.bias'].</code></pre>
</div>
</div>
</section>
<section id="plotting-our-models-loss-curves" class="level3" data-number="14.5">
<h3 data-number="14.5" class="anchored" data-anchor-id="plotting-our-models-loss-curves"><span class="header-section-number">14.5</span> Plotting our model’s loss curves</h3>
<p>Looking at the metrics output by our model’s training process, we can see the loss values going down on the training and evaluation datasets.</p>
<p>As well as the mAP going up almost universally across the board.</p>
<p>Let’s make things visual by inspecting the loss curves and evaluation metric curves of our model.</p>
<p>We can extract our model’s training history values via the <code>model_v1_trainer.state.log_history</code> attribute, this will return us a list of dictionaries containing training metrics related to each epoch.</p>
<p>Once we’ve got these, we can create lists of relevant values based on their keys and then plot them with <code>matplotlib</code>.</p>
<div id="cell-214" class="cell" data-execution_count="88">
<div class="sourceCode cell-code" id="cb168"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb168-1"><a href="#cb168-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb168-2"><a href="#cb168-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb168-3"><a href="#cb168-3" aria-hidden="true" tabindex="-1"></a>log_history <span class="op">=</span> model_v1_trainer.state.log_history</span>
<span id="cb168-4"><a href="#cb168-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb168-5"><a href="#cb168-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Exctract loss values</span></span>
<span id="cb168-6"><a href="#cb168-6" aria-hidden="true" tabindex="-1"></a>train_loss <span class="op">=</span> [item[<span class="st">"loss"</span>] <span class="cf">for</span> item <span class="kw">in</span> log_history <span class="cf">if</span> <span class="st">"loss"</span> <span class="kw">in</span> item]</span>
<span id="cb168-7"><a href="#cb168-7" aria-hidden="true" tabindex="-1"></a>eval_loss <span class="op">=</span> [item[<span class="st">"eval_loss"</span>] <span class="cf">for</span> item <span class="kw">in</span> log_history <span class="cf">if</span> <span class="st">"eval_loss"</span> <span class="kw">in</span> item]</span>
<span id="cb168-8"><a href="#cb168-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb168-9"><a href="#cb168-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract mAP values</span></span>
<span id="cb168-10"><a href="#cb168-10" aria-hidden="true" tabindex="-1"></a>eval_map <span class="op">=</span> [item[<span class="st">"eval_map"</span>] <span class="cf">for</span> item <span class="kw">in</span> log_history <span class="cf">if</span> <span class="st">"eval_map"</span> <span class="kw">in</span> item]</span>
<span id="cb168-11"><a href="#cb168-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb168-12"><a href="#cb168-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot loss curves and mAP</span></span>
<span id="cb168-13"><a href="#cb168-13" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">1</span>, ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">7</span>))</span>
<span id="cb168-14"><a href="#cb168-14" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(train_loss, label<span class="op">=</span><span class="st">"Train loss"</span>)</span>
<span id="cb168-15"><a href="#cb168-15" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(eval_loss, label<span class="op">=</span><span class="st">"Eval loss"</span>)</span>
<span id="cb168-16"><a href="#cb168-16" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"Loss Curves (lower is better)"</span>)</span>
<span id="cb168-17"><a href="#cb168-17" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">"Loss Value"</span>)</span>
<span id="cb168-18"><a href="#cb168-18" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb168-19"><a href="#cb168-19" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].legend()</span>
<span id="cb168-20"><a href="#cb168-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb168-21"><a href="#cb168-21" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(eval_map, label<span class="op">=</span><span class="st">"Eval mAP"</span>)</span>
<span id="cb168-22"><a href="#cb168-22" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"Eval mAP (higher is better)"</span>)</span>
<span id="cb168-23"><a href="#cb168-23" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="st">"mAP (Mean Average Precision)"</span>)</span>
<span id="cb168-24"><a href="#cb168-24" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb168-25"><a href="#cb168-25" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].legend()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="hugging_face_object_detection_tutorial_files/figure-html/cell-87-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Beautiful!</p>
<p>Those are the exact kind of performance curves we’re looking for.</p>
<p>In an ideal world, the loss curves trend downwards and the mAP (Mean Average Percision) curves trend upwards.</p>
<p>We’ve only trained for 10 epochs here (10 laps of the data), perhaps our metrics would be even better if we were to train for longer?</p>
<p>I’ll leave this as an extension for you to try.</p>
</section>
</section>
<section id="making-predictions-on-the-test-dataset" class="level2" data-number="15">
<h2 data-number="15" class="anchored" data-anchor-id="making-predictions-on-the-test-dataset"><span class="header-section-number">15</span> Making predictions on the test dataset</h2>
<p>We’ve trained a model on our training data (<code>processed_dataset["train"]</code>) and considering the metrics on the validation data (<code>processed_dataset["validation"]</code>) it looks like it’s performing well.</p>
<p>However, there’s nothing quite like performing predictions on <em>unseen</em> test data and seeing how they go.</p>
<p>We can make predictions using our trained model by passing it samples formatted in the same way it was trained on.</p>
<p>Good news is, we’ve already got preprocessed test samples (our model has never seen these) in <code>processed_dataset["test"]</code>.</p>
<p>Let’s start by inspecting a single processed test sample and then we’ll make predictions on the whole test dataset.</p>
<div id="cell-217" class="cell" data-execution_count="89">
<div class="sourceCode cell-code" id="cb169"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb169-1"><a href="#cb169-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Our dataset is broken into "train", "validation", "test"</span></span>
<span id="cb169-2"><a href="#cb169-2" aria-hidden="true" tabindex="-1"></a>processed_dataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="89">
<pre><code>{'train': Dataset({
     features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],
     num_rows: 789
 }),
 'validation': Dataset({
     features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],
     num_rows: 115
 }),
 'test': Dataset({
     features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],
     num_rows: 224
 })}</code></pre>
</div>
</div>
<div id="cell-218" class="cell" data-execution_count="90">
<div class="sourceCode cell-code" id="cb171"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb171-1"><a href="#cb171-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect a single sample of the processed test dataset</span></span>
<span id="cb171-2"><a href="#cb171-2" aria-hidden="true" tabindex="-1"></a>processed_dataset[<span class="st">"test"</span>][<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="90">
<pre><code>{'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         ...,
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1]]),
 'pixel_values': tensor([[[0.2627, 0.3176, 0.2627,  ..., 0.0510, 0.0667, 0.1843],
          [0.1882, 0.2706, 0.3961,  ..., 0.0510, 0.0902, 0.3569],
          [0.1451, 0.2235, 0.4392,  ..., 0.0549, 0.1922, 0.3608],
          ...,
          [0.7882, 0.7882, 0.7922,  ..., 0.3373, 0.4196, 0.2588],
          [0.7843, 0.7961, 0.8078,  ..., 0.2863, 0.4941, 0.3725],
          [0.7765, 0.7922, 0.8078,  ..., 0.2627, 0.5255, 0.4471]],
 
         [[0.3333, 0.3765, 0.3098,  ..., 0.0745, 0.0941, 0.2118],
          [0.2588, 0.3333, 0.4471,  ..., 0.0784, 0.1137, 0.3843],
          [0.2157, 0.2902, 0.4902,  ..., 0.0863, 0.2196, 0.3882],
          ...,
          [0.0745, 0.0745, 0.0784,  ..., 0.3686, 0.4627, 0.2941],
          [0.0706, 0.0824, 0.0941,  ..., 0.3176, 0.5412, 0.4157],
          [0.0627, 0.0784, 0.0941,  ..., 0.2980, 0.5725, 0.4902]],
 
         [[0.1686, 0.2471, 0.2196,  ..., 0.0275, 0.0471, 0.1765],
          [0.0941, 0.1922, 0.3412,  ..., 0.0235, 0.0784, 0.3490],
          [0.0353, 0.1373, 0.3686,  ..., 0.0314, 0.1725, 0.3412],
          ...,
          [0.1216, 0.1216, 0.1255,  ..., 0.1922, 0.2196, 0.1294],
          [0.1176, 0.1294, 0.1412,  ..., 0.1451, 0.2863, 0.1804],
          [0.1098, 0.1255, 0.1412,  ..., 0.1020, 0.2941, 0.2039]]]),
 'labels': {'size': tensor([640, 480]), 'image_id': tensor([61]), 'class_labels': tensor([4, 5, 1, 0]), 'boxes': tensor([[0.2104, 0.8563, 0.2855, 0.2720],
         [0.4194, 0.4927, 0.2398, 0.1785],
         [0.3610, 0.6227, 0.2706, 0.2330],
         [0.4974, 0.4785, 0.3829, 0.3820]]), 'area': tensor([23860.4043, 13150.1748, 19368.0898, 44929.9102]), 'iscrowd': tensor([0, 0, 0, 0]), 'orig_size': tensor([1280,  960])}}</code></pre>
</div>
</div>
<p>Wonderful, looks like these are ready to go!</p>
<p>We can make predictions on the test dataset using the <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.predict"><code>transformers.Trainer.predict</code> method</a>, this will output a named tuple of <code>predictions</code> and <code>label_ids</code>.</p>
<div id="cell-220" class="cell" data-execution_count="91">
<div class="sourceCode cell-code" id="cb173"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb173-1"><a href="#cb173-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions with trainer containing trained model</span></span>
<span id="cb173-2"><a href="#cb173-2" aria-hidden="true" tabindex="-1"></a>test_dataset_preds <span class="op">=</span> model_v1_trainer.predict(test_dataset<span class="op">=</span>processed_dataset[<span class="st">"test"</span>])</span>
<span id="cb173-3"><a href="#cb173-3" aria-hidden="true" tabindex="-1"></a><span class="co"># test_dataset_preds</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

</div>
</div>
<p>Our predictions come in batches, just like our training was done.</p>
<p>We can extract the prediction values (logits and predicted bounding boxes) via the <code>.predictions</code> attribute. And the label outputs (the ground truths) via the <code>label_ids</code> attribute.</p>
<div id="cell-222" class="cell" data-execution_count="92">
<div class="sourceCode cell-code" id="cb174"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb174-1"><a href="#cb174-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions come in the same batch size as our training setup</span></span>
<span id="cb174-2"><a href="#cb174-2" aria-hidden="true" tabindex="-1"></a>test_dataset_prediction_outputs <span class="op">=</span> test_dataset_preds.predictions</span>
<span id="cb174-3"><a href="#cb174-3" aria-hidden="true" tabindex="-1"></a>test_dataset_label_outputs <span class="op">=</span> test_dataset_preds.label_ids</span>
<span id="cb174-4"><a href="#cb174-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb174-5"><a href="#cb174-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Found </span><span class="sc">{</span><span class="bu">len</span>(test_dataset_prediction_outputs)<span class="sc">}</span><span class="ss"> batches of prediction samples and </span><span class="sc">{</span><span class="bu">len</span>(test_dataset_label_outputs)<span class="sc">}</span><span class="ss"> batches of labels."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Found 14 batches of prediction samples and 14 batches of labels.</code></pre>
</div>
</div>
<p>We can inspect a batch of predictions by taking the 0th index of <code>test_dataset_prediction_outputs</code>, inside this batch are:</p>
<ul>
<li>Index <code>0</code> - Metrics in the form of a dictionary.</li>
<li>Index <code>1</code> - Logits in the form of a NumPy array.</li>
<li>Index <code>2</code> - Bounding box coordinates in the form of a NumPy array.</li>
</ul>
<div id="cell-224" class="cell" data-execution_count="93">
<div class="sourceCode cell-code" id="cb176"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb176-1"><a href="#cb176-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the first batch of test prediction samples </span></span>
<span id="cb176-2"><a href="#cb176-2" aria-hidden="true" tabindex="-1"></a>test_batch_metrics <span class="op">=</span> test_dataset_prediction_outputs[<span class="dv">0</span>][<span class="dv">0</span>] <span class="co"># metrics come at index 0 in the form of a dictionary</span></span>
<span id="cb176-3"><a href="#cb176-3" aria-hidden="true" tabindex="-1"></a>test_batch_logits <span class="op">=</span> test_dataset_prediction_outputs[<span class="dv">0</span>][<span class="dv">1</span>] <span class="co"># logits come at index 1 in a numpy array</span></span>
<span id="cb176-4"><a href="#cb176-4" aria-hidden="true" tabindex="-1"></a>test_batch_boxes <span class="op">=</span> test_dataset_prediction_outputs[<span class="dv">0</span>][<span class="dv">2</span>] <span class="co"># predicted boxes come at index 2 in a numpy array</span></span>
<span id="cb176-5"><a href="#cb176-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb176-6"><a href="#cb176-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Metrics keys: </span><span class="sc">{</span>test_batch_metrics<span class="sc">.</span>keys()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb176-7"><a href="#cb176-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Test predictions single batch logits shape: </span><span class="sc">{</span>test_batch_logits<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; (batch_size, num_predictions, logit_per_class)"</span>)</span>
<span id="cb176-8"><a href="#cb176-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Test predictions single batch boxes shape: </span><span class="sc">{</span>test_batch_boxes<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; (batch_size, num_predictions, box_coordinates)"</span>)</span>
<span id="cb176-9"><a href="#cb176-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Test logits type: </span><span class="sc">{</span><span class="bu">type</span>(test_batch_logits)<span class="sc">}</span><span class="ss"> | Test boxes type: </span><span class="sc">{</span><span class="bu">type</span>(test_batch_boxes)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Metrics keys: dict_keys(['loss_vfl', 'loss_bbox', 'loss_giou', 'loss_vfl_aux_0', 'loss_bbox_aux_0', 'loss_giou_aux_0', 'loss_vfl_aux_1', 'loss_bbox_aux_1', 'loss_giou_aux_1', 'loss_vfl_aux_2', 'loss_bbox_aux_2', 'loss_giou_aux_2', 'loss_vfl_aux_3', 'loss_bbox_aux_3', 'loss_giou_aux_3', 'loss_vfl_aux_4', 'loss_bbox_aux_4', 'loss_giou_aux_4', 'loss_vfl_aux_5', 'loss_bbox_aux_5', 'loss_giou_aux_5'])
[INFO] Test predictions single batch logits shape: (16, 300, 7) -&gt; (batch_size, num_predictions, logit_per_class)
[INFO] Test predictions single batch boxes shape: (16, 300, 4) -&gt; (batch_size, num_predictions, box_coordinates)
[INFO] Test logits type: &lt;class 'numpy.ndarray'&gt; | Test boxes type: &lt;class 'numpy.ndarray'&gt;</code></pre>
</div>
</div>
<p>Let’s concatenate all of the batches of test predictions into one single <code>numpy.ndarray</code>. We’ll then turn them into <code>torch.tensor</code>’s so we can use them with our post-processing methods.</p>
<div id="cell-226" class="cell" data-execution_count="94">
<div class="sourceCode cell-code" id="cb178"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb178-1"><a href="#cb178-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We can stack these together to get the full outputs</span></span>
<span id="cb178-2"><a href="#cb178-2" aria-hidden="true" tabindex="-1"></a>test_dataset_pred_logits <span class="op">=</span> []</span>
<span id="cb178-3"><a href="#cb178-3" aria-hidden="true" tabindex="-1"></a>test_dataset_pred_boxes <span class="op">=</span> []</span>
<span id="cb178-4"><a href="#cb178-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb178-5"><a href="#cb178-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> test_pred_batch <span class="kw">in</span> test_dataset_prediction_outputs:</span>
<span id="cb178-6"><a href="#cb178-6" aria-hidden="true" tabindex="-1"></a>    test_dataset_pred_logits.append(test_pred_batch[<span class="dv">1</span>]) <span class="co"># logits come at index 1</span></span>
<span id="cb178-7"><a href="#cb178-7" aria-hidden="true" tabindex="-1"></a>    test_dataset_pred_boxes.append(test_pred_batch[<span class="dv">2</span>]) <span class="co"># boxes come at index 2</span></span>
<span id="cb178-8"><a href="#cb178-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb178-9"><a href="#cb178-9" aria-hidden="true" tabindex="-1"></a>test_dataset_pred_logits <span class="op">=</span> torch.tensor(np.concatenate(test_dataset_pred_logits))</span>
<span id="cb178-10"><a href="#cb178-10" aria-hidden="true" tabindex="-1"></a>test_dataset_pred_boxes <span class="op">=</span> torch.tensor(np.concatenate(test_dataset_pred_boxes))</span>
<span id="cb178-11"><a href="#cb178-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb178-12"><a href="#cb178-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Test predictions logits shape: </span><span class="sc">{</span>test_dataset_pred_logits<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; (num_samples, num_predictions, logit_per_class)"</span>)</span>
<span id="cb178-13"><a href="#cb178-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Test predictions boxes shape: </span><span class="sc">{</span>test_dataset_pred_boxes<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> -&gt; (num_samples, num_predictions, box_coordinates - CXCYWH normalize format)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Test predictions logits shape: torch.Size([224, 300, 7]) -&gt; (num_samples, num_predictions, logit_per_class)
[INFO] Test predictions boxes shape: torch.Size([224, 300, 4]) -&gt; (num_samples, num_predictions, box_coordinates - CXCYWH normalize format)</code></pre>
</div>
</div>
<section id="evaluating-our-test-predictions" class="level3" data-number="15.1">
<h3 data-number="15.1" class="anchored" data-anchor-id="evaluating-our-test-predictions"><span class="header-section-number">15.1</span> Evaluating our test predictions</h3>
<p>Now we’ve got our predicted logits and boxes, we can format them in a way so we can evaluate them with <a href="https://lightning.ai/docs/torchmetrics/stable/detection/mean_average_precision.html"><code>torchmetrics.detection.meap_ap.MeanAveragePrecision</code></a>.</p>
<p>The <code>MeanAveragePrecision</code> metric wants the following:</p>
<ul>
<li><code>preds</code> (<code>List</code>) - a list of dictionaries (one per image) with the keys <code>boxes</code> (in the default format XYXY and absolute), <code>scores</code> and <code>labels</code>. Where all values in the dictionaries are <code>torch.Tensor</code>.</li>
<li><code>target</code> (<code>List</code>) - a list of dictionaries (one per image) with the keys <code>boxes</code> (in the default format XYXY and absolute), <code>labels</code>. Where all values in the dictionaries are <code>torch.Tensor</code>.</li>
</ul>
<p>In essence, our <code>preds</code> have scores (prediction probabilities) where as our <code>target</code>s do not.</p>
<p>Let’s start by collecting a list of dictionaries for our <code>preds</code>.</p>
<p>We’ll do so by iterating over our <code>test_dataset_pred_logits</code> and <code>test_dataset_pred_boxes</code> and passing the required inputs to <a href="https://huggingface.co/docs/transformers/main/en/model_doc/rt_detr#transformers.RTDetrImageProcessor.post_process_object_detection"><code>transformers.RTDetrImageProcessor.post_process_object_detection</code></a>.</p>
<div id="cell-228" class="cell" data-execution_count="95">
<div class="sourceCode cell-code" id="cb180"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb180-1"><a href="#cb180-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an empty list for preds</span></span>
<span id="cb180-2"><a href="#cb180-2" aria-hidden="true" tabindex="-1"></a>test_dataset_prediction_dicts <span class="op">=</span> []</span>
<span id="cb180-3"><a href="#cb180-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb180-4"><a href="#cb180-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set a threshold for prediction probabilities (we'll use 0.0 to allow all possible predictions, change this if you feel like)</span></span>
<span id="cb180-5"><a href="#cb180-5" aria-hidden="true" tabindex="-1"></a>THRESHOLD <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb180-6"><a href="#cb180-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb180-7"><a href="#cb180-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate through prediction logits and prediction boxes</span></span>
<span id="cb180-8"><a href="#cb180-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(test_dataset_pred_boxes)):</span>
<span id="cb180-9"><a href="#cb180-9" aria-hidden="true" tabindex="-1"></a>    pred_logits <span class="op">=</span> test_dataset_pred_logits[i].unsqueeze(<span class="dv">0</span>) <span class="co"># add a batch dimension of 1</span></span>
<span id="cb180-10"><a href="#cb180-10" aria-hidden="true" tabindex="-1"></a>    pred_boxes <span class="op">=</span> test_dataset_pred_boxes[i].unsqueeze(<span class="dv">0</span>) </span>
<span id="cb180-11"><a href="#cb180-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb180-12"><a href="#cb180-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get original size of input image (required for post processing)</span></span>
<span id="cb180-13"><a href="#cb180-13" aria-hidden="true" tabindex="-1"></a>    original_size <span class="op">=</span> processed_dataset[<span class="st">"test"</span>][i][<span class="st">"labels"</span>][<span class="st">"orig_size"</span>].unsqueeze(<span class="dv">0</span>) <span class="co"># comes in height, width, we add a batch dimension of 1</span></span>
<span id="cb180-14"><a href="#cb180-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb180-15"><a href="#cb180-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Collect prediction outputs</span></span>
<span id="cb180-16"><a href="#cb180-16" aria-hidden="true" tabindex="-1"></a>    pred_outputs <span class="op">=</span> ModelOutput(logits<span class="op">=</span>pred_logits,</span>
<span id="cb180-17"><a href="#cb180-17" aria-hidden="true" tabindex="-1"></a>                               pred_boxes<span class="op">=</span>pred_boxes)</span>
<span id="cb180-18"><a href="#cb180-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb180-19"><a href="#cb180-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Post process (boxes will automatically be output in XYXY absolute format)</span></span>
<span id="cb180-20"><a href="#cb180-20" aria-hidden="true" tabindex="-1"></a>    pred_outputs_post_processed <span class="op">=</span> image_processor.post_process_object_detection(</span>
<span id="cb180-21"><a href="#cb180-21" aria-hidden="true" tabindex="-1"></a>        outputs<span class="op">=</span>pred_outputs,</span>
<span id="cb180-22"><a href="#cb180-22" aria-hidden="true" tabindex="-1"></a>        threshold<span class="op">=</span>THRESHOLD,</span>
<span id="cb180-23"><a href="#cb180-23" aria-hidden="true" tabindex="-1"></a>        target_sizes<span class="op">=</span>original_size</span>
<span id="cb180-24"><a href="#cb180-24" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb180-25"><a href="#cb180-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb180-26"><a href="#cb180-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a dictionary of post processed outputs</span></span>
<span id="cb180-27"><a href="#cb180-27" aria-hidden="true" tabindex="-1"></a>    prediction_dict <span class="op">=</span> {<span class="st">"boxes"</span>: pred_outputs_post_processed[<span class="dv">0</span>][<span class="st">"boxes"</span>],</span>
<span id="cb180-28"><a href="#cb180-28" aria-hidden="true" tabindex="-1"></a>                       <span class="st">"scores"</span>: pred_outputs_post_processed[<span class="dv">0</span>][<span class="st">"scores"</span>],</span>
<span id="cb180-29"><a href="#cb180-29" aria-hidden="true" tabindex="-1"></a>                       <span class="st">"labels"</span>: pred_outputs_post_processed[<span class="dv">0</span>][<span class="st">"labels"</span>]}</span>
<span id="cb180-30"><a href="#cb180-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb180-31"><a href="#cb180-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Append dictionary to list</span></span>
<span id="cb180-32"><a href="#cb180-32" aria-hidden="true" tabindex="-1"></a>    test_dataset_prediction_dicts.append(prediction_dict)</span>
<span id="cb180-33"><a href="#cb180-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb180-34"><a href="#cb180-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Number of prediction dicts: </span><span class="sc">{</span><span class="bu">len</span>(test_dataset_prediction_dicts)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb180-35"><a href="#cb180-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Example prediction dict:"</span>)</span>
<span id="cb180-36"><a href="#cb180-36" aria-hidden="true" tabindex="-1"></a>test_dataset_prediction_dicts[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Number of prediction dicts: 224
[INFO] Example prediction dict:</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="95">
<pre><code>{'boxes': tensor([[ 221.0374,  646.1925,  466.7746,  948.0635],
         [ 284.1779,  500.8758,  513.3552,  748.1806],
         [ 297.0548,  382.5186,  656.9966,  854.1584],
         ...,
         [ 360.6175,  257.2795,  654.1285,  513.4293],
         [   6.4380,  659.8294,  456.9098, 1273.5051],
         [ 509.2941,  337.9673,  714.2663,  552.0175]]),
 'scores': tensor([0.6817, 0.5330, 0.5030, 0.3065, 0.1816, 0.1395, 0.1393, 0.1264, 0.1200,
         0.1047, 0.1012, 0.1011, 0.0998, 0.0969, 0.0922, 0.0863, 0.0857, 0.0818,
         0.0805, 0.0775, 0.0774, 0.0763, 0.0707, 0.0700, 0.0689, 0.0678, 0.0671,
         0.0656, 0.0637, 0.0603, 0.0591, 0.0583, 0.0572, 0.0565, 0.0565, 0.0564,
         0.0550, 0.0548, 0.0537, 0.0533, 0.0530, 0.0527, 0.0522, 0.0520, 0.0514,
         0.0514, 0.0505, 0.0502, 0.0500, 0.0500, 0.0489, 0.0485, 0.0480, 0.0480,
         0.0477, 0.0473, 0.0472, 0.0466, 0.0466, 0.0461, 0.0460, 0.0459, 0.0458,
         0.0457, 0.0450, 0.0449, 0.0448, 0.0448, 0.0445, 0.0444, 0.0442, 0.0442,
         0.0438, 0.0436, 0.0422, 0.0422, 0.0419, 0.0416, 0.0415, 0.0410, 0.0408,
         0.0404, 0.0399, 0.0399, 0.0397, 0.0394, 0.0393, 0.0393, 0.0392, 0.0392,
         0.0390, 0.0389, 0.0389, 0.0387, 0.0385, 0.0385, 0.0384, 0.0383, 0.0382,
         0.0380, 0.0380, 0.0380, 0.0380, 0.0380, 0.0380, 0.0374, 0.0373, 0.0372,
         0.0371, 0.0370, 0.0370, 0.0368, 0.0368, 0.0368, 0.0365, 0.0365, 0.0365,
         0.0364, 0.0362, 0.0362, 0.0361, 0.0358, 0.0358, 0.0356, 0.0354, 0.0353,
         0.0352, 0.0349, 0.0349, 0.0348, 0.0348, 0.0348, 0.0347, 0.0345, 0.0344,
         0.0344, 0.0343, 0.0342, 0.0342, 0.0338, 0.0332, 0.0332, 0.0331, 0.0331,
         0.0330, 0.0330, 0.0330, 0.0328, 0.0327, 0.0326, 0.0326, 0.0323, 0.0323,
         0.0322, 0.0322, 0.0320, 0.0319, 0.0319, 0.0318, 0.0317, 0.0316, 0.0315,
         0.0315, 0.0314, 0.0312, 0.0311, 0.0309, 0.0308, 0.0306, 0.0304, 0.0302,
         0.0300, 0.0300, 0.0299, 0.0299, 0.0296, 0.0294, 0.0293, 0.0291, 0.0289,
         0.0287, 0.0287, 0.0286, 0.0284, 0.0281, 0.0281, 0.0280, 0.0280, 0.0279,
         0.0279, 0.0277, 0.0276, 0.0276, 0.0275, 0.0275, 0.0273, 0.0271, 0.0270,
         0.0268, 0.0267, 0.0267, 0.0267, 0.0265, 0.0264, 0.0263, 0.0263, 0.0263,
         0.0262, 0.0261, 0.0260, 0.0260, 0.0259, 0.0259, 0.0257, 0.0257, 0.0256,
         0.0256, 0.0256, 0.0256, 0.0256, 0.0254, 0.0254, 0.0253, 0.0253, 0.0252,
         0.0252, 0.0250, 0.0249, 0.0248, 0.0248, 0.0247, 0.0247, 0.0246, 0.0246,
         0.0245, 0.0245, 0.0244, 0.0242, 0.0241, 0.0241, 0.0241, 0.0241, 0.0240,
         0.0239, 0.0239, 0.0239, 0.0239, 0.0239, 0.0238, 0.0238, 0.0238, 0.0238,
         0.0237, 0.0237, 0.0237, 0.0236, 0.0236, 0.0235, 0.0234, 0.0232, 0.0232,
         0.0232, 0.0231, 0.0231, 0.0230, 0.0229, 0.0228, 0.0228, 0.0228, 0.0228,
         0.0228, 0.0228, 0.0227, 0.0227, 0.0227, 0.0226, 0.0225, 0.0224, 0.0222,
         0.0222, 0.0222, 0.0221, 0.0221, 0.0220, 0.0220, 0.0219, 0.0219, 0.0219,
         0.0219, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0218, 0.0217, 0.0217,
         0.0217, 0.0216, 0.0216]),
 'labels': tensor([1, 5, 0, 0, 4, 5, 5, 5, 4, 1, 4, 0, 4, 1, 1, 4, 1, 3, 1, 4, 4, 2, 0, 1,
         4, 1, 5, 0, 4, 0, 1, 6, 1, 4, 5, 4, 0, 0, 0, 3, 1, 0, 4, 3, 6, 0, 1, 4,
         5, 0, 4, 1, 4, 0, 4, 1, 4, 1, 0, 5, 3, 4, 1, 1, 0, 1, 1, 3, 3, 6, 5, 4,
         2, 4, 0, 5, 4, 2, 1, 4, 4, 1, 0, 0, 5, 4, 1, 1, 0, 4, 1, 1, 3, 3, 4, 5,
         0, 4, 1, 3, 5, 1, 0, 4, 4, 5, 4, 4, 6, 4, 5, 4, 4, 4, 4, 1, 5, 3, 2, 4,
         3, 4, 4, 0, 4, 0, 3, 0, 0, 2, 2, 5, 5, 3, 4, 0, 4, 3, 0, 5, 0, 1, 4, 1,
         5, 4, 1, 1, 2, 5, 4, 1, 4, 4, 2, 5, 0, 2, 1, 1, 2, 0, 6, 4, 4, 1, 5, 1,
         4, 1, 4, 1, 3, 3, 1, 5, 0, 3, 3, 1, 0, 5, 5, 0, 0, 4, 5, 4, 0, 5, 1, 1,
         1, 1, 4, 5, 5, 4, 2, 1, 4, 4, 1, 0, 4, 0, 0, 0, 0, 5, 1, 0, 4, 1, 6, 3,
         1, 5, 4, 5, 3, 1, 1, 0, 1, 6, 2, 5, 2, 0, 3, 4, 4, 0, 1, 5, 0, 6, 5, 5,
         0, 0, 5, 1, 0, 6, 1, 1, 0, 5, 0, 3, 5, 4, 4, 5, 2, 6, 4, 5, 1, 3, 3, 1,
         0, 0, 0, 2, 0, 2, 3, 5, 5, 5, 3, 4, 4, 5, 3, 0, 3, 4, 4, 0, 1, 2, 0, 0,
         5, 1, 4, 6, 4, 4, 5, 1, 5, 1, 3, 1])}</code></pre>
</div>
</div>
<p>Beautiful! We’ve now got a list of prediction dictionaries.</p>
<p>Let’s do the same for our targets.</p>
<p>We’ll iterate through each sample in <code>proecess_dataset["test"]</code> and create a target dictionary for each sample.</p>
<p>The main difference is that we’ll have to convert the boxes from CXCYWH normalized to XYXY absolute.</p>
<p>Luckily, we’ve got our handy <code>convert_bbox_cxcywh_to_xyxy_absolute</code> helper function to do just that!</p>
<div id="cell-230" class="cell" data-execution_count="96">
<div class="sourceCode cell-code" id="cb183"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb183-1"><a href="#cb183-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list for targets</span></span>
<span id="cb183-2"><a href="#cb183-2" aria-hidden="true" tabindex="-1"></a>test_dataset_target_dicts <span class="op">=</span> []</span>
<span id="cb183-3"><a href="#cb183-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb183-4"><a href="#cb183-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate through test samples</span></span>
<span id="cb183-5"><a href="#cb183-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> test_sample <span class="kw">in</span> processed_dataset[<span class="st">"test"</span>]:</span>
<span id="cb183-6"><a href="#cb183-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb183-7"><a href="#cb183-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract truth labels</span></span>
<span id="cb183-8"><a href="#cb183-8" aria-hidden="true" tabindex="-1"></a>    sample_labels <span class="op">=</span> test_sample[<span class="st">"labels"</span>]</span>
<span id="cb183-9"><a href="#cb183-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb183-10"><a href="#cb183-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract class labels and boxes</span></span>
<span id="cb183-11"><a href="#cb183-11" aria-hidden="true" tabindex="-1"></a>    truth_class_labels <span class="op">=</span> sample_labels[<span class="st">"class_labels"</span>]</span>
<span id="cb183-12"><a href="#cb183-12" aria-hidden="true" tabindex="-1"></a>    truth_boxes <span class="op">=</span> sample_labels[<span class="st">"boxes"</span>]</span>
<span id="cb183-13"><a href="#cb183-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb183-14"><a href="#cb183-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get original size of image</span></span>
<span id="cb183-15"><a href="#cb183-15" aria-hidden="true" tabindex="-1"></a>    original_size <span class="op">=</span> sample_labels[<span class="st">"orig_size"</span>] <span class="co"># size of original image in (height, width)</span></span>
<span id="cb183-16"><a href="#cb183-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb183-17"><a href="#cb183-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert boxes from CXCYWH normalized to XYXY absolute</span></span>
<span id="cb183-18"><a href="#cb183-18" aria-hidden="true" tabindex="-1"></a>    truth_boxes_xyxy <span class="op">=</span> convert_bbox_cxcywh_to_xyxy_absolute(boxes<span class="op">=</span>truth_boxes, </span>
<span id="cb183-19"><a href="#cb183-19" aria-hidden="true" tabindex="-1"></a>                                                            image_size_target<span class="op">=</span>original_size)</span>
<span id="cb183-20"><a href="#cb183-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb183-21"><a href="#cb183-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create target truth dictionary</span></span>
<span id="cb183-22"><a href="#cb183-22" aria-hidden="true" tabindex="-1"></a>    target_dict <span class="op">=</span> {<span class="st">"boxes"</span>: truth_boxes_xyxy,</span>
<span id="cb183-23"><a href="#cb183-23" aria-hidden="true" tabindex="-1"></a>                   <span class="st">"labels"</span>: truth_class_labels}</span>
<span id="cb183-24"><a href="#cb183-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb183-25"><a href="#cb183-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Append target dictionary to list</span></span>
<span id="cb183-26"><a href="#cb183-26" aria-hidden="true" tabindex="-1"></a>    test_dataset_target_dicts.append(target_dict)</span>
<span id="cb183-27"><a href="#cb183-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb183-28"><a href="#cb183-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Number of target dictionaries: </span><span class="sc">{</span><span class="bu">len</span>(test_dataset_target_dicts)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb183-29"><a href="#cb183-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Example target dictionary:"</span>)</span>
<span id="cb183-30"><a href="#cb183-30" aria-hidden="true" tabindex="-1"></a>test_dataset_target_dicts[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Number of target dictionaries: 224
[INFO] Example target dictionary:</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="96">
<pre><code>{'boxes': tensor([[  64.9000,  922.0001,  339.0000, 1270.2000],
         [ 287.5000,  516.4000,  517.7000,  744.9000],
         [ 216.7000,  647.9999,  476.5000,  946.2000],
         [ 293.7000,  368.0000,  661.3000,  856.9000]]),
 'labels': tensor([4, 5, 1, 0])}</code></pre>
</div>
</div>
<p>Alright, now we’ve got a list of <code>preds</code> in <code>test_dataset_prediction_dicts</code> and a list of <code>targets</code> in <code>test_dataset_target_dicts</code>, let’s create an instance of <code>MeanAveragePrecision</code> and use to calculate metrics comparing our predictions to the ground truth.</p>
<p>We’ll set the <code>class_metrics=True</code> parameter so we can get a breakdown of the mAP (Mean Average Precision) and mAR (Mean Average Recall) for each class.</p>
<div id="cell-232" class="cell" data-execution_count="97">
<div class="sourceCode cell-code" id="cb186"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb186-1"><a href="#cb186-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare predictions to targets</span></span>
<span id="cb186-2"><a href="#cb186-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchmetrics.detection.mean_ap <span class="im">import</span> MeanAveragePrecision</span>
<span id="cb186-3"><a href="#cb186-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb186-4"><a href="#cb186-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate MAP metric instance</span></span>
<span id="cb186-5"><a href="#cb186-5" aria-hidden="true" tabindex="-1"></a>map_metric <span class="op">=</span> MeanAveragePrecision(iou_type<span class="op">=</span><span class="st">"bbox"</span>, </span>
<span id="cb186-6"><a href="#cb186-6" aria-hidden="true" tabindex="-1"></a>                                  class_metrics<span class="op">=</span><span class="va">True</span>) <span class="co"># we want metrics for each individual class </span></span>
<span id="cb186-7"><a href="#cb186-7" aria-hidden="true" tabindex="-1"></a>map_metric.warn_on_many_detections <span class="op">=</span> <span class="va">False</span> <span class="co"># hide extra detection warnings</span></span>
<span id="cb186-8"><a href="#cb186-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb186-9"><a href="#cb186-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Update our metric with list of pred dicts and list of target dicts</span></span>
<span id="cb186-10"><a href="#cb186-10" aria-hidden="true" tabindex="-1"></a>map_metric.update(preds<span class="op">=</span>test_dataset_prediction_dicts, </span>
<span id="cb186-11"><a href="#cb186-11" aria-hidden="true" tabindex="-1"></a>                  target<span class="op">=</span>test_dataset_target_dicts)</span>
<span id="cb186-12"><a href="#cb186-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb186-13"><a href="#cb186-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the metric</span></span>
<span id="cb186-14"><a href="#cb186-14" aria-hidden="true" tabindex="-1"></a>test_metric_outputs <span class="op">=</span> map_metric.compute()</span>
<span id="cb186-15"><a href="#cb186-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb186-16"><a href="#cb186-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract per class metrics (we'll use these later on)</span></span>
<span id="cb186-17"><a href="#cb186-17" aria-hidden="true" tabindex="-1"></a>test_map_per_class <span class="op">=</span> test_metric_outputs.pop(<span class="st">"map_per_class"</span>)</span>
<span id="cb186-18"><a href="#cb186-18" aria-hidden="true" tabindex="-1"></a>test_mar_per_class <span class="op">=</span> test_metric_outputs.pop(<span class="st">"mar_100_per_class"</span>)</span>
<span id="cb186-19"><a href="#cb186-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb186-20"><a href="#cb186-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect the metrics</span></span>
<span id="cb186-21"><a href="#cb186-21" aria-hidden="true" tabindex="-1"></a>test_metric_outputs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="97">
<pre><code>{'map': tensor(0.3779),
 'map_50': tensor(0.5424),
 'map_75': tensor(0.4236),
 'map_small': tensor(0.),
 'map_medium': tensor(0.0672),
 'map_large': tensor(0.3950),
 'mar_1': tensor(0.4512),
 'mar_10': tensor(0.6943),
 'mar_100': tensor(0.7473),
 'mar_small': tensor(0.),
 'mar_medium': tensor(0.5421),
 'mar_large': tensor(0.7628),
 'classes': tensor([0, 1, 2, 3, 4, 5, 6], dtype=torch.int32)}</code></pre>
</div>
</div>
</section>
<section id="visualizing-our-test-dataset-evaluation-mertics" class="level3" data-number="15.2">
<h3 data-number="15.2" class="anchored" data-anchor-id="visualizing-our-test-dataset-evaluation-mertics"><span class="header-section-number">15.2</span> Visualizing our test dataset evaluation mertics</h3>
<p>We’ve now got some test dataset evaluation metrics, how about we follow the data explorer’s motto and visualize, visualize, visualize!</p>
<p>Let’s visualize a these in a bar chart.</p>
<div id="cell-234" class="cell" data-execution_count="98">
<div class="sourceCode cell-code" id="cb188"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb188-1"><a href="#cb188-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb188-2"><a href="#cb188-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb188-3"><a href="#cb188-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract mAP and mAR metrics</span></span>
<span id="cb188-4"><a href="#cb188-4" aria-hidden="true" tabindex="-1"></a>test_map_metrics <span class="op">=</span> {key: value <span class="cf">for</span> key, value <span class="kw">in</span> test_metric_outputs.items() <span class="cf">if</span> <span class="st">"map"</span> <span class="kw">in</span> key}</span>
<span id="cb188-5"><a href="#cb188-5" aria-hidden="true" tabindex="-1"></a>test_mar_metrics <span class="op">=</span> {key: value <span class="cf">for</span> key, value <span class="kw">in</span> test_metric_outputs.items() <span class="cf">if</span> <span class="st">"mar"</span> <span class="kw">in</span> key}</span>
<span id="cb188-6"><a href="#cb188-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb188-7"><a href="#cb188-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Get labels and values</span></span>
<span id="cb188-8"><a href="#cb188-8" aria-hidden="true" tabindex="-1"></a>test_map_labels, test_map_values <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span><span class="bu">sorted</span>(test_map_metrics.items()))</span>
<span id="cb188-9"><a href="#cb188-9" aria-hidden="true" tabindex="-1"></a>test_mar_labels, test_mar_values <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span><span class="bu">sorted</span>(test_mar_metrics.items()))</span>
<span id="cb188-10"><a href="#cb188-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb188-11"><a href="#cb188-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a subplot</span></span>
<span id="cb188-12"><a href="#cb188-12" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">1</span>, ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>), sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb188-13"><a href="#cb188-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb188-14"><a href="#cb188-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Add mAP values</span></span>
<span id="cb188-15"><a href="#cb188-15" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].bar(test_map_labels, test_map_values)</span>
<span id="cb188-16"><a href="#cb188-16" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"Mean Average Precision (mAP)"</span>)</span>
<span id="cb188-17"><a href="#cb188-17" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"Metric"</span>)</span>
<span id="cb188-18"><a href="#cb188-18" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">"Value"</span>)</span>
<span id="cb188-19"><a href="#cb188-19" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].tick_params(axis<span class="op">=</span><span class="st">"x"</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb188-20"><a href="#cb188-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb188-21"><a href="#cb188-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Add mAR values</span></span>
<span id="cb188-22"><a href="#cb188-22" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].bar(test_mar_labels, test_mar_values, color<span class="op">=</span><span class="st">"tab:orange"</span>)</span>
<span id="cb188-23"><a href="#cb188-23" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"Mean Average Recall (mAR)"</span>)</span>
<span id="cb188-24"><a href="#cb188-24" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"Metric"</span>)</span>
<span id="cb188-25"><a href="#cb188-25" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].tick_params(axis<span class="op">=</span><span class="st">"x"</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb188-26"><a href="#cb188-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb188-27"><a href="#cb188-27" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="hugging_face_object_detection_tutorial_files/figure-html/cell-97-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Nice! It looks like our model generally has a higher recall than precision, this is most likely because we set our prediction probability threshold to 0.0 (<code>THRESHOLD=0.0</code>).</p>
<p>This means all possible predictions are allowed through, in turn leading to the highest possible recall metric.</p>
<p>Let’s now visualize per class values.</p>
<div id="cell-236" class="cell" data-execution_count="99">
<div class="sourceCode cell-code" id="cb189"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb189-1"><a href="#cb189-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Map class names to metric values </span></span>
<span id="cb189-2"><a href="#cb189-2" aria-hidden="true" tabindex="-1"></a>test_map_per_class_dict <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">zip</span>(<span class="bu">list</span>(label2id.keys()), test_map_per_class))</span>
<span id="cb189-3"><a href="#cb189-3" aria-hidden="true" tabindex="-1"></a>test_mar_per_class_dict <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">zip</span>(<span class="bu">list</span>(label2id.keys()), test_mar_per_class))</span>
<span id="cb189-4"><a href="#cb189-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-5"><a href="#cb189-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Get labels and values</span></span>
<span id="cb189-6"><a href="#cb189-6" aria-hidden="true" tabindex="-1"></a>test_map_per_class_labels, test_map_per_class_values <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span><span class="bu">sorted</span>(test_map_per_class_dict.items()))</span>
<span id="cb189-7"><a href="#cb189-7" aria-hidden="true" tabindex="-1"></a>test_mar_per_class_labels, test_mar_per_class_values <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span><span class="bu">sorted</span>(test_mar_per_class_dict.items()))</span>
<span id="cb189-8"><a href="#cb189-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-9"><a href="#cb189-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list of RGB colour floats for matplotlib</span></span>
<span id="cb189-10"><a href="#cb189-10" aria-hidden="true" tabindex="-1"></a>label_to_colour_dict <span class="op">=</span> {key: normalize_rgb(value) <span class="cf">for</span> key, value <span class="kw">in</span> colour_palette.items()}</span>
<span id="cb189-11"><a href="#cb189-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-12"><a href="#cb189-12" aria-hidden="true" tabindex="-1"></a>colours_for_map <span class="op">=</span> [label_to_colour_dict.get(label_name, (<span class="fl">0.6</span>, <span class="fl">0.6</span>, <span class="fl">0.6</span>)) <span class="cf">for</span> label_name <span class="kw">in</span> test_map_per_class_labels] <span class="co"># (0.6, 0.6, 0.6) = fallback to grey colour</span></span>
<span id="cb189-13"><a href="#cb189-13" aria-hidden="true" tabindex="-1"></a>colours_for_mar <span class="op">=</span> [label_to_colour_dict.get(label_name, (<span class="fl">0.6</span>, <span class="fl">0.6</span>, <span class="fl">0.6</span>)) <span class="cf">for</span> label_name <span class="kw">in</span> test_mar_per_class_labels] <span class="co"># (0.6, 0.6, 0.6) = fallback to grey colour</span></span>
<span id="cb189-14"><a href="#cb189-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-15"><a href="#cb189-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a subplot</span></span>
<span id="cb189-16"><a href="#cb189-16" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">1</span>, ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>), sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb189-17"><a href="#cb189-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-18"><a href="#cb189-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Add mAP values</span></span>
<span id="cb189-19"><a href="#cb189-19" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].bar(test_map_per_class_labels, </span>
<span id="cb189-20"><a href="#cb189-20" aria-hidden="true" tabindex="-1"></a>          test_map_per_class_values,</span>
<span id="cb189-21"><a href="#cb189-21" aria-hidden="true" tabindex="-1"></a>          color<span class="op">=</span>colours_for_map)</span>
<span id="cb189-22"><a href="#cb189-22" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"Mean Average Precision (mAP)"</span>)</span>
<span id="cb189-23"><a href="#cb189-23" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"Metric"</span>)</span>
<span id="cb189-24"><a href="#cb189-24" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">"Value"</span>)</span>
<span id="cb189-25"><a href="#cb189-25" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].tick_params(axis<span class="op">=</span><span class="st">"x"</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb189-26"><a href="#cb189-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-27"><a href="#cb189-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Add mAR values</span></span>
<span id="cb189-28"><a href="#cb189-28" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].bar(test_mar_per_class_labels, </span>
<span id="cb189-29"><a href="#cb189-29" aria-hidden="true" tabindex="-1"></a>          test_mar_per_class_values, </span>
<span id="cb189-30"><a href="#cb189-30" aria-hidden="true" tabindex="-1"></a>          color<span class="op">=</span>colours_for_mar)</span>
<span id="cb189-31"><a href="#cb189-31" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"Mean Average Recall (mAR)"</span>)</span>
<span id="cb189-32"><a href="#cb189-32" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"Metric"</span>)</span>
<span id="cb189-33"><a href="#cb189-33" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].tick_params(axis<span class="op">=</span><span class="st">"x"</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb189-34"><a href="#cb189-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-35"><a href="#cb189-35" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="hugging_face_object_detection_tutorial_files/figure-html/cell-98-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Beautiful!</p>
<p>It looks like our main target classes (<code>bin</code>, <code>hand</code>, <code>trash</code>) are performing quite similarly on precision and recall.</p>
<p>We could balance the prediction probability threshold depending on what we’re trying to optimize for.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Which metric should you optimize for?</p>
<p>Precision or recall?</p>
<p>To avoid false positives, optimize for <strong>precision</strong> (higher predicition probability threshold), this will mean less predictions will be made overall but they will have a higher likelihood of being correct.</p>
<p>To avoid false negatives, optimize for <strong>recall</strong> (lower prediction probability threshold), more overall predictions will be made, making it more likely that items will not be missed.</p>
<p>Which you choose will depend on your problem space.</p>
<p>If you are in a safety critical space, you might want to optimize for <strong>recall</strong> (less chance of something being missed but more false positives).</p>
<p>If user experience matters most, for example, in a consumer app like Trashify, optimize for <strong>recall</strong>, users often find deleting wrong results preferable to adding missed items.</p>
<p>If a false positive predicition is costly, optimize for <strong>precision</strong>.</p>
</div>
</div>
</section>
<section id="evaluating-and-visualizing-predictions-one-by-one" class="level3" data-number="15.3">
<h3 data-number="15.3" class="anchored" data-anchor-id="evaluating-and-visualizing-predictions-one-by-one"><span class="header-section-number">15.3</span> Evaluating and visualizing predictions one by one</h3>
<p>We’ve seen how our model performs on the test dataset in metric form but nothing quite compares to visualizing actual predictions.</p>
<p>To do so, we’ll extract a random sample from <code>processed_dataset["test"]</code>, pass it to our model, post process the outputs and then plot the predicted boxes on an actual image.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>If your predictions aren’t the exact same as below, this is because of the randomness of machine learning, what’s important is that the direction is similar. For example, do your loss curves go down and evaluation metrics trend up?</p>
<p>Ideally, your predictions will be not too dissimiliar.</p>
</div>
</div>
<div id="cell-239" class="cell" data-execution_count="100">
<div class="sourceCode cell-code" id="cb190"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb190-1"><a href="#cb190-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb190-2"><a href="#cb190-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-3"><a href="#cb190-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a random sample from the test preds</span></span>
<span id="cb190-4"><a href="#cb190-4" aria-hidden="true" tabindex="-1"></a>random_test_pred_index <span class="op">=</span> random.randint(<span class="dv">0</span>, <span class="bu">len</span>(processed_dataset[<span class="st">"test"</span>]))</span>
<span id="cb190-5"><a href="#cb190-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Making predictions on test item with index: </span><span class="sc">{</span>random_test_pred_index<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb190-6"><a href="#cb190-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-7"><a href="#cb190-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a random sample from the processed dataset</span></span>
<span id="cb190-8"><a href="#cb190-8" aria-hidden="true" tabindex="-1"></a>random_test_sample <span class="op">=</span> processed_dataset[<span class="st">"test"</span>][random_test_pred_index]</span>
<span id="cb190-9"><a href="#cb190-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-10"><a href="#cb190-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Do a single forward pass with the model (we'll time how long it takes for fun)</span></span>
<span id="cb190-11"><a href="#cb190-11" aria-hidden="true" tabindex="-1"></a>start_pred_time <span class="op">=</span> time.time()</span>
<span id="cb190-12"><a href="#cb190-12" aria-hidden="true" tabindex="-1"></a>random_test_sample_outputs <span class="op">=</span> model(pixel_values<span class="op">=</span>random_test_sample[<span class="st">"pixel_values"</span>].unsqueeze(<span class="dv">0</span>).to(<span class="st">"cuda"</span>), <span class="co"># model expects input [batch_size, color_channels, height, width]</span></span>
<span id="cb190-13"><a href="#cb190-13" aria-hidden="true" tabindex="-1"></a>                                   pixel_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb190-14"><a href="#cb190-14" aria-hidden="true" tabindex="-1"></a>end_pred_time <span class="op">=</span> time.time()</span>
<span id="cb190-15"><a href="#cb190-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Total time to perform prediction: </span><span class="sc">{</span><span class="bu">round</span>(end_pred_time <span class="op">-</span> start_pred_time, <span class="dv">3</span>)<span class="sc">}</span><span class="ss"> seconds."</span>)</span>
<span id="cb190-16"><a href="#cb190-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-17"><a href="#cb190-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Post process a random item from test preds</span></span>
<span id="cb190-18"><a href="#cb190-18" aria-hidden="true" tabindex="-1"></a>random_test_sample_outputs_post_processed <span class="op">=</span> image_processor.post_process_object_detection(</span>
<span id="cb190-19"><a href="#cb190-19" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>random_test_sample_outputs,</span>
<span id="cb190-20"><a href="#cb190-20" aria-hidden="true" tabindex="-1"></a>    threshold<span class="op">=</span><span class="fl">0.35</span>, <span class="co"># prediction probability threshold for boxes (note: boxes from an untrained model will likely be bad)</span></span>
<span id="cb190-21"><a href="#cb190-21" aria-hidden="true" tabindex="-1"></a>    target_sizes<span class="op">=</span>random_test_sample[<span class="st">"labels"</span>][<span class="st">"orig_size"</span>].unsqueeze(<span class="dv">0</span>) <span class="co"># original input image size (or whichever target size you'd like), required to be same number of input items in a list</span></span>
<span id="cb190-22"><a href="#cb190-22" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb190-23"><a href="#cb190-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-24"><a href="#cb190-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract scores, labels and boxes</span></span>
<span id="cb190-25"><a href="#cb190-25" aria-hidden="true" tabindex="-1"></a>random_test_sample_pred_scores <span class="op">=</span> random_test_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"scores"</span>]</span>
<span id="cb190-26"><a href="#cb190-26" aria-hidden="true" tabindex="-1"></a>random_test_sample_pred_labels <span class="op">=</span> random_test_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"labels"</span>]</span>
<span id="cb190-27"><a href="#cb190-27" aria-hidden="true" tabindex="-1"></a>random_test_sample_pred_boxes <span class="op">=</span> half_boxes(random_test_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"boxes"</span>])</span>
<span id="cb190-28"><a href="#cb190-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-29"><a href="#cb190-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list of labels and colours to plot on the boxes </span></span>
<span id="cb190-30"><a href="#cb190-30" aria-hidden="true" tabindex="-1"></a>random_test_sample_pred_to_score_tuples <span class="op">=</span> [(id2label[label_pred.item()], <span class="bu">round</span>(score_pred.item(), <span class="dv">4</span>)) </span>
<span id="cb190-31"><a href="#cb190-31" aria-hidden="true" tabindex="-1"></a>                                           <span class="cf">for</span> label_pred, score_pred <span class="kw">in</span> <span class="bu">zip</span>(random_test_sample_pred_labels, random_test_sample_pred_scores)]</span>
<span id="cb190-32"><a href="#cb190-32" aria-hidden="true" tabindex="-1"></a>random_test_sample_labels_to_plot <span class="op">=</span> [<span class="ss">f"Pred: </span><span class="sc">{</span>item[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>item[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">)"</span> <span class="cf">for</span> item <span class="kw">in</span> random_test_sample_pred_to_score_tuples]</span>
<span id="cb190-33"><a href="#cb190-33" aria-hidden="true" tabindex="-1"></a>random_test_sample_colours_to_plot <span class="op">=</span> [colour_palette[item[<span class="dv">0</span>]] <span class="cf">for</span> item <span class="kw">in</span> random_test_sample_pred_to_score_tuples]</span>
<span id="cb190-34"><a href="#cb190-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-35"><a href="#cb190-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Labels with scores:"</span>)</span>
<span id="cb190-36"><a href="#cb190-36" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label <span class="kw">in</span> random_test_sample_labels_to_plot:</span>
<span id="cb190-37"><a href="#cb190-37" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(label)</span>
<span id="cb190-38"><a href="#cb190-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-39"><a href="#cb190-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the predicted boxes on the random test image </span></span>
<span id="cb190-40"><a href="#cb190-40" aria-hidden="true" tabindex="-1"></a>test_pred_box_image <span class="op">=</span> to_pil_image(</span>
<span id="cb190-41"><a href="#cb190-41" aria-hidden="true" tabindex="-1"></a>    pic<span class="op">=</span>draw_bounding_boxes(</span>
<span id="cb190-42"><a href="#cb190-42" aria-hidden="true" tabindex="-1"></a>        image<span class="op">=</span>pil_to_tensor(pic<span class="op">=</span>half_image(dataset[<span class="st">"test"</span>][random_test_pred_index][<span class="st">"image"</span>])),</span>
<span id="cb190-43"><a href="#cb190-43" aria-hidden="true" tabindex="-1"></a>        boxes<span class="op">=</span>random_test_sample_pred_boxes,</span>
<span id="cb190-44"><a href="#cb190-44" aria-hidden="true" tabindex="-1"></a>        colors<span class="op">=</span>random_test_sample_colours_to_plot,</span>
<span id="cb190-45"><a href="#cb190-45" aria-hidden="true" tabindex="-1"></a>        labels<span class="op">=</span>random_test_sample_labels_to_plot,</span>
<span id="cb190-46"><a href="#cb190-46" aria-hidden="true" tabindex="-1"></a>        width<span class="op">=</span><span class="dv">3</span></span>
<span id="cb190-47"><a href="#cb190-47" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb190-48"><a href="#cb190-48" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb190-49"><a href="#cb190-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-50"><a href="#cb190-50" aria-hidden="true" tabindex="-1"></a>test_pred_box_image</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Making predictions on test item with index: 163
[INFO] Total time to perform prediction: 0.07 seconds.
[INFO] Labels with scores:
Pred: hand (0.7147)
Pred: bin (0.555)
Pred: trash (0.5036)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="100">
<div>
<figure class="figure">
<p><img src="hugging_face_object_detection_tutorial_files/figure-html/cell-99-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Nice!</p>
<p>These prediction boxes look far better than our randomly predicted boxes with an untrained model.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you have two predictions appearing for the same class on the image (e.g.&nbsp;two boxes around the <code>hand</code>), this can be remedied by post processing the predicted boxes with a technique called NMS (Non-maximum Suppression).</p>
<p>NMS can help to only keep the highest scoring box per class (the one with the <em>maximum</em> prediction probability).</p>
<p>This would mean that if there are two (or more) boxes predicted for the <code>hand</code> class, only the one with the highest prediction probability will remain.</p>
<p>This same filtering technique can be applied to each predicted class.</p>
</div>
</div>
</section>
<section id="comparing-our-models-predicted-boxes-to-the-ground-truth-boxes" class="level3" data-number="15.4">
<h3 data-number="15.4" class="anchored" data-anchor-id="comparing-our-models-predicted-boxes-to-the-ground-truth-boxes"><span class="header-section-number">15.4</span> Comparing our model’s predicted boxes to the ground truth boxes</h3>
<p>How about we compare our model’s predicted boxes to the ground truth boxes?</p>
<p>To do so, we’ll extract the same test sample from the test dataset, plot the ground truth boxes on it and then create a side by side comparison of truth versus predictions.</p>
<div id="cell-242" class="cell" data-execution_count="101">
<div class="sourceCode cell-code" id="cb192"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb192-1"><a href="#cb192-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get ground truth image</span></span>
<span id="cb192-2"><a href="#cb192-2" aria-hidden="true" tabindex="-1"></a>ground_truth_image <span class="op">=</span> half_image(dataset[<span class="st">"test"</span>][random_test_pred_index][<span class="st">"image"</span>])</span>
<span id="cb192-3"><a href="#cb192-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb192-4"><a href="#cb192-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get ground truth boxes (we'll convert these from CXCYWH -&gt; XYXY to be in the same format as our prediction boxes)</span></span>
<span id="cb192-5"><a href="#cb192-5" aria-hidden="true" tabindex="-1"></a>ground_truth_boxes <span class="op">=</span> [convert_bbox_cxcywh_to_xyxy_absolute(boxes<span class="op">=</span>input_box,</span>
<span id="cb192-6"><a href="#cb192-6" aria-hidden="true" tabindex="-1"></a>                                                           image_size_target<span class="op">=</span>random_test_sample[<span class="st">"labels"</span>][<span class="st">"orig_size"</span>]) <span class="cf">for</span> input_box <span class="kw">in</span> random_test_sample[<span class="st">"labels"</span>][<span class="st">"boxes"</span>]]</span>
<span id="cb192-7"><a href="#cb192-7" aria-hidden="true" tabindex="-1"></a>ground_truth_boxes <span class="op">=</span> torch.stack(half_boxes(ground_truth_boxes))</span>
<span id="cb192-8"><a href="#cb192-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb192-9"><a href="#cb192-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Get ground truth labels and colours</span></span>
<span id="cb192-10"><a href="#cb192-10" aria-hidden="true" tabindex="-1"></a>ground_truth_labels <span class="op">=</span> [id2label[label.item()] <span class="cf">for</span> label <span class="kw">in</span> random_test_sample[<span class="st">"labels"</span>][<span class="st">"class_labels"</span>]]</span>
<span id="cb192-11"><a href="#cb192-11" aria-hidden="true" tabindex="-1"></a>ground_truth_colours <span class="op">=</span> [colour_palette[label] <span class="cf">for</span> label <span class="kw">in</span> ground_truth_labels]</span>
<span id="cb192-12"><a href="#cb192-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb192-13"><a href="#cb192-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create ground truth box plot image</span></span>
<span id="cb192-14"><a href="#cb192-14" aria-hidden="true" tabindex="-1"></a>test_ground_truth_box_image <span class="op">=</span> to_pil_image(</span>
<span id="cb192-15"><a href="#cb192-15" aria-hidden="true" tabindex="-1"></a>    pic<span class="op">=</span>draw_bounding_boxes(</span>
<span id="cb192-16"><a href="#cb192-16" aria-hidden="true" tabindex="-1"></a>        image<span class="op">=</span>pil_to_tensor(pic<span class="op">=</span>ground_truth_image),</span>
<span id="cb192-17"><a href="#cb192-17" aria-hidden="true" tabindex="-1"></a>        boxes<span class="op">=</span>ground_truth_boxes,</span>
<span id="cb192-18"><a href="#cb192-18" aria-hidden="true" tabindex="-1"></a>        colors<span class="op">=</span>ground_truth_colours,</span>
<span id="cb192-19"><a href="#cb192-19" aria-hidden="true" tabindex="-1"></a>        labels<span class="op">=</span>ground_truth_labels,</span>
<span id="cb192-20"><a href="#cb192-20" aria-hidden="true" tabindex="-1"></a>        width<span class="op">=</span><span class="dv">3</span></span>
<span id="cb192-21"><a href="#cb192-21" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb192-22"><a href="#cb192-22" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb192-23"><a href="#cb192-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb192-24"><a href="#cb192-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot ground truth image and boxes to predicted image and boxes</span></span>
<span id="cb192-25"><a href="#cb192-25" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">10</span>))</span>
<span id="cb192-26"><a href="#cb192-26" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].imshow(test_ground_truth_box_image)</span>
<span id="cb192-27"><a href="#cb192-27" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"Ground Truth Image and Boxes"</span>)</span>
<span id="cb192-28"><a href="#cb192-28" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].axis(<span class="va">False</span>)</span>
<span id="cb192-29"><a href="#cb192-29" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].imshow(test_pred_box_image)</span>
<span id="cb192-30"><a href="#cb192-30" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"Predicted Boxes"</span>)</span>
<span id="cb192-31"><a href="#cb192-31" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].axis(<span class="va">False</span>)</span>
<span id="cb192-32"><a href="#cb192-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb192-33"><a href="#cb192-33" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="hugging_face_object_detection_tutorial_files/figure-html/cell-100-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Woah! It looks like our model does fairly well to reproduce boxes that are similar to the ground truth.</p>
<p>There are some slight mistakes such as where our model predicts more than one of the same box in similar areas (this could be filtered later on with NMS or <a href="https://paperswithcode.com/method/non-maximum-suppression">non-maximum suppression</a> which removes all but the highest prediction probability boxes for each class).</p>
</section>
<section id="predict-on-image-from-the-wild" class="level3" data-number="15.5">
<h3 data-number="15.5" class="anchored" data-anchor-id="predict-on-image-from-the-wild"><span class="header-section-number">15.5</span> Predict on image from the wild</h3>
<p>We’ve seen how our model performs on test data which is similar to our training data.</p>
<p>But how does it do on an image from the wild?</p>
<p>For the image below, I searched for “person putting trash in bin” and selected one of the first images to appear.</p>
<p>You can see it at the URL: <a href="https://images.pexels.com/photos/7565384/pexels-photo-7565384.jpeg"><code>https://images.pexels.com/photos/7565384/pexels-photo-7565384.jpeg</code></a>.</p>
<p>If this image doesn’t work, we could even try our model on an AI generated image of a person throwing trash in a bin and see how it performs.</p>
<p>Let’s write some code to download our target image from the URL above and save it to file.</p>
<div id="cell-245" class="cell" data-execution_count="102">
<div class="sourceCode cell-code" id="cb193"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb193-1"><a href="#cb193-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb193-2"><a href="#cb193-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb193-3"><a href="#cb193-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-4"><a href="#cb193-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Example image of person putting trash in bin</span></span>
<span id="cb193-5"><a href="#cb193-5" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://images.pexels.com/photos/7565384/pexels-photo-7565384.jpeg"</span></span>
<span id="cb193-6"><a href="#cb193-6" aria-hidden="true" tabindex="-1"></a>filename <span class="op">=</span> <span class="st">"pexels-photo-7565384.jpeg"</span></span>
<span id="cb193-7"><a href="#cb193-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-8"><a href="#cb193-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Donwload image</span></span>
<span id="cb193-9"><a href="#cb193-9" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> requests.get(url, stream<span class="op">=</span><span class="va">True</span>, timeout<span class="op">=</span><span class="dv">10</span>) <span class="im">as</span> response:</span>
<span id="cb193-10"><a href="#cb193-10" aria-hidden="true" tabindex="-1"></a>    response.raise_for_status() <span class="co"># ensure the download succeeded</span></span>
<span id="cb193-11"><a href="#cb193-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(filename, <span class="st">"wb"</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb193-12"><a href="#cb193-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> chunk <span class="kw">in</span> response.iter_content(chunk_size<span class="op">=</span><span class="dv">8192</span>):</span>
<span id="cb193-13"><a href="#cb193-13" aria-hidden="true" tabindex="-1"></a>            <span class="bu">file</span>.write(chunk)</span>
<span id="cb193-14"><a href="#cb193-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb193-15"><a href="#cb193-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Saved to </span><span class="sc">{</span>filename<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb193-16"><a href="#cb193-16" aria-hidden="true" tabindex="-1"></a>image_from_wild <span class="op">=</span> Image.<span class="bu">open</span>(filename)</span>
<span id="cb193-17"><a href="#cb193-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>): <span class="co"># the default image is quite large so we'll half it three times</span></span>
<span id="cb193-18"><a href="#cb193-18" aria-hidden="true" tabindex="-1"></a>    image_from_wild <span class="op">=</span> half_image(image_from_wild)</span>
<span id="cb193-19"><a href="#cb193-19" aria-hidden="true" tabindex="-1"></a>image_from_wild</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Saved to pexels-photo-7565384.jpeg</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="102">
<div>
<figure class="figure">
<p><img src="hugging_face_object_detection_tutorial_files/figure-html/cell-101-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Nice!</p>
<p>This one looks slightly different to some of the images our model saw during training, so it’ll be interesting to see how it goes.</p>
<p>To make predictions on the downloaded image we’ll go through the following steps:</p>
<ol type="1">
<li>Open the image.</li>
<li>Preprocess the image with <code>image_processor</code>.</li>
<li>Make predictions on the processed image with our <code>model</code>.</li>
<li>Get the original size of the image for doing box post processing conversions.</li>
<li>Set a prediction probability threshold of how confident we’d like our model to be in its predictions.</li>
<li>Post process our model’s predictions.</li>
<li>Extract the post processed labels, scores and box coordinates.</li>
<li>Create a list of labels, scores and colours to plot.</li>
<li>Draw our model’s predicted bounding boxes on the target image with <code>draw_bounding_boxes</code> and <code>to_pil_image</code>.</li>
</ol>
<p>Let’s do it!</p>
<div id="cell-247" class="cell" data-execution_count="103">
<div class="sourceCode cell-code" id="cb195"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb195-1"><a href="#cb195-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pred on image from pathname</span></span>
<span id="cb195-2"><a href="#cb195-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb195-3"><a href="#cb195-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb195-4"><a href="#cb195-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-5"><a href="#cb195-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_image_dimensions_from_pil(image: Image.Image) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb195-6"><a href="#cb195-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb195-7"><a href="#cb195-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Convert the dimensions of a PIL image to a PyTorch tensor in the order (height, width).</span></span>
<span id="cb195-8"><a href="#cb195-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-9"><a href="#cb195-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb195-10"><a href="#cb195-10" aria-hidden="true" tabindex="-1"></a><span class="co">        image (Image.Image): The input PIL image.</span></span>
<span id="cb195-11"><a href="#cb195-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-12"><a href="#cb195-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb195-13"><a href="#cb195-13" aria-hidden="true" tabindex="-1"></a><span class="co">        torch.Tensor: A tensor containing the height and width of the image.</span></span>
<span id="cb195-14"><a href="#cb195-14" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb195-15"><a href="#cb195-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get (width, height) of image (PIL.Image.size returns width, height)</span></span>
<span id="cb195-16"><a href="#cb195-16" aria-hidden="true" tabindex="-1"></a>    width, height <span class="op">=</span> image.size</span>
<span id="cb195-17"><a href="#cb195-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-18"><a href="#cb195-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to a tensor in the order (height, width)</span></span>
<span id="cb195-19"><a href="#cb195-19" aria-hidden="true" tabindex="-1"></a>    image_dimensions_tensor <span class="op">=</span> torch.tensor([height, width])</span>
<span id="cb195-20"><a href="#cb195-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-21"><a href="#cb195-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> image_dimensions_tensor</span>
<span id="cb195-22"><a href="#cb195-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-23"><a href="#cb195-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a test image </span></span>
<span id="cb195-24"><a href="#cb195-24" aria-hidden="true" tabindex="-1"></a>test_image_pil <span class="op">=</span> Image.<span class="bu">open</span>(<span class="st">"pexels-photo-7565384.jpeg"</span>).resize(size<span class="op">=</span>(<span class="dv">640</span>, <span class="dv">640</span>))</span>
<span id="cb195-25"><a href="#cb195-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-26"><a href="#cb195-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocess the image</span></span>
<span id="cb195-27"><a href="#cb195-27" aria-hidden="true" tabindex="-1"></a>test_image_preprocessed <span class="op">=</span> image_processor.preprocess(images<span class="op">=</span>test_image_pil,</span>
<span id="cb195-28"><a href="#cb195-28" aria-hidden="true" tabindex="-1"></a>                                                     return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb195-29"><a href="#cb195-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-30"><a href="#cb195-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions on the preprocessed image</span></span>
<span id="cb195-31"><a href="#cb195-31" aria-hidden="true" tabindex="-1"></a>random_test_sample_outputs <span class="op">=</span> model(pixel_values<span class="op">=</span>test_image_preprocessed[<span class="st">"pixel_values"</span>].to(<span class="st">"cuda"</span>), <span class="co"># model expects input [batch_size, color_channels, height, width]</span></span>
<span id="cb195-32"><a href="#cb195-32" aria-hidden="true" tabindex="-1"></a>                                   pixel_mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb195-33"><a href="#cb195-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-34"><a href="#cb195-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Get image original size</span></span>
<span id="cb195-35"><a href="#cb195-35" aria-hidden="true" tabindex="-1"></a>test_image_size <span class="op">=</span> get_image_dimensions_from_pil(image<span class="op">=</span>test_image_pil)</span>
<span id="cb195-36"><a href="#cb195-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Test image size: </span><span class="sc">{</span>test_image_size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb195-37"><a href="#cb195-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-38"><a href="#cb195-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the threshold, we can adjust this based on how confident we'd like our model to be about its predictions</span></span>
<span id="cb195-39"><a href="#cb195-39" aria-hidden="true" tabindex="-1"></a>THRESHOLD <span class="op">=</span> <span class="fl">0.4</span></span>
<span id="cb195-40"><a href="#cb195-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-41"><a href="#cb195-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Post process the predictions</span></span>
<span id="cb195-42"><a href="#cb195-42" aria-hidden="true" tabindex="-1"></a>random_test_sample_outputs_post_processed <span class="op">=</span> image_processor.post_process_object_detection(</span>
<span id="cb195-43"><a href="#cb195-43" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>random_test_sample_outputs,</span>
<span id="cb195-44"><a href="#cb195-44" aria-hidden="true" tabindex="-1"></a>    threshold<span class="op">=</span>THRESHOLD,</span>
<span id="cb195-45"><a href="#cb195-45" aria-hidden="true" tabindex="-1"></a>    target_sizes<span class="op">=</span>test_image_size.unsqueeze(<span class="dv">0</span>) <span class="co"># needs to be same length as batch dimension of the logits (e.g. [[height, width]])</span></span>
<span id="cb195-46"><a href="#cb195-46" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb195-47"><a href="#cb195-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-48"><a href="#cb195-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract scores, labels and boxes</span></span>
<span id="cb195-49"><a href="#cb195-49" aria-hidden="true" tabindex="-1"></a>random_test_sample_pred_scores <span class="op">=</span> random_test_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"scores"</span>]</span>
<span id="cb195-50"><a href="#cb195-50" aria-hidden="true" tabindex="-1"></a>random_test_sample_pred_labels <span class="op">=</span> random_test_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"labels"</span>]</span>
<span id="cb195-51"><a href="#cb195-51" aria-hidden="true" tabindex="-1"></a>random_test_sample_pred_boxes <span class="op">=</span> random_test_sample_outputs_post_processed[<span class="dv">0</span>][<span class="st">"boxes"</span>]</span>
<span id="cb195-52"><a href="#cb195-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-53"><a href="#cb195-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list of labels to plot on the boxes </span></span>
<span id="cb195-54"><a href="#cb195-54" aria-hidden="true" tabindex="-1"></a>random_test_sample_labels_to_plot <span class="op">=</span> [<span class="ss">f"Pred: </span><span class="sc">{</span>id2label[label_pred.item()]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="bu">round</span>(score_pred.item(), <span class="dv">4</span>)<span class="sc">}</span><span class="ss">)"</span></span>
<span id="cb195-55"><a href="#cb195-55" aria-hidden="true" tabindex="-1"></a>                                     <span class="cf">for</span> label_pred, score_pred <span class="kw">in</span> <span class="bu">zip</span>(random_test_sample_pred_labels, random_test_sample_pred_scores)]</span>
<span id="cb195-56"><a href="#cb195-56" aria-hidden="true" tabindex="-1"></a>random_test_sample_colours_to_plot <span class="op">=</span> [colour_palette[id2label[label_pred.item()]] <span class="cf">for</span> label_pred <span class="kw">in</span> random_test_sample_pred_labels]</span>
<span id="cb195-57"><a href="#cb195-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-58"><a href="#cb195-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"[INFO] Labels with scores:"</span>)</span>
<span id="cb195-59"><a href="#cb195-59" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> item <span class="kw">in</span> random_test_sample_labels_to_plot:</span>
<span id="cb195-60"><a href="#cb195-60" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(item)</span>
<span id="cb195-61"><a href="#cb195-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-62"><a href="#cb195-62" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the predicted boxes on the random test image </span></span>
<span id="cb195-63"><a href="#cb195-63" aria-hidden="true" tabindex="-1"></a>to_pil_image(</span>
<span id="cb195-64"><a href="#cb195-64" aria-hidden="true" tabindex="-1"></a>    pic<span class="op">=</span>draw_bounding_boxes(</span>
<span id="cb195-65"><a href="#cb195-65" aria-hidden="true" tabindex="-1"></a>        image<span class="op">=</span>pil_to_tensor(pic<span class="op">=</span>test_image_pil),</span>
<span id="cb195-66"><a href="#cb195-66" aria-hidden="true" tabindex="-1"></a>        colors<span class="op">=</span>random_test_sample_colours_to_plot,                     </span>
<span id="cb195-67"><a href="#cb195-67" aria-hidden="true" tabindex="-1"></a>        boxes<span class="op">=</span>random_test_sample_pred_boxes,</span>
<span id="cb195-68"><a href="#cb195-68" aria-hidden="true" tabindex="-1"></a>        labels<span class="op">=</span>random_test_sample_labels_to_plot,</span>
<span id="cb195-69"><a href="#cb195-69" aria-hidden="true" tabindex="-1"></a>        width<span class="op">=</span><span class="dv">3</span></span>
<span id="cb195-70"><a href="#cb195-70" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb195-71"><a href="#cb195-71" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Test image size: tensor([640, 640])
[INFO] Labels with scores:
Pred: trash (0.7413)
Pred: trash (0.5808)
Pred: bin (0.4705)
Pred: trash (0.4051)
Pred: trash (0.4042)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="103">
<div>
<figure class="figure">
<p><img src="hugging_face_object_detection_tutorial_files/figure-html/cell-102-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>How did the model do?</p>
<p>It’s good to test on random images which may be in our domain (e.g.&nbsp;random photos or AI generated photos of someone putting trash in a bin), this way we can test to see if there are any conditions where our model fails.</p>
<p>In the example above, I noticed the model often fails to detect the hand.</p>
<p>This is likely because many of our training images are from first person point of views rather than third person point of views.</p>
<p>To fix this, we could incorportate more diverse training data into our pipeline.</p>
</section>
</section>
<section id="uploading-our-trained-model-to-hugging-face-hub" class="level2" data-number="16">
<h2 data-number="16" class="anchored" data-anchor-id="uploading-our-trained-model-to-hugging-face-hub"><span class="header-section-number">16</span> Uploading our trained model to Hugging Face Hub</h2>
<p>Since our model looks like it’s working quite well, how about we upload to the Hugging Face Hub to make it accessible to others?</p>
<p>We’ll first start by creating a path to save the model to locally.</p>
<p>Then we’ll save the model to file using the <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.save_model"><code>transformers.Trainer.save_model</code></a> method.</p>
<div id="cell-250" class="cell" data-execution_count="104">
<div class="sourceCode cell-code" id="cb197"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb197-1"><a href="#cb197-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the model</span></span>
<span id="cb197-2"><a href="#cb197-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datetime <span class="im">import</span> datetime <span class="co"># optional: add a date of when we trained our model</span></span>
<span id="cb197-3"><a href="#cb197-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb197-4"><a href="#cb197-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get details to add to model's save path</span></span>
<span id="cb197-5"><a href="#cb197-5" aria-hidden="true" tabindex="-1"></a>training_epochs_ <span class="op">=</span> training_args.num_train_epochs</span>
<span id="cb197-6"><a href="#cb197-6" aria-hidden="true" tabindex="-1"></a>learning_rate_ <span class="op">=</span> <span class="st">"</span><span class="sc">{:.0e}</span><span class="st">"</span>.<span class="bu">format</span>(training_args.learning_rate)</span>
<span id="cb197-7"><a href="#cb197-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb197-8"><a href="#cb197-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create model save path with some training details</span></span>
<span id="cb197-9"><a href="#cb197-9" aria-hidden="true" tabindex="-1"></a>model_save_path <span class="op">=</span> <span class="ss">f"models/learn_hf_rt_detrv2_finetuned_trashify_box_dataset_only_manual_data_no_aug_</span><span class="sc">{</span>training_epochs_<span class="sc">}</span><span class="ss">_epochs_lr_</span><span class="sc">{</span>learning_rate_<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb197-10"><a href="#cb197-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb197-11"><a href="#cb197-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Save model to file</span></span>
<span id="cb197-12"><a href="#cb197-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Saving model to: </span><span class="sc">{</span>model_save_path<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb197-13"><a href="#cb197-13" aria-hidden="true" tabindex="-1"></a>model_v1_trainer.save_model(model_save_path)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Saving model to: models/learn_hf_rt_detrv2_finetuned_trashify_box_dataset_only_manual_data_no_aug_10_epochs_lr_1e-04</code></pre>
</div>
</div>
<p>Now let’s make sure we add our model’s <code>image_processor</code> to the our <code>Trainer</code> instance, so when someone loads our model, it automatically knows how to preprocess an input sample.</p>
<p>This is usually done automatically but I’ve run into some issues in the past where the model doesn’t load the preprocessor.</p>
<p>To do this we can see the <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.processing_class"><code>processing_class</code></a> attribute of <code>model_v1_trainer</code> to be our <code>image_processor</code>.</p>
<div id="cell-252" class="cell" data-execution_count="105">
<div class="sourceCode cell-code" id="cb199"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb199-1"><a href="#cb199-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make sure trainer has the processor class (this can sometimes be automatically assigned, however, we'll hard code it just to be safe)</span></span>
<span id="cb199-2"><a href="#cb199-2" aria-hidden="true" tabindex="-1"></a>model_v1_trainer.processing_class <span class="op">=</span> image_processor</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Nice!</p>
<p>Now let’s push our <code>model_v1_trainer</code> to the Hugging Face Hub using <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.push_to_hub"><code>transformers.Trainer.push_to_hub</code></a> (this will push our trained model and processing class to the Hugging Face Hub).</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Whenever you try to push something to the Hugging Face Hub, make sure you’ve got your Hugging Face account and token credentials setup correctly.</p>
<p>See the <a href="https://www.learnhuggingface.com/extras/setup">Hugging Face setup guide</a> for a walkthrough of how to do this.</p>
</div>
</div>
<div id="cell-254" class="cell" data-execution_count="106">
<div class="sourceCode cell-code" id="cb200"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb200-1"><a href="#cb200-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Push the model to the hub</span></span>
<span id="cb200-2"><a href="#cb200-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: this will require you to have your Hugging Face account setup </span></span>
<span id="cb200-3"><a href="#cb200-3" aria-hidden="true" tabindex="-1"></a>model_on_hub_url <span class="op">=</span> model_v1_trainer.push_to_hub(commit_message<span class="op">=</span><span class="st">"upload fine-tuned RT-DETRv2 trashify object detection model"</span>,</span>
<span id="cb200-4"><a href="#cb200-4" aria-hidden="true" tabindex="-1"></a>                                                <span class="co"># token=None # Optional to add a token manually</span></span>
<span id="cb200-5"><a href="#cb200-5" aria-hidden="true" tabindex="-1"></a>                                                )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"762c87ecedd747e381411795bddaef76","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"a8931afc26ef4bed91c6cc005d5ca8ee","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"4e3b8b9e31cd447aac84b168582431c1","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<p>Perfect! Our model has been uploaded to the Hugging Face Hub.</p>
<p>If no changes have been made to a previously uploaded model file, you might see a message like the following:</p>
<blockquote class="blockquote">
<p>No files have been modified since last commit. Skipping to prevent empty commit.</p>
</blockquote>
<p>Otherwise, we can check the commit URL of our model using the <code>commit_url</code> attribute.</p>
<div id="cell-256" class="cell" data-execution_count="107">
<div class="sourceCode cell-code" id="cb201"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb201-1"><a href="#cb201-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Our model has been uploaded with the following commit URL: </span><span class="sc">{</span>model_on_hub_url<span class="sc">.</span>commit_url<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Our model has been uploaded with the following commit URL: https://huggingface.co/mrdbourke/rt_detrv2_finetuned_trashify_box_detector_v1/commit/46003b6b8f8e9855a0d8979ba5cdb1b8ca437646</code></pre>
</div>
</div>
</section>
<section id="creating-a-demo-of-our-model-with-gradio" class="level2" data-number="17">
<h2 data-number="17" class="anchored" data-anchor-id="creating-a-demo-of-our-model-with-gradio"><span class="header-section-number">17</span> Creating a demo of our model with Gradio</h2>
<p>One of the best ways to share your machine learning work is by creating a demo application.</p>
<p>And one of the best places to share your applications is <a href="https://huggingface.co/docs/hub/spaces">Hugging Face Spaces</a>.</p>
<p>Hugging Face Spaces allows you to host machine learning (and non-machine learning) applications for free (with <a href="https://huggingface.co/docs/hub/spaces-overview#hardware-resources">optional paid hardware upgrades</a>).</p>
<p>If you’re familiar with GitHub, Hugging Face Spaces works similar to a GitHub repository (each Space is a Git repository itself).</p>
<p>If not, that’s okay, think of Hugging Face Spaces as an online folder where you can upload your files and have them accessed by others.</p>
<p>Creating a Hugging Face Space can be done in two main ways:</p>
<ol type="1">
<li><strong>Manually</strong> - By going to the <a href="https://huggingface.co/spaces">Hugging Face Spaces</a> website and clicking “Create new space”. Or by going directly to <a href="https://huggingface.co/new-space">https://www.huggingface.co/new-space</a>. Here, you’ll be able to setup a few settings for your Space and choose the framework/runtime (e.g.&nbsp;Streamlit, Gradio, Docker and more).</li>
<li><strong>Programmatically</strong> - By using the <a href="https://huggingface.co/docs/huggingface_hub/package_reference/hf_api">Hugging Face Hub Python API</a> we can write code to <a href="https://www.gradio.app/guides/using-hugging-face-integrations#hosting-your-gradio-demos-on-spaces">directly upload files to the Hugging Face Hub</a>, including Hugging Face Spaces.</li>
</ol>
<p>Both are great options but we’re going to take the second approach.</p>
<p>This is so we can create our Hugging Face Space right from this notebook.</p>
<p>To do so, we’ll create three files and a folder:</p>
<ol type="1">
<li><code>app.py</code> (main file that Hugging Face Spaces looks for) - This will be the Python file which will be the main running file on our Hugging Face Space. Inside we’ll include all the code necessary to run our Gradio demo (as above). Hugging Face Spaces will automatically recoginize the <code>app.py</code> file and run it for us.</li>
<li><code>requirements.txt</code> - This text file will include all of the Python packages we need to run our <code>app.py</code> file. Before our Space starts to run, all of the packages in this file will be installed.</li>
<li><code>README.md</code> - This markdown file will include details about our Space as well as specific Space-related metadata (we’ll see this later on).</li>
<li><code>trashify_examples/</code> - This folder will contain several images that our Trashify demo will showcase as examples in the demo.</li>
</ol>
<p>We’ll create these files with the following file structure:</p>
<pre><code>demos/
└── trashify_object_detector/
    ├── app.py
    ├── README.md
    ├── requirements.txt
    └── trashify_examples/
        ├── trashify_example_1.jpeg
        ├── trashify_example_2.jpeg
        └── trashify_example_3.jpeg</code></pre>
<p>Why this way?</p>
<p>Doing it in the above style means we’ll have a directory which contains all of our demos (<code>demos/</code>) as well as a dedicated directory which contains our Trashify demo application (<code>trashify_object_detector/</code>).</p>
<p>This way, we’ll be able to upload the whole <code>demos/trashify_object_detector/</code> folder to Hugging Face Spaces.</p>
<p>Let’s start by making a directory to store our demo application files.</p>
<div id="cell-258" class="cell" data-execution_count="108">
<div class="sourceCode cell-code" id="cb204"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb204-1"><a href="#cb204-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup path to trashify demo folder (we'll store all of our demo requirements in here)</span></span>
<span id="cb204-2"><a href="#cb204-2" aria-hidden="true" tabindex="-1"></a>demo_path <span class="op">=</span> Path(<span class="st">"../demos/trashify_object_detector"</span>)</span>
<span id="cb204-3"><a href="#cb204-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-4"><a href="#cb204-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the directory</span></span>
<span id="cb204-5"><a href="#cb204-5" aria-hidden="true" tabindex="-1"></a>demo_path.mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="making-an-app-file" class="level3" data-number="17.1">
<h3 data-number="17.1" class="anchored" data-anchor-id="making-an-app-file"><span class="header-section-number">17.1</span> Making an app file</h3>
<p>UPTOHERE - write the steps required for the app.py file</p>
<p>Our <code>app.py</code> file will be the main part of our Hugging Face Space.</p>
<p>Inside the <code>app.py</code> file we’ll:</p>
<ol type="1">
<li>Import the required libraries/packages for running our demo app.</li>
<li>Setup preprocessing and helper functions for our trained ojbect detection model. Because our model is already hosted on the Hugging Face Hub, we can load it directly with <a href="https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForObjectDetection"><code>transformers.AutoModelForObjectDetection.from_pretrained</code></a> and passing it our model’s name (e.g.&nbsp;<code>mrdbourke/rt_detrv2_finetuned_trashify_box_detector_v1</code>) and when we upload our <code>app.py</code> file to Hugging Face Spaces, it will load the model directly from the Hub.
<ul>
<li><strong>Note:</strong> Be sure to change “<code>mrdbourke</code>” to your own Hugging Face username.</li>
</ul></li>
<li>Create a function <code>predict_on_image</code> to:
<ul>
<li>Take in an image and confidence threshold.</li>
<li>Predict on the image with our model.</li>
<li>Post process the predictions.</li>
<li>Draw the predictions on the target image (see step 4).</li>
<li>Return the target image with drawn predictions as well as a text label output as to whether trash, bin and hand were detected (see step 4).</li>
</ul></li>
<li>We’ll draw the model’s predicted boxes (if there are any) on the image with <a href="https://pillow.readthedocs.io/en/stable/reference/ImageDraw.html"><code>PIL.ImageDraw</code></a>.</li>
<li>Write some logic to detect whether trash, bin and hand objects are detected as this is the overall goal of Trashify, so if all three are present, we’ll output a message saying +1! for the person picking up trash.</li>
<li>We’ll create a demo using Gradio’s <a href="https://www.gradio.app/docs/gradio/interface"><code>gr.Interface</code></a> class. This will take an image and float as <code>inputs</code> as well as an image and string as <code>outputs</code>. We can add descriptions and other information to our demo so they are visible in the live app. To finish off, we’ll launch the demo with <a href="https://www.gradio.app/docs/gradio/interface#interface-launch"><code>gr.Interface.launch</code></a>.</li>
</ol>
<p>We can write all of the above in a notebook cell.</p>
<p>And we can turn it into a file by using the <a href="https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-writefile"><code>%%writefile</code></a> magic command and passing it our target filepath.</p>
<p>Let’s do it!</p>
<div id="cell-260" class="cell" data-execution_count="117">
<div class="sourceCode cell-code" id="cb205"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb205-1"><a href="#cb205-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile ..<span class="op">/</span>demos<span class="op">/</span>trashify_object_detector<span class="op">/</span>app.py</span>
<span id="cb205-2"><a href="#cb205-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-3"><a href="#cb205-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Import the required libraries and packages</span></span>
<span id="cb205-4"><a href="#cb205-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gradio <span class="im">as</span> gr</span>
<span id="cb205-5"><a href="#cb205-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb205-6"><a href="#cb205-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image, ImageDraw, ImageFont <span class="co"># could also use torch utilities for drawing</span></span>
<span id="cb205-7"><a href="#cb205-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-8"><a href="#cb205-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoImageProcessor</span>
<span id="cb205-9"><a href="#cb205-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForObjectDetection</span>
<span id="cb205-10"><a href="#cb205-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-11"><a href="#cb205-11" aria-hidden="true" tabindex="-1"></a><span class="co">### 2. Setup preprocessing and helper functions </span><span class="al">###</span></span>
<span id="cb205-12"><a href="#cb205-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-13"><a href="#cb205-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup target model path to load</span></span>
<span id="cb205-14"><a href="#cb205-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: Can load from Hugging Face or can load from local </span></span>
<span id="cb205-15"><a href="#cb205-15" aria-hidden="true" tabindex="-1"></a>model_save_path <span class="op">=</span> <span class="st">"mrdbourke/rt_detrv2_finetuned_trashify_box_detector_v1"</span></span>
<span id="cb205-16"><a href="#cb205-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-17"><a href="#cb205-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the model and preprocessor</span></span>
<span id="cb205-18"><a href="#cb205-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Because this app.py file is running directly on Hugging Face Spaces, the model will be loaded from the Hugging Face Hub</span></span>
<span id="cb205-19"><a href="#cb205-19" aria-hidden="true" tabindex="-1"></a>image_processor <span class="op">=</span> AutoImageProcessor.from_pretrained(model_save_path)</span>
<span id="cb205-20"><a href="#cb205-20" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForObjectDetection.from_pretrained(model_save_path)</span>
<span id="cb205-21"><a href="#cb205-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-22"><a href="#cb205-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the target device (use CUDA/GPU if it is available)</span></span>
<span id="cb205-23"><a href="#cb205-23" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb205-24"><a href="#cb205-24" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.to(device)</span>
<span id="cb205-25"><a href="#cb205-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-26"><a href="#cb205-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the id2label dictionary from the model</span></span>
<span id="cb205-27"><a href="#cb205-27" aria-hidden="true" tabindex="-1"></a>id2label <span class="op">=</span> model.config.id2label</span>
<span id="cb205-28"><a href="#cb205-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-29"><a href="#cb205-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up a colour dictionary for plotting boxes with different colours</span></span>
<span id="cb205-30"><a href="#cb205-30" aria-hidden="true" tabindex="-1"></a>color_dict <span class="op">=</span> {   </span>
<span id="cb205-31"><a href="#cb205-31" aria-hidden="true" tabindex="-1"></a>    <span class="st">"bin"</span>: <span class="st">"green"</span>,</span>
<span id="cb205-32"><a href="#cb205-32" aria-hidden="true" tabindex="-1"></a>    <span class="st">"trash"</span>: <span class="st">"blue"</span>,</span>
<span id="cb205-33"><a href="#cb205-33" aria-hidden="true" tabindex="-1"></a>    <span class="st">"hand"</span>: <span class="st">"purple"</span>,</span>
<span id="cb205-34"><a href="#cb205-34" aria-hidden="true" tabindex="-1"></a>    <span class="st">"trash_arm"</span>: <span class="st">"yellow"</span>,</span>
<span id="cb205-35"><a href="#cb205-35" aria-hidden="true" tabindex="-1"></a>    <span class="st">"not_trash"</span>: <span class="st">"red"</span>,</span>
<span id="cb205-36"><a href="#cb205-36" aria-hidden="true" tabindex="-1"></a>    <span class="st">"not_bin"</span>: <span class="st">"red"</span>,</span>
<span id="cb205-37"><a href="#cb205-37" aria-hidden="true" tabindex="-1"></a>    <span class="st">"not_hand"</span>: <span class="st">"red"</span>,</span>
<span id="cb205-38"><a href="#cb205-38" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb205-39"><a href="#cb205-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-40"><a href="#cb205-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Create helper functions for seeing if items from one list are in another </span></span>
<span id="cb205-41"><a href="#cb205-41" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> any_in_list(list_a, list_b):</span>
<span id="cb205-42"><a href="#cb205-42" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Returns True if *any* item from list_a is in list_b, otherwise False."</span></span>
<span id="cb205-43"><a href="#cb205-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">any</span>(item <span class="kw">in</span> list_b <span class="cf">for</span> item <span class="kw">in</span> list_a)</span>
<span id="cb205-44"><a href="#cb205-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-45"><a href="#cb205-45" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> all_in_list(list_a, list_b):</span>
<span id="cb205-46"><a href="#cb205-46" aria-hidden="true" tabindex="-1"></a>    <span class="co">"Returns True if *all* items from list_a are in list_b, otherwise False."</span></span>
<span id="cb205-47"><a href="#cb205-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">all</span>(item <span class="kw">in</span> list_b <span class="cf">for</span> item <span class="kw">in</span> list_a)</span>
<span id="cb205-48"><a href="#cb205-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-49"><a href="#cb205-49" aria-hidden="true" tabindex="-1"></a><span class="co">### 3. Create function to predict on a given image with a given confidence threshold </span><span class="al">###</span></span>
<span id="cb205-50"><a href="#cb205-50" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict_on_image(image, conf_threshold):</span>
<span id="cb205-51"><a href="#cb205-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make sure model is in eval mode</span></span>
<span id="cb205-52"><a href="#cb205-52" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb205-53"><a href="#cb205-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-54"><a href="#cb205-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make a prediction on target image </span></span>
<span id="cb205-55"><a href="#cb205-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb205-56"><a href="#cb205-56" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> image_processor(images<span class="op">=</span>[image], return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb205-57"><a href="#cb205-57" aria-hidden="true" tabindex="-1"></a>        model_outputs <span class="op">=</span> model(<span class="op">**</span>inputs.to(device))</span>
<span id="cb205-58"><a href="#cb205-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-59"><a href="#cb205-59" aria-hidden="true" tabindex="-1"></a>        target_sizes <span class="op">=</span> torch.tensor([[image.size[<span class="dv">1</span>], image.size[<span class="dv">0</span>]]]) <span class="co"># -&gt; [batch_size, height, width] </span></span>
<span id="cb205-60"><a href="#cb205-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb205-61"><a href="#cb205-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Post process the raw outputs from the model </span></span>
<span id="cb205-62"><a href="#cb205-62" aria-hidden="true" tabindex="-1"></a>        results <span class="op">=</span> image_processor.post_process_object_detection(model_outputs,</span>
<span id="cb205-63"><a href="#cb205-63" aria-hidden="true" tabindex="-1"></a>                                                                threshold<span class="op">=</span>conf_threshold,</span>
<span id="cb205-64"><a href="#cb205-64" aria-hidden="true" tabindex="-1"></a>                                                                target_sizes<span class="op">=</span>target_sizes)[<span class="dv">0</span>]</span>
<span id="cb205-65"><a href="#cb205-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-66"><a href="#cb205-66" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return all items in results to CPU (we'll want this for displaying outputs with matplotlib)</span></span>
<span id="cb205-67"><a href="#cb205-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> key, value <span class="kw">in</span> results.items():</span>
<span id="cb205-68"><a href="#cb205-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb205-69"><a href="#cb205-69" aria-hidden="true" tabindex="-1"></a>            results[key] <span class="op">=</span> value.item().cpu() <span class="co"># can't get scalar as .item() so add try/except block</span></span>
<span id="cb205-70"><a href="#cb205-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span>:</span>
<span id="cb205-71"><a href="#cb205-71" aria-hidden="true" tabindex="-1"></a>            results[key] <span class="op">=</span> value.cpu()</span>
<span id="cb205-72"><a href="#cb205-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-73"><a href="#cb205-73" aria-hidden="true" tabindex="-1"></a>    <span class="co">### 4. Draw the predictions on the target image </span><span class="al">###</span></span>
<span id="cb205-74"><a href="#cb205-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-75"><a href="#cb205-75" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Can return results as plotted on a PIL image (then display the image)</span></span>
<span id="cb205-76"><a href="#cb205-76" aria-hidden="true" tabindex="-1"></a>    draw <span class="op">=</span> ImageDraw.Draw(image)</span>
<span id="cb205-77"><a href="#cb205-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-78"><a href="#cb205-78" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get a font from ImageFont</span></span>
<span id="cb205-79"><a href="#cb205-79" aria-hidden="true" tabindex="-1"></a>    font <span class="op">=</span> ImageFont.load_default(size<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb205-80"><a href="#cb205-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-81"><a href="#cb205-81" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get class names as text for print out</span></span>
<span id="cb205-82"><a href="#cb205-82" aria-hidden="true" tabindex="-1"></a>    class_name_text_labels <span class="op">=</span> []</span>
<span id="cb205-83"><a href="#cb205-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-84"><a href="#cb205-84" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iterate through the predictions of the model and draw them on the target image</span></span>
<span id="cb205-85"><a href="#cb205-85" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> box, score, label <span class="kw">in</span> <span class="bu">zip</span>(results[<span class="st">"boxes"</span>], results[<span class="st">"scores"</span>], results[<span class="st">"labels"</span>]):</span>
<span id="cb205-86"><a href="#cb205-86" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create coordinates</span></span>
<span id="cb205-87"><a href="#cb205-87" aria-hidden="true" tabindex="-1"></a>        x, y, x2, y2 <span class="op">=</span> <span class="bu">tuple</span>(box.tolist())</span>
<span id="cb205-88"><a href="#cb205-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-89"><a href="#cb205-89" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get label_name</span></span>
<span id="cb205-90"><a href="#cb205-90" aria-hidden="true" tabindex="-1"></a>        label_name <span class="op">=</span> id2label[label.item()]</span>
<span id="cb205-91"><a href="#cb205-91" aria-hidden="true" tabindex="-1"></a>        targ_color <span class="op">=</span> color_dict[label_name]</span>
<span id="cb205-92"><a href="#cb205-92" aria-hidden="true" tabindex="-1"></a>        class_name_text_labels.append(label_name)</span>
<span id="cb205-93"><a href="#cb205-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-94"><a href="#cb205-94" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Draw the rectangle</span></span>
<span id="cb205-95"><a href="#cb205-95" aria-hidden="true" tabindex="-1"></a>        draw.rectangle(xy<span class="op">=</span>(x, y, x2, y2), </span>
<span id="cb205-96"><a href="#cb205-96" aria-hidden="true" tabindex="-1"></a>                       outline<span class="op">=</span>targ_color,</span>
<span id="cb205-97"><a href="#cb205-97" aria-hidden="true" tabindex="-1"></a>                       width<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb205-98"><a href="#cb205-98" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb205-99"><a href="#cb205-99" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create a text string to display</span></span>
<span id="cb205-100"><a href="#cb205-100" aria-hidden="true" tabindex="-1"></a>        text_string_to_show <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>label_name<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="bu">round</span>(score.item(), <span class="dv">3</span>)<span class="sc">}</span><span class="ss">)"</span></span>
<span id="cb205-101"><a href="#cb205-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-102"><a href="#cb205-102" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Draw the text on the image</span></span>
<span id="cb205-103"><a href="#cb205-103" aria-hidden="true" tabindex="-1"></a>        draw.text(xy<span class="op">=</span>(x, y),</span>
<span id="cb205-104"><a href="#cb205-104" aria-hidden="true" tabindex="-1"></a>                  text<span class="op">=</span>text_string_to_show,</span>
<span id="cb205-105"><a href="#cb205-105" aria-hidden="true" tabindex="-1"></a>                  fill<span class="op">=</span><span class="st">"white"</span>,</span>
<span id="cb205-106"><a href="#cb205-106" aria-hidden="true" tabindex="-1"></a>                  font<span class="op">=</span>font)</span>
<span id="cb205-107"><a href="#cb205-107" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb205-108"><a href="#cb205-108" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove the draw each time</span></span>
<span id="cb205-109"><a href="#cb205-109" aria-hidden="true" tabindex="-1"></a>    <span class="kw">del</span> draw</span>
<span id="cb205-110"><a href="#cb205-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-111"><a href="#cb205-111" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Setup blank string to print out</span></span>
<span id="cb205-112"><a href="#cb205-112" aria-hidden="true" tabindex="-1"></a>    return_string <span class="op">=</span> <span class="st">""</span></span>
<span id="cb205-113"><a href="#cb205-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-114"><a href="#cb205-114" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Setup list of target items to discover</span></span>
<span id="cb205-115"><a href="#cb205-115" aria-hidden="true" tabindex="-1"></a>    target_items <span class="op">=</span> [<span class="st">"trash"</span>, <span class="st">"bin"</span>, <span class="st">"hand"</span>]</span>
<span id="cb205-116"><a href="#cb205-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-117"><a href="#cb205-117" aria-hidden="true" tabindex="-1"></a>    <span class="co">### 5. Create logic for outputting information message </span><span class="al">###</span><span class="co"> </span></span>
<span id="cb205-118"><a href="#cb205-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-119"><a href="#cb205-119" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If no items detected or trash, bin, hand not in list, return notification </span></span>
<span id="cb205-120"><a href="#cb205-120" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="bu">len</span>(class_name_text_labels) <span class="op">==</span> <span class="dv">0</span>) <span class="kw">or</span> <span class="kw">not</span> (any_in_list(list_a<span class="op">=</span>target_items, list_b<span class="op">=</span>class_name_text_labels)):</span>
<span id="cb205-121"><a href="#cb205-121" aria-hidden="true" tabindex="-1"></a>        return_string <span class="op">=</span> <span class="ss">f"No trash, bin or hand detected at confidence threshold </span><span class="sc">{</span>conf_threshold<span class="sc">}</span><span class="ss">. Try another image or lowering the confidence threshold."</span></span>
<span id="cb205-122"><a href="#cb205-122" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image, return_string</span>
<span id="cb205-123"><a href="#cb205-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-124"><a href="#cb205-124" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If there are some missing, print the ones which are missing</span></span>
<span id="cb205-125"><a href="#cb205-125" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="kw">not</span> all_in_list(list_a<span class="op">=</span>target_items, list_b<span class="op">=</span>class_name_text_labels):</span>
<span id="cb205-126"><a href="#cb205-126" aria-hidden="true" tabindex="-1"></a>        missing_items <span class="op">=</span> []</span>
<span id="cb205-127"><a href="#cb205-127" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> item <span class="kw">in</span> target_items:</span>
<span id="cb205-128"><a href="#cb205-128" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> item <span class="kw">not</span> <span class="kw">in</span> class_name_text_labels:</span>
<span id="cb205-129"><a href="#cb205-129" aria-hidden="true" tabindex="-1"></a>                missing_items.append(item)</span>
<span id="cb205-130"><a href="#cb205-130" aria-hidden="true" tabindex="-1"></a>        return_string <span class="op">=</span> <span class="ss">f"Detected the following items: </span><span class="sc">{</span>class_name_text_labels<span class="sc">}</span><span class="ss">. But missing the following in order to get +1: </span><span class="sc">{</span>missing_items<span class="sc">}</span><span class="ss">. If this is an error, try another image or altering the confidence threshold. Otherwise, the model may need to be updated with better data."</span></span>
<span id="cb205-131"><a href="#cb205-131" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb205-132"><a href="#cb205-132" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If all 3 trash, bin, hand occur = + 1</span></span>
<span id="cb205-133"><a href="#cb205-133" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> all_in_list(list_a<span class="op">=</span>target_items, list_b<span class="op">=</span>class_name_text_labels):</span>
<span id="cb205-134"><a href="#cb205-134" aria-hidden="true" tabindex="-1"></a>        return_string <span class="op">=</span> <span class="ss">f"+1! Found the following items: </span><span class="sc">{</span>class_name_text_labels<span class="sc">}</span><span class="ss">, thank you for cleaning up the area!"</span></span>
<span id="cb205-135"><a href="#cb205-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-136"><a href="#cb205-136" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(return_string)</span>
<span id="cb205-137"><a href="#cb205-137" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb205-138"><a href="#cb205-138" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> image, return_string</span>
<span id="cb205-139"><a href="#cb205-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-140"><a href="#cb205-140" aria-hidden="true" tabindex="-1"></a><span class="co">### 6. Setup the demo application to take in image, make a prediction with our model, return the image with drawn predicitons </span><span class="al">###</span><span class="co"> </span></span>
<span id="cb205-141"><a href="#cb205-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-142"><a href="#cb205-142" aria-hidden="true" tabindex="-1"></a><span class="co"># Write description for our demo application</span></span>
<span id="cb205-143"><a href="#cb205-143" aria-hidden="true" tabindex="-1"></a>description <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb205-144"><a href="#cb205-144" aria-hidden="true" tabindex="-1"></a><span class="st">Help clean up your local area! Upload an image and get +1 if there is all of the following items detected: trash, bin, hand.</span></span>
<span id="cb205-145"><a href="#cb205-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-146"><a href="#cb205-146" aria-hidden="true" tabindex="-1"></a><span class="st">Model is a fine-tuned version of [RT-DETRv2](https://huggingface.co/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2Config) on the [Trashify dataset](https://huggingface.co/datasets/mrdbourke/trashify_manual_labelled_images).</span></span>
<span id="cb205-147"><a href="#cb205-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-148"><a href="#cb205-148" aria-hidden="true" tabindex="-1"></a><span class="st">See the full data loading and training code on [learnhuggingface.com](https://www.learnhuggingface.com/notebooks/hugging_face_object_detection_tutorial).</span></span>
<span id="cb205-149"><a href="#cb205-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-150"><a href="#cb205-150" aria-hidden="true" tabindex="-1"></a><span class="st">This version is v4 because the first three versions were using a different model and did not perform as well, see the [README](https://huggingface.co/spaces/mrdbourke/trashify_demo_v4/blob/main/README.md) for more.</span></span>
<span id="cb205-151"><a href="#cb205-151" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb205-152"><a href="#cb205-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-153"><a href="#cb205-153" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the Gradio interface to accept an image and confidence threshold and return an image with drawn prediction boxes</span></span>
<span id="cb205-154"><a href="#cb205-154" aria-hidden="true" tabindex="-1"></a>demo <span class="op">=</span> gr.Interface(</span>
<span id="cb205-155"><a href="#cb205-155" aria-hidden="true" tabindex="-1"></a>    fn<span class="op">=</span>predict_on_image,</span>
<span id="cb205-156"><a href="#cb205-156" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>[</span>
<span id="cb205-157"><a href="#cb205-157" aria-hidden="true" tabindex="-1"></a>        gr.Image(<span class="bu">type</span><span class="op">=</span><span class="st">"pil"</span>, label<span class="op">=</span><span class="st">"Target Image"</span>),</span>
<span id="cb205-158"><a href="#cb205-158" aria-hidden="true" tabindex="-1"></a>        gr.Slider(minimum<span class="op">=</span><span class="dv">0</span>, maximum<span class="op">=</span><span class="dv">1</span>, value<span class="op">=</span><span class="fl">0.3</span>, label<span class="op">=</span><span class="st">"Confidence Threshold"</span>)</span>
<span id="cb205-159"><a href="#cb205-159" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb205-160"><a href="#cb205-160" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>[</span>
<span id="cb205-161"><a href="#cb205-161" aria-hidden="true" tabindex="-1"></a>        gr.Image(<span class="bu">type</span><span class="op">=</span><span class="st">"pil"</span>, label<span class="op">=</span><span class="st">"Image Output"</span>),</span>
<span id="cb205-162"><a href="#cb205-162" aria-hidden="true" tabindex="-1"></a>        gr.Text(label<span class="op">=</span><span class="st">"Text Output"</span>)</span>
<span id="cb205-163"><a href="#cb205-163" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb205-164"><a href="#cb205-164" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"🚮 Trashify Object Detection Demo V4"</span>,</span>
<span id="cb205-165"><a href="#cb205-165" aria-hidden="true" tabindex="-1"></a>    description<span class="op">=</span>description,</span>
<span id="cb205-166"><a href="#cb205-166" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Examples come in the form of a list of lists, where each inner list contains elements to prefill the `inputs` parameter with</span></span>
<span id="cb205-167"><a href="#cb205-167" aria-hidden="true" tabindex="-1"></a>    <span class="co"># See where the examples originate from here: https://huggingface.co/datasets/mrdbourke/trashify_examples/</span></span>
<span id="cb205-168"><a href="#cb205-168" aria-hidden="true" tabindex="-1"></a>    examples<span class="op">=</span>[</span>
<span id="cb205-169"><a href="#cb205-169" aria-hidden="true" tabindex="-1"></a>        [<span class="st">"trashify_examples/trashify_example_1.jpeg"</span>, <span class="fl">0.3</span>],</span>
<span id="cb205-170"><a href="#cb205-170" aria-hidden="true" tabindex="-1"></a>        [<span class="st">"trashify_examples/trashify_example_2.jpeg"</span>, <span class="fl">0.3</span>], </span>
<span id="cb205-171"><a href="#cb205-171" aria-hidden="true" tabindex="-1"></a>        [<span class="st">"trashify_examples/trashify_example_3.jpeg"</span>, <span class="fl">0.3</span>],</span>
<span id="cb205-172"><a href="#cb205-172" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb205-173"><a href="#cb205-173" aria-hidden="true" tabindex="-1"></a>    cache_examples<span class="op">=</span><span class="va">True</span></span>
<span id="cb205-174"><a href="#cb205-174" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb205-175"><a href="#cb205-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-176"><a href="#cb205-176" aria-hidden="true" tabindex="-1"></a><span class="co"># Launch the demo</span></span>
<span id="cb205-177"><a href="#cb205-177" aria-hidden="true" tabindex="-1"></a>demo.launch()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Overwriting ../demos/trashify_object_detector/app.py</code></pre>
</div>
</div>
</section>
<section id="making-a-requirements-file" class="level3" data-number="17.2">
<h3 data-number="17.2" class="anchored" data-anchor-id="making-a-requirements-file"><span class="header-section-number">17.2</span> Making a requirements file</h3>
<p>When you upload an <code>app.py</code> file to Hugging Face Spaces, it will attempt to run it automatically.</p>
<p>And just like running the file locally, we need to make sure all of the required packages are available.</p>
<p>Otherwise our Space will produce an error like the following:</p>
<pre><code>===== Application Startup at ... =====

Traceback (most recent call last):
  File "/home/user/app/app.py", line 1, in &lt;module&gt;
    import torch
ModuleNotFoundError: No module named 'torch'</code></pre>
<p>Good news is, our demo only has three requirements: <code>gradio</code>, <code>torch</code>, <code>transformers</code>.</p>
<p>Let’s create a <code>requirements.txt</code> file with the packages we need and save it to the same directory as our <code>app.py</code> file.</p>
<p>%%writefile ../demos/trashify_object_detector/requirements.txt timm gradio torch transformers</p>
</section>
<section id="making-a-readme-file" class="level3" data-number="17.3">
<h3 data-number="17.3" class="anchored" data-anchor-id="making-a-readme-file"><span class="header-section-number">17.3</span> Making a README file</h3>
<p>Our <code>app.py</code> can contain information about our demo, however, we can also use a <code>README.md</code> file to further communicate our work.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>It is common practice in Git repositories (including GitHub and Hugging Face Hub) to add a <code>README.md</code> file to your project so people can read more (hence “read me”) about what your project is about.</p>
</div>
</div>
<p>We can include anything in <a href="https://huggingface.co/docs/hub/en/spaces-config-reference">markdown-style text</a> in the <code>README.md</code> file.</p>
<p>However, Spaces also have a special <a href="https://simple.wikipedia.org/wiki/YAML">YAML block</a> at the top of the <code>README.md</code> file in the root directory with configuration details.</p>
<p>Inside the YAML block you can put special metadata details about your Space including:</p>
<ul>
<li><code>title</code> - The title of your Space (e.g.&nbsp;<code>title: Trashify Demo V4 🚮</code>).</li>
<li><code>emoji</code> - The emoji to display on your Space (e.g.&nbsp;<code>emoji: 🗑️</code>).</li>
<li><code>app_file</code> - The target app file for Spaces to run (set to <code>app_file: app.py</code> by default).</li>
</ul>
<p>And there are plenty more in the <a href="https://huggingface.co/docs/hub/en/spaces-config-reference">Spaces Configuration References documentation</a>.</p>
<figure style="text-align: center;" class="figure">
<!-- figtemplate -->
<img src="https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-trashify-object-detection/09-trashify-spaces-front-matter.png" alt="A screenshot of a Hugging Face Space 'README.md' editor for 'trashify_demo_v4', divided into two green-outlined sections: the top section, labeled 'YAML front matter', displays a 'Metadata UI' with fields for 'license' (Apache License 2.0), 'sdk' (Gradio), 'colorFrom' (purple), etc., and below it, the corresponding YAML code with 'title: Trashify Demo V4', 'emoji: 🗑️', 'colorFrom: purple', and other settings; the bottom section, labeled 'Markdown description', shows the markdown content starting with '# 🗑️ Trashify Object Detector V4', followed by descriptions under '## Dataset', '## Demos', and '## Learn more', including links to datasets and other demo versions." style="width: 100%; max-width: 900px; height: auto;" class="figure-img">
<figcaption>
Example of Hugging Face Spaces README.md file with YAML front matter (front matter is another term for “things at the front/top of the file”) for formatting the Space.
</figcaption>
</figure>
<p>Let’s create a <code>README.md</code> file with a YAML block at the top detailing some of the metadata about our project.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The YAML block at the top of the <code>README.md</code> can take some practice.</p>
<p>If you want to see a demo of how one gets created, try making a Hugging Face Space with the “Create new Space” button on the <a href="https://huggingface.co/spaces">https://huggingface.co/spaces</a> page and seeing what the <code>README.md</code> file starts with (that’s how I found out what to do!).</p>
</div>
</div>
<div id="cell-264" class="cell" data-execution_count="118">
<div class="sourceCode cell-code" id="cb208"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb208-1"><a href="#cb208-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile ..<span class="op">/</span>demos<span class="op">/</span>trashify_object_detector<span class="op">/</span>README.md</span>
<span id="cb208-2"><a href="#cb208-2" aria-hidden="true" tabindex="-1"></a><span class="op">---</span></span>
<span id="cb208-3"><a href="#cb208-3" aria-hidden="true" tabindex="-1"></a>title: Trashify Demo V4 🚮</span>
<span id="cb208-4"><a href="#cb208-4" aria-hidden="true" tabindex="-1"></a>emoji: 🗑️</span>
<span id="cb208-5"><a href="#cb208-5" aria-hidden="true" tabindex="-1"></a>colorFrom: purple</span>
<span id="cb208-6"><a href="#cb208-6" aria-hidden="true" tabindex="-1"></a>colorTo: blue</span>
<span id="cb208-7"><a href="#cb208-7" aria-hidden="true" tabindex="-1"></a>sdk: gradio</span>
<span id="cb208-8"><a href="#cb208-8" aria-hidden="true" tabindex="-1"></a>sdk_version: <span class="fl">5.34.0</span></span>
<span id="cb208-9"><a href="#cb208-9" aria-hidden="true" tabindex="-1"></a>app_file: app.py</span>
<span id="cb208-10"><a href="#cb208-10" aria-hidden="true" tabindex="-1"></a>pinned: false</span>
<span id="cb208-11"><a href="#cb208-11" aria-hidden="true" tabindex="-1"></a>license: apache<span class="op">-</span><span class="fl">2.0</span></span>
<span id="cb208-12"><a href="#cb208-12" aria-hidden="true" tabindex="-1"></a><span class="op">---</span></span>
<span id="cb208-13"><a href="#cb208-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-14"><a href="#cb208-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 🚮 Trashify Object Detector V4 </span></span>
<span id="cb208-15"><a href="#cb208-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-16"><a href="#cb208-16" aria-hidden="true" tabindex="-1"></a>Object detection demo to detect `trash`, `bin`, `hand`, `trash_arm`, `not_trash`, `not_bin`, `not_hand`. </span>
<span id="cb208-17"><a href="#cb208-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-18"><a href="#cb208-18" aria-hidden="true" tabindex="-1"></a>Used <span class="im">as</span> example <span class="cf">for</span> encouraging people to cleanup their local area.</span>
<span id="cb208-19"><a href="#cb208-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-20"><a href="#cb208-20" aria-hidden="true" tabindex="-1"></a>If `trash`, `hand`, `bin` <span class="bu">all</span> detected <span class="op">=</span> <span class="op">+</span><span class="dv">1</span> point.</span>
<span id="cb208-21"><a href="#cb208-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-22"><a href="#cb208-22" aria-hidden="true" tabindex="-1"></a><span class="co">## Dataset</span></span>
<span id="cb208-23"><a href="#cb208-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-24"><a href="#cb208-24" aria-hidden="true" tabindex="-1"></a>All Trashify models are trained on a custom hand<span class="op">-</span>labelled dataset of people picking up trash <span class="kw">and</span> placing it <span class="kw">in</span> a <span class="bu">bin</span>.</span>
<span id="cb208-25"><a href="#cb208-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-26"><a href="#cb208-26" aria-hidden="true" tabindex="-1"></a>The dataset can be found on Hugging Face <span class="im">as</span> [`mrdbourke<span class="op">/</span>trashify_manual_labelled_images`](https:<span class="op">//</span>huggingface.co<span class="op">/</span>datasets<span class="op">/</span>mrdbourke<span class="op">/</span>trashify_manual_labelled_images).</span>
<span id="cb208-27"><a href="#cb208-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-28"><a href="#cb208-28" aria-hidden="true" tabindex="-1"></a><span class="co">## Demos</span></span>
<span id="cb208-29"><a href="#cb208-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-30"><a href="#cb208-30" aria-hidden="true" tabindex="-1"></a><span class="op">*</span> [V1](https:<span class="op">//</span>huggingface.co<span class="op">/</span>spaces<span class="op">/</span>mrdbourke<span class="op">/</span>trashify_demo_v1) <span class="op">=</span> Fine<span class="op">-</span>tuned [Conditional DETR](https:<span class="op">//</span>huggingface.co<span class="op">/</span>docs<span class="op">/</span>transformers<span class="op">/</span>en<span class="op">/</span>model_doc<span class="op">/</span>conditional_detr) model trained <span class="op">*</span>without<span class="op">*</span> data augmentation.</span>
<span id="cb208-31"><a href="#cb208-31" aria-hidden="true" tabindex="-1"></a><span class="op">*</span> [V2](https:<span class="op">//</span>huggingface.co<span class="op">/</span>spaces<span class="op">/</span>mrdbourke<span class="op">/</span>trashify_demo_v2) <span class="op">=</span> Fine<span class="op">-</span>tuned Conditional DETR model trained <span class="op">*</span><span class="cf">with</span><span class="op">*</span> data augmentation.</span>
<span id="cb208-32"><a href="#cb208-32" aria-hidden="true" tabindex="-1"></a><span class="op">*</span> [V3](https:<span class="op">//</span>huggingface.co<span class="op">/</span>spaces<span class="op">/</span>mrdbourke<span class="op">/</span>trashify_demo_v3) <span class="op">=</span> Fine<span class="op">-</span>tuned Conditional DETR model trained <span class="op">*</span><span class="cf">with</span><span class="op">*</span> data augmentation (same <span class="im">as</span> V2) <span class="cf">with</span> an NMS (Non Maximum Suppression) post<span class="op">-</span>processing step.</span>
<span id="cb208-33"><a href="#cb208-33" aria-hidden="true" tabindex="-1"></a><span class="op">*</span> [V4](https:<span class="op">//</span>huggingface.co<span class="op">/</span>spaces<span class="op">/</span>mrdbourke<span class="op">/</span>trashify_demo_v4) <span class="op">=</span> Fine<span class="op">-</span>tuned [RT<span class="op">-</span>DETRv2](https:<span class="op">//</span>huggingface.co<span class="op">/</span>docs<span class="op">/</span>transformers<span class="op">/</span>main<span class="op">/</span>en<span class="op">/</span>model_doc<span class="op">/</span>rt_detr_v2) model trained <span class="op">*</span>without<span class="op">*</span> data augmentation <span class="kw">or</span> NMS post<span class="op">-</span>processing (current best mAP).</span>
<span id="cb208-34"><a href="#cb208-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-35"><a href="#cb208-35" aria-hidden="true" tabindex="-1"></a><span class="co">## Learn more</span></span>
<span id="cb208-36"><a href="#cb208-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-37"><a href="#cb208-37" aria-hidden="true" tabindex="-1"></a>See the full end<span class="op">-</span>to<span class="op">-</span>end code of how this demo was built at [learnhuggingface.com](https:<span class="op">//</span>www.learnhuggingface.com<span class="op">/</span>notebooks<span class="op">/</span>hugging_face_object_detection_tutorial). </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Overwriting ../demos/trashify_object_detector/README.md</code></pre>
</div>
</div>
</section>
<section id="making-an-examples-folder" class="level3" data-number="17.4">
<h3 data-number="17.4" class="anchored" data-anchor-id="making-an-examples-folder"><span class="header-section-number">17.4</span> Making an examples folder</h3>
<p>When we create our demo application, it’ll be good to show people how to use it.</p>
<p>To do so, we can add some example images to use with our demo.</p>
<p>First we’ll create a folder to store the demo images.</p>
<div id="cell-266" class="cell" data-execution_count="119">
<div class="sourceCode cell-code" id="cb210"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb210-1"><a href="#cb210-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make a directory to save examples to</span></span>
<span id="cb210-2"><a href="#cb210-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb210-3"><a href="#cb210-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb210-4"><a href="#cb210-4" aria-hidden="true" tabindex="-1"></a>demo_example_dir <span class="op">=</span> <span class="st">"../demos/trashify_object_detector/trashify_examples/"</span></span>
<span id="cb210-5"><a href="#cb210-5" aria-hidden="true" tabindex="-1"></a>Path(demo_example_dir).mkdir(exist_ok<span class="op">=</span><span class="va">True</span>, parents<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And now we can download some pre-made examples I’ve added to Hugging Face Datasets (none of these were in the Trashify training data).</p>
<p>You can find the example Trashify images at <a href="https://huggingface.co/datasets/mrdbourke/trashify_examples"><code>mrdbourke/trashify_examples</code></a>.</p>
<div id="cell-268" class="cell" data-execution_count="120">
<div class="sourceCode cell-code" id="cb211"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb211-1"><a href="#cb211-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Download the examples from Hugging Face Datasets</span></span>
<span id="cb211-2"><a href="#cb211-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb211-3"><a href="#cb211-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb211-4"><a href="#cb211-4" aria-hidden="true" tabindex="-1"></a>trashify_examples <span class="op">=</span> load_dataset(<span class="st">"mrdbourke/trashify_examples"</span>)</span>
<span id="cb211-5"><a href="#cb211-5" aria-hidden="true" tabindex="-1"></a>trashify_examples</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="120">
<pre><code>DatasetDict({
    train: Dataset({
        features: ['image'],
        num_rows: 3
    })
})</code></pre>
</div>
</div>
<p>Perfect!</p>
<p>Now let’s save each of these images to our target example folder in the Trashify demo directory.</p>
<div id="cell-270" class="cell" data-execution_count="121">
<div class="sourceCode cell-code" id="cb213"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb213-1"><a href="#cb213-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, sample <span class="kw">in</span> <span class="bu">enumerate</span>(trashify_examples[<span class="st">"train"</span>]):</span>
<span id="cb213-2"><a href="#cb213-2" aria-hidden="true" tabindex="-1"></a>    save_path <span class="op">=</span> Path(demo_example_dir, <span class="ss">f"trashify_example_</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">.jpeg"</span>)</span>
<span id="cb213-3"><a href="#cb213-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"[INFO] Saving image to: </span><span class="sc">{</span>save_path<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb213-4"><a href="#cb213-4" aria-hidden="true" tabindex="-1"></a>    sample[<span class="st">"image"</span>].save(save_path)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Saving image to: ../demos/trashify_object_detector/trashify_examples/trashify_example_1.jpeg
[INFO] Saving image to: ../demos/trashify_object_detector/trashify_examples/trashify_example_2.jpeg
[INFO] Saving image to: ../demos/trashify_object_detector/trashify_examples/trashify_example_3.jpeg</code></pre>
</div>
</div>
<p>Now let’s check the demo folder for Trashify.</p>
<div id="cell-272" class="cell" data-execution_count="122">
<div class="sourceCode cell-code" id="cb215"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb215-1"><a href="#cb215-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>ls ..<span class="op">/</span>demos<span class="op">/</span>trashify_object_detector<span class="op">/</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>README.md  app.py  requirements.txt  trashify_examples</code></pre>
</div>
</div>
<p>Perfect!</p>
<p>Looks like we’ve got all the files we need to create our Space.</p>
<p>Let’s upload them to the Hugging Face Hub.</p>
</section>
<section id="uploading-our-demo-to-hugging-face-spaces" class="level3" data-number="17.5">
<h3 data-number="17.5" class="anchored" data-anchor-id="uploading-our-demo-to-hugging-face-spaces"><span class="header-section-number">17.5</span> Uploading our demo to Hugging Face Spaces</h3>
<p>We’ve created all of the files required for our demo, now for the fun part!</p>
<p>Let’s upload them to Hugging Face Spaces.</p>
<p>To do so programmatically, we can use the <a href="https://huggingface.co/docs/huggingface_hub/en/package_reference/hf_api">Hugging Face Hub Python API</a>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <a href="https://huggingface.co/docs/huggingface_hub/en/package_reference/hf_api">Hugging Face Hub Python API</a> has many different options for interacting with the Hugging Face Hub programmatically.</p>
<p>You can create repositories, upload files, upload folders, add comments, change permissions and much much more.</p>
<p>Be sure to explore the documentation for at least 10-15 minutes to get an idea of what’s possible.</p>
</div>
</div>
<p>To get our demo hosted on Hugging Face Spaces we’ll go through the following steps:</p>
<ol type="1">
<li>Import the required methods from the <code>huggingface_hub</code> package, including <a href="https://huggingface.co/docs/huggingface_hub/package_reference/hf_api#huggingface_hub.HfApi.create_repo"><code>create_repo</code></a>, <a href="https://huggingface.co/docs/huggingface_hub/package_reference/hf_api#huggingface_hub.HfApi.get_full_repo_name"><code>get_full_repo_name</code></a>, <a href="https://huggingface.co/docs/huggingface_hub/en/package_reference/hf_api#huggingface_hub.HfApi.upload_file"><code>upload_file</code></a> (optional, we’ll be using <code>upload_folder</code>) and <a href="https://huggingface.co/docs/huggingface_hub/en/package_reference/hf_api#huggingface_hub.HfApi.upload_folder"><code>upload_folder</code></a>.</li>
<li>Define the demo folder we’d like to upload as well as the different parameters for the Hugging Face Space such as repo type (<code>"space"</code>), our target Space name, the target Space SDK (<code>"gradio"</code>), our <a href="https://huggingface.co/docs/hub/en/security-tokens">Hugging Face token</a> with write access (optional if it already isn’t setup).</li>
<li>Create a repository on Hugging Face Spaces using the <code>huggingface_hub.create_repo</code> method and filling out the appropriate parameters.</li>
<li>Get the full name of our created repository using the <code>huggingface_hub.get_full_repo_name</code> method (we could hard code this but I like to get it programmatically incase things change).</li>
<li>Upload the contents of our target demo folder (<code>../demos/trashify_object_detector/</code>) to Hugging Face Hub with <a href="https://huggingface.co/docs/huggingface_hub/en/package_reference/hf_api#huggingface_hub.HfApi.upload_folder"><code>huggingface_hub.upload_folder</code></a>.</li>
<li>Hope it all works and inspect the results! 🤞</li>
</ol>
<p>A fair few steps but we’ve got this!</p>
<div id="cell-275" class="cell" data-execution_count="123">
<div class="sourceCode cell-code" id="cb217"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb217-1"><a href="#cb217-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Import the required methods for uploading to the Hugging Face Hub</span></span>
<span id="cb217-2"><a href="#cb217-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> huggingface_hub <span class="im">import</span> (</span>
<span id="cb217-3"><a href="#cb217-3" aria-hidden="true" tabindex="-1"></a>    create_repo,</span>
<span id="cb217-4"><a href="#cb217-4" aria-hidden="true" tabindex="-1"></a>    get_full_repo_name,</span>
<span id="cb217-5"><a href="#cb217-5" aria-hidden="true" tabindex="-1"></a>    upload_file, <span class="co"># for uploading a single file (if necessary)</span></span>
<span id="cb217-6"><a href="#cb217-6" aria-hidden="true" tabindex="-1"></a>    upload_folder <span class="co"># for uploading multiple files (in a folder)</span></span>
<span id="cb217-7"><a href="#cb217-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb217-8"><a href="#cb217-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb217-9"><a href="#cb217-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Define the parameters we'd like to use for the upload</span></span>
<span id="cb217-10"><a href="#cb217-10" aria-hidden="true" tabindex="-1"></a>LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD <span class="op">=</span> <span class="st">"../demos/trashify_object_detector"</span> </span>
<span id="cb217-11"><a href="#cb217-11" aria-hidden="true" tabindex="-1"></a>HF_TARGET_SPACE_NAME <span class="op">=</span> <span class="st">"trashify_demo_v4"</span></span>
<span id="cb217-12"><a href="#cb217-12" aria-hidden="true" tabindex="-1"></a>HF_REPO_TYPE <span class="op">=</span> <span class="st">"space"</span> <span class="co"># we're creating a Hugging Face Space</span></span>
<span id="cb217-13"><a href="#cb217-13" aria-hidden="true" tabindex="-1"></a>HF_SPACE_SDK <span class="op">=</span> <span class="st">"gradio"</span></span>
<span id="cb217-14"><a href="#cb217-14" aria-hidden="true" tabindex="-1"></a>HF_TOKEN <span class="op">=</span> <span class="st">""</span> <span class="co"># optional: set to your Hugging Face token (but I'd advise storing this as an environment variable as previously discussed)</span></span>
<span id="cb217-15"><a href="#cb217-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb217-16"><a href="#cb217-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Create a Space repository on Hugging Face Hub </span></span>
<span id="cb217-17"><a href="#cb217-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Creating repo on Hugging Face Hub with name: </span><span class="sc">{</span>HF_TARGET_SPACE_NAME<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb217-18"><a href="#cb217-18" aria-hidden="true" tabindex="-1"></a>create_repo(</span>
<span id="cb217-19"><a href="#cb217-19" aria-hidden="true" tabindex="-1"></a>    repo_id<span class="op">=</span>HF_TARGET_SPACE_NAME,</span>
<span id="cb217-20"><a href="#cb217-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># token=HF_TOKEN, # optional: set token manually (though it will be automatically recognized if it's available as an environment variable)</span></span>
<span id="cb217-21"><a href="#cb217-21" aria-hidden="true" tabindex="-1"></a>    repo_type<span class="op">=</span>HF_REPO_TYPE,</span>
<span id="cb217-22"><a href="#cb217-22" aria-hidden="true" tabindex="-1"></a>    private<span class="op">=</span><span class="va">False</span>, <span class="co"># set to True if you don't want your Space to be accessible to others</span></span>
<span id="cb217-23"><a href="#cb217-23" aria-hidden="true" tabindex="-1"></a>    space_sdk<span class="op">=</span>HF_SPACE_SDK,</span>
<span id="cb217-24"><a href="#cb217-24" aria-hidden="true" tabindex="-1"></a>    exist_ok<span class="op">=</span><span class="va">True</span>, <span class="co"># set to False if you want an error to raise if the repo_id already exists </span></span>
<span id="cb217-25"><a href="#cb217-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb217-26"><a href="#cb217-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb217-27"><a href="#cb217-27" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Get the full repository name (e.g. {username}/{model_id} or {username}/{space_name})</span></span>
<span id="cb217-28"><a href="#cb217-28" aria-hidden="true" tabindex="-1"></a>full_hf_repo_name <span class="op">=</span> get_full_repo_name(model_id<span class="op">=</span>HF_TARGET_SPACE_NAME)</span>
<span id="cb217-29"><a href="#cb217-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Full Hugging Face Hub repo name: </span><span class="sc">{</span>full_hf_repo_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb217-30"><a href="#cb217-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb217-31"><a href="#cb217-31" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Upload our demo folder</span></span>
<span id="cb217-32"><a href="#cb217-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Uploading </span><span class="sc">{</span>LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD<span class="sc">}</span><span class="ss"> to repo: </span><span class="sc">{</span>full_hf_repo_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb217-33"><a href="#cb217-33" aria-hidden="true" tabindex="-1"></a>folder_upload_url <span class="op">=</span> upload_folder(</span>
<span id="cb217-34"><a href="#cb217-34" aria-hidden="true" tabindex="-1"></a>    repo_id<span class="op">=</span>full_hf_repo_name,</span>
<span id="cb217-35"><a href="#cb217-35" aria-hidden="true" tabindex="-1"></a>    folder_path<span class="op">=</span>LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD,</span>
<span id="cb217-36"><a href="#cb217-36" aria-hidden="true" tabindex="-1"></a>    path_in_repo<span class="op">=</span><span class="st">"."</span>, <span class="co"># upload our folder to the root directory ("." means "base" or "root", this is the default)</span></span>
<span id="cb217-37"><a href="#cb217-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># token=HF_TOKEN, # optional: set token manually</span></span>
<span id="cb217-38"><a href="#cb217-38" aria-hidden="true" tabindex="-1"></a>    repo_type<span class="op">=</span>HF_REPO_TYPE,</span>
<span id="cb217-39"><a href="#cb217-39" aria-hidden="true" tabindex="-1"></a>    commit_message<span class="op">=</span><span class="st">"Uploading Trashify box detection model app.py"</span></span>
<span id="cb217-40"><a href="#cb217-40" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb217-41"><a href="#cb217-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"[INFO] Demo folder successfully uploaded with commit URL: </span><span class="sc">{</span>folder_upload_url<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[INFO] Creating repo on Hugging Face Hub with name: trashify_demo_v4
[INFO] Full Hugging Face Hub repo name: mrdbourke/trashify_demo_v4
[INFO] Uploading ../demos/trashify_object_detector to repo: mrdbourke/trashify_demo_v4
[INFO] Demo folder successfully uploaded with commit URL: https://huggingface.co/spaces/mrdbourke/trashify_demo_v4/tree/main/.</code></pre>
</div>
</div>
<p>Woohoo!! Looks like our demo is now live on the Hugging Face Hub!</p>
</section>
<section id="testing-the-hosted-demo" class="level3" data-number="17.6">
<h3 data-number="17.6" class="anchored" data-anchor-id="testing-the-hosted-demo"><span class="header-section-number">17.6</span> Testing the hosted demo</h3>
<p>We are showing Trashify v4 because like Star Wars episodes, I’ve already done a few experiments before launching it.</p>
<p>One of the cool things about using Hugging Facce Spaces is we can embed the Space in our notebook using HTML.</p>
<p>To do so, you can click the “…” button (three dots) in the top right hand corner of the Hugging Face Space and choose the option “Embed this Space”.</p>
<p>To embed it with HTML, you can choose the <code>Iframe</code> option.</p>
<div id="cell-278" class="cell" data-execution_count="116">
<div class="sourceCode cell-code" id="cb219"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb219-1"><a href="#cb219-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> HTML</span>
<span id="cb219-2"><a href="#cb219-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb219-3"><a href="#cb219-3" aria-hidden="true" tabindex="-1"></a><span class="co"># You can get embeddable HTML code for your demo by clicking the "Embed" button on the demo page</span></span>
<span id="cb219-4"><a href="#cb219-4" aria-hidden="true" tabindex="-1"></a>HTML(data<span class="op">=</span><span class="st">'''</span></span>
<span id="cb219-5"><a href="#cb219-5" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;iframe</span></span>
<span id="cb219-6"><a href="#cb219-6" aria-hidden="true" tabindex="-1"></a><span class="st">    src="https://mrdbourke-trashify-demo-v4.hf.space"</span></span>
<span id="cb219-7"><a href="#cb219-7" aria-hidden="true" tabindex="-1"></a><span class="st">    frameborder="0"</span></span>
<span id="cb219-8"><a href="#cb219-8" aria-hidden="true" tabindex="-1"></a><span class="st">    width="850"</span></span>
<span id="cb219-9"><a href="#cb219-9" aria-hidden="true" tabindex="-1"></a><span class="st">    height="1000"</span></span>
<span id="cb219-10"><a href="#cb219-10" aria-hidden="true" tabindex="-1"></a><span class="st">&gt;&lt;/iframe&gt;     </span></span>
<span id="cb219-11"><a href="#cb219-11" aria-hidden="true" tabindex="-1"></a><span class="st">'''</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="116">

<iframe src="https://mrdbourke-trashify-demo-v4.hf.space" frameborder="0" width="850" height="1000"></iframe>     
</div>
</div>
<p>Isn’t that cool!</p>
<p>Our very our object detection model trained on a custom dataset and now live on the internet for other people to try out!</p>
</section>
</section>
<section id="summary" class="level2" data-number="18">
<h2 data-number="18" class="anchored" data-anchor-id="summary"><span class="header-section-number">18</span> Summary</h2>
<p>We’ve followed the data, model, demo paradigm and built a custom object detection model on a custom dataset and deployed it into a functional demo.</p>
<p>Object detection models can be a bit of challenge to begin with as you need coordinate data for items in your images as well as label data.</p>
<p>However, as we’ve seen, with the right mix of data, model and training techniques we can take an existing object detection model like RT-DETRv2 and tailor it to our own projects.</p>
<p>The following extensions and extra-curriculum are good follow ups to practice what we’ve learned here.</p>
<section id="extensions" class="level3" data-number="18.1">
<h3 data-number="18.1" class="anchored" data-anchor-id="extensions"><span class="header-section-number">18.1</span> Extensions</h3>
<ul>
<li>Can you improve the model by training it for longer? What happens if you double the amount of epochs we did?</li>
<li>What are some ways to improve the model on differnet kinds of data? Could you take 10-30 photos of your own and add it to the dataset to improve the model?</li>
<li>Data augmentation is one way to improve image classification models but it can also work for object detection models, how might you implement data augmentation into our training pipeline?
<ul>
<li>Hint: See <a href="https://docs.pytorch.org/vision/main/auto_examples/transforms/plot_transforms_e2e.html#transforms-v2-end-to-end-object-detection-segmentation-example">PyTorch’s guide for data augmentation on detection problems</a>.</li>
</ul></li>
<li>So far the RT-DETRv2 model we used seemed to work pretty well, what happens if you try the <a href="https://huggingface.co/docs/transformers/main/model_doc/d_fine">D-FINE model</a>?</li>
<li>Sometimes our model predicts multiple of the same kind of boxes, for example, it will predict 2 boxes for “hand” when there is only one, a technique to help with this is called NMS (<a href="https://paperswithcode.com/method/non-maximum-suppression">Non Maximum Suppression</a>), how might you implement this into our post processing pipeline?</li>
</ul>
</section>
<section id="extra-curriculum" class="level3" data-number="18.2">
<h3 data-number="18.2" class="anchored" data-anchor-id="extra-curriculum"><span class="header-section-number">18.2</span> Extra-Curriculum</h3>
<ul>
<li>“But what if I have images but no box labels?” Great questions. One way to acquire labels is to use a zero-shot detection model such as <a href="https://huggingface.co/omlab/omdet-turbo-swin-tiny-hf">OmDet Turbo</a> or <a href="https://huggingface.co/docs/transformers/en/model_doc/grounding-dino">Grounding DINO</a> which are capable of producing box labels on images given a text input. You could use these to bootstrap a labelled dataset and then train a custom model on them/improve them by reviewing.</li>
<li>Another way to get high quality labels is to manually annotate images, you can do with tools such as <a href="https://labelstud.io">Label Studio</a> and <a href="https://prodi.gy/">Prodigy</a>.</li>
<li>For more on the RT-DETRv2 model, I’d encourage you to read the original paper where it was introduced, <a href="https://huggingface.co/papers/2407.17140"><em>RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer</em></a>.</li>
</ul>
</section>
<section id="extra-resources" class="level3" data-number="18.3">
<h3 data-number="18.3" class="anchored" data-anchor-id="extra-resources"><span class="header-section-number">18.3</span> Extra resources</h3>
<ul>
<li><a href="https://www.learnml.io/posts/apache-object-detection-models/">Apache 2.0 object detector models</a> - For a list of high-performing Apache 2.0 (permissive open-source licence which enables commercial use) object detection models, I’ve created a short guide which collects them.</li>
<li><a href="https://github.com/roboflow/supervision"><code>supervision</code> library</a> - An excellent open-source library with plenty of visualization utilities for computer vision projects.</li>
<li><a href="https://blog.roboflow.com/object-detection-metrics/">Object detection evaluation metrics</a> by Roboflow - This is a great guide to all of the important detection metrics you’ll want to look at when creating object detection models.</li>
<li><a href="https://www.learnml.io/posts/a-guide-to-bounding-box-formats/">A Guide to Bounding Box Formats and How to Draw Them</a> by Daniel Bourke - One of the most important things when it comes to evaluating object detection models is to see how they look on images, this guide shows you how to draw bounding boxes on images.</li>
<li><a href="https://www.learnml.io/posts/a-gentle-guide-to-intersection-over-union/">A Hands-on Guide to IoU (Intersection over Union) for Bounding Boxes</a> by Daniel Bourke - Intersection over Union (IoU) is a measure of how much one box overlaps another and is used for evaluating the quality of your predicted bounding boxes, this guide walks through code examples of calculating IoU.</li>
</ul>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/www\.learnhuggingface\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/mrdbourke/learn-huggingface/issues" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.mrdbourke.com">
      <i class="bi bi-globe" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/mrdbourke/learn-huggingface">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.youtube.com/channel/UCr8O8l5cCX85Oem1d18EezQ">
      <i class="bi bi-youtube" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/mrdbourke">
      <i class="bi bi-twitter-x" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/mrdbourke/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>